Physically Based Rendering is a terriﬁc book. It covers all the marvelous math, fascinating physics, practical software engineering, and clever tricks that are necessary to write a stateof-the-art photorealistic renderer. All of these topics are dealt with in a clear and pedagogical manner without omitting the all-important practical details.
pbrt is not just a “toy” implementation of a ray tracer but a general and robust full-scale global illumination renderer. It contains many important optimizations to reduce execution time and memory consumption for complex scenes. Furthermore, pbrt is easy to extend to experiment with other rendering algorithm variations.
This book is not only a textbook for students but also a useful reference book for practitioners in the ﬁeld. The third edition has been extended with new sections on bidirectional path tracing, realistic camera models, and a state-of-the-art explanation of subsurface scattering.
Per Christensen Senior Software Developer, RenderMan Products, Pixar Animation Studios
Looking for a job in research or high end rendering? Get your kick-start education and create your own project with this book that comes along with both theory and real examples, meaning real code and real content for your renderer.
With their third edition, Matt Pharr, Greg Humphreys, and Wenzel Jakob provide easy access to even the most advanced rendering techniques like multiplexed Metropolis light transport and quasi-Monte Carlo methods. Most importantly, the framework lets you skip the bootstrap pain of getting data into and out of your renderer.
The holistic approach of literate programming results in a clear logic of an easy-to-study text. If you are serious about graphics, there is no way around this unique and extremely valuable book that is closest to the state of the art.
Alexander Keller Director of Research, NVIDIA

This page intentionally left blank

Physically Based Rendering
FROM THEORY TO IMPLEMENTATION
THIRD EDITION
MATT PHARR WENZEL JAKOB GREG HUMPHREYS
AMSTERDAM • BOSTON • HEIDELBERG • LONDON NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO Morgan Kaufmann is an imprint of Elsevier

Morgan Kaufmann is an imprint of Elsevier 50 Hampshire Street, 5th Floor, Cambridge, MA 02139, USA © 2017 Elsevier Inc. All rights reserved. No part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or any information storage and retrieval system, without permission in writing from the publisher. Details on how to seek permission, further information about the Publisher’s permissions policies and our arrangements with organizations such as the Copyright Clearance Center and the Copyright Licensing Agency, can be found at our website: www.elsevier.com/permissions. This book and the individual contributions contained in it are protected under copyright by the Publisher (other than as may be noted herein). Notices Knowledge and best practice in this field are constantly changing. As new research and experience broaden our understanding, changes in research methods, professional practices, or medical treatment may become necessary. Practitioners and researchers must always rely on their own experience and knowledge in evaluating and using any information, methods, compounds, or experiments described herein. In using such information or methods they should be mindful of their own safety and the safety of others, including parties for whom they have a professional responsibility. To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability for any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or from any use or operation of any methods, products, instructions, or ideas contained in the material herein.
Library of Congress Cataloging-in-Publication Data A catalog record for this book is available from the Library of Congress British Library Cataloguing-in-Publication Data A catalogue record for this book is available from the British Library ISBN: 978-0-12-800645-0
For information on all Morgan Kaufmann publications visit our website at https://www.elsevier.com/
Publisher: Todd Green Editorial Project Manager: Jennifer Pierce Production Project Manager: Mohana Natarajan Cover Designer: Victoria Pearson Typeset by: Windfall Software and SPi global

To Deirdre, who even let me bring the manuscript on our honeymoon.

M. P.

To Olesya, who thought it was cute that my favorite book is a computer program. W. J.

To Isabel and Leila, the two most extraordinary people I’ve ever met. May your pixels never be little squares.
G. H.

ABOUT THE AUTHORS
Matt Pharr is a Software Engineer at Google. He previously co-founded Neoptica, which was acquired by Intel, and co-founded Exluna, which was acquired by NVIDIA. He has a B.S. degree from Yale and a Ph.D. from the Stanford Graphics Lab, where he worked under the supervision of Pat Hanrahan.
Wenzel Jakob is an assistant professor in the School of Computer and Communication Sciences at E´ cole Polytechnique Fe´de´rale de Lausanne (EPFL). His research interests revolve around material appearance modeling, rendering algorithms, and the highdimensional geometry of light paths. Wenzel obtained his Ph.D. at Cornell University under the supervision of Steve Marschner, after which he joined ETH Zu¨rich for postdoctoral studies under the supervision of Olga Sorkine Hornung. Wenzel is also the lead developer of the Mitsuba renderer, a research-oriented rendering system.
Greg Humphreys is Director of Engineering at FanDuel, having previously worked on the Chrome graphics team at Google and the OptiX GPU ray-tracing engine at NVIDIA. Before that, he was a professor of Computer Science at the University of Virginia, where he conducted research in both high-performance and physically based computer graphics, as well as computer architecture and visualization. Greg has a B.S.E. degree from Princeton and a Ph.D. in Computer Science from Stanford under the supervision of Pat Hanrahan. When he’s not tracing rays, Greg can usually be found playing tournament bridge.

Contents

PREFACE

xix

CHAPTER 01. INTRODUCTION

1

1.1

Literate Programming

1

1.1.1

Indexing and Cross-Referencing

3

1.2

Photorealistic Rendering and the Ray-Tracing Algorithm

4

1.2.1

Cameras

5

1.2.2

Ray–Object Intersections

7

1.2.3

Light Distribution

8

1.2.4

Visibility

10

1.2.5

Surface Scattering

10

1.2.6

Indirect Light Transport

11

1.2.7

Ray Propagation

13

1.3

pbrt: System Overview

15

1.3.1

Phases of Execution

17

1.3.2

Scene Representation

19

1.3.3

Integrator Interface and SamplerIntegrator

24

1.3.4

The Main Rendering Loop

26

1.3.5

An Integrator for Whitted Ray Tracing

32

1.4

Parallelization of pbrt

38

1.4.1

Data Races and Coordination

39

1.4.2

Conventions in pbrt

42

1.4.3

Thread Safety Expectations in pbrt

43

1.5

How to Proceed through This Book

44

1.5.1

The Exercises

45

1.6

Using and Understanding the Code

45

1.6.1

Pointer or Reference?

45

1.6.2

Abstraction versus Efﬁciency

46

1.6.3

Code Optimization

46

1.6.4

The Book Web site

47

1.6.5

Extending the System

47

1.6.6

Bugs

47

1.7

A Brief History of Physically Based Rendering

48

1.7.1

Research

48

1.7.2

Production

50

Further Reading

53

Exercise

55

viii

CONTENTS

CHAPTER 02. GEOMETRY AND TRANSFORMATIONS 57

2.1

Coordinate Systems

57

2.1.1

Coordinate System Handedness

58

2.2

Vectors

59

2.2.1

Dot and Cross Product

63

2.2.2

Normalization

65

2.2.3

Miscellaneous Operations

66

2.2.4

Coordinate System from a Vector

67

2.3

Points

67

2.4

Normals

71

2.5

Rays

72

2.5.1

Ray Differentials

74

2.6

Bounding Boxes

75

2.7

Transformations

81

2.7.1

Homogeneous Coordinates

82

2.7.2

Basic Operations

84

2.7.3

Translations

85

2.7.4

Scaling

87

2.7.5

x, y, and z Axis Rotations

88

2.7.6

Rotation around an Arbitrary Axis

89

2.7.7

The Look-At Transformation

91

2.8

Applying Transformations

93

2.8.1

Points

93

2.8.2

Vectors

93

2.8.3

Normals

93

2.8.4

Rays

95

2.8.5

Bounding Boxes

95

2.8.6

Composition of Transformations

96

2.8.7

Transformations and Coordinate System Handedness

96

∗ 2.9

Animating Transformations

97

2.9.1

Quaternions

99

2.9.2

Quaternion Interpolation

101

2.9.3

AnimatedTransform Implementation

103

2.9.4

Bounding Moving Bounding Boxes

107

2.10

Interactions

114

2.10.1

Surface Interaction

116

Further Reading

120

Exercises

121

CHAPTER 03. SHAPES

3.1

Basic Shape Interface

3.1.1 3.1.2

Bounding Ray–Bounds Intersections

123
123 124 125

∗ An asterisk denotes a section with advanced content that can be skipped on a ﬁrst reading.

CONTENTS

ix

3.1.3

Intersection Tests

129

3.1.4

Surface Area

130

3.1.5

Sidedness

131

3.2

Spheres

131

3.2.1

Bounding

133

3.2.2

Intersection Tests

134

∗ 3.2.3

Partial Derivatives of Normal Vectors

138

3.2.4

SurfaceInteraction Initialization

140

3.2.5

Surface Area

141

3.3

Cylinders

142

3.3.1

Bounding

143

3.3.2

Intersection Tests

144

3.3.3

Surface Area

146

3.4

Disks

146

3.4.1

Bounding

147

3.4.2

Intersection Tests

148

3.4.3

Surface Area

150

3.5

Other Quadrics

150

3.5.1

Cones

150

3.5.2

Paraboloids

151

3.5.3

Hyperboloids

152

3.6

Triangle Meshes

152

3.6.1

Triangle

155

3.6.2

Triangle Intersection

157

3.6.3

Shading Geometry

166

3.6.4

Surface Area

167

∗ 3.7

Curves

167

∗ 3.8

Subdivision Surfaces

181

3.8.1

Mesh Representation

184

3.8.2

Subdivison

192

∗ 3.9

Managing Rounding Error

206

3.9.1

Floating-Point Arithmetic

208

3.9.2

Conservative Ray–Bounds Intersections

220

3.9.3

Robust Triangle Intersections

221

3.9.4

Bounding Intersection Point Error

222

3.9.5

Robust Spawned Ray Origins

230

3.9.6

Avoiding Intersections Behind Ray Origins

233

3.9.7

Discussion

235

Further Reading

236

Exercises

238

CHAPTER 04. PRIMITIVES AND INTERSECTION ACCELERATION

247

4.1

Primitive Interface and Geometric Primitives

248

4.1.1 4.1.2

Geometric Primitives

250

TransformedPrimitive: Object Instancing and Animated Primitives 251

4.2

Aggregates

254

x

4.3

Bounding Volume Hierarchies

4.3.1 4.3.2 4.3.3 4.3.4 4.3.5

BVH Construction The Surface Area Heuristic Linear Bounding Volume Hierarchies Compact BVH For Traversal Traversal

4.4

Kd-Tree Accelerator

4.4.1 4.4.2 4.4.3

Tree Representation Tree Construction Traversal

Further Reading

Exercises

CHAPTER 05. COLOR AND RADIOMETRY

5.1

Spectral Representation

5.1.1 5.1.2

The Spectrum Type CoefﬁcientSpectrum Implementation

5.2

The SampledSpectrum Class

5.2.1 5.2.2

XYZ Color RGB Color

5.3

RGBSpectrum Implementation

5.4

Radiometry

5.4.1 5.4.2 5.4.3

Basic Quantities Incident and Exitant Radiance Functions Luminance and Photometry

5.5

Working with Radiometric Integrals

5.5.1 5.5.2 5.5.3

Integrals over Projected Solid Angle Integrals over Spherical Coordinates Integrals over Area

5.6

Surface Reﬂection

5.6.1 5.6.2

The BRDF The BSSRDF

Further Reading

Exercises

CHAPTER 06. CAMERA MODELS

6.1

Camera Model

6.1.1

Camera Coordinate Spaces

6.2

Projective Camera Models

6.2.1 6.2.2 6.2.3

Orthographic Camera Perspective Camera The Thin Lens Model and Depth of Field

6.3

Environment Camera

CONTENTS
255 257 263 268 280 282 284 286 288 297 302 308
313
313 315 315 318 322 325 331 334 335 339 341 343 343 344 347 348 349 351 352 353
355
356 358 358 361 365 368 375

CONTENTS

xi

∗ 6.4

Realistic Cameras

377

6.4.1

Lens System Representation

379

6.4.2

Tracing Rays through Lenses

382

6.4.3

The Thick Lens Approximation

386

6.4.4

Focusing

388

6.4.5

The Exit Pupil

389

6.4.6

Generating Rays

394

6.4.7

The Camera Measurement Equation

395

Further Reading

397

Exercises

398

CHAPTER 07. SAMPLING AND RECONSTRUCTION

7.1
7.2
7.3 ∗ 7.4 ∗ 7.5 ∗ 7.6 ∗ 7.7
7.8 7.9

Sampling Theory

7.1.1 7.1.2 7.1.3 7.1.4 7.1.5 7.1.6 7.1.7

The Frequency Domain and the Fourier Transform Ideal Sampling and Reconstruction Aliasing Antialiasing Techniques Application to Image Synthesis Sources of Aliasing in Rendering Understanding Pixels

Sampling Interface

∗ 7.2.1

Evaluating Sample Patterns: Discrepancy

7.2.2

Basic Sampler Interface

7.2.3

Sampler Implementation

7.2.4

Pixel Sampler

7.2.5

Global Sampler

Stratiﬁed Sampling

The Halton Sampler

7.4.1 7.4.2

Hammersley and Halton Sequences Halton Sampler Implementation

(0, 2)-Sequence Sampler

7.5.1 7.5.2

Sampling with Generator Matrices Sampler Implementation

Maximized Minimal Distance Sampler

Sobol’ Sampler

Image Reconstruction

7.8.1

Filter Functions

Film and the Imaging Pipeline

7.9.1 7.9.2 7.9.3

The Film Class Supplying Pixel Values to the Film Image Output

Further Reading

Exercises

401
402 403 405 409 410 413 414 415
416 417 421 425 427 428
432
441 443 450
454 455 462
465
467
472 474
483 484 488 494
496
501

xii

CHAPTER 08. REFLECTION MODELS

8.1

Basic Interface

8.1.1 8.1.2

Reﬂectance BxDF Scaling Adapter

8.2

Specular Reﬂection and Transmission

8.2.1 8.2.2 8.2.3 8.2.4

Fresnel Reﬂectance Specular Reﬂection Specular Transmission Fresnel-Modulated Specular Reﬂection and Transmission

8.3

Lambertian Reﬂection

8.4

Microfacet Models

8.4.1 8.4.2 8.4.3 8.4.4

Oren–Nayar Diffuse Reﬂection Microfacet Distribution Functions Masking and Shadowing The Torrance–Sparrow Model

8.5

Fresnel Incidence Effects

8.6

Fourier Basis BSDFs

8.6.1

Spline Interpolation

Further Reading

Exercises

CONTENTS
507
512 514 515 516 516 523 526 531 532 533 534 537 541 544 549 552 560 563 567

CHAPTER 09. MATERIALS

9.1

BSDFs

9.1.1

BSDF Memory Management

9.2

Material Interface and Implementations

9.2.1 9.2.2 9.2.3 9.2.4 9.2.5

Matte Material Plastic Material Mix Material Fourier Material Additional Materials

9.3

Bump Mapping

Further Reading

Exercises

571
571 576 577 578 579 581 583 584 584 591 592

CHAPTER 10. TEXTURE

10.1 10.2

Sampling and Antialiasing

10.1.1
10.1.2 ∗ 10.1.3

Finding the Texture Sampling Rate Filtering Texture Functions Ray Differentials for Specular Reﬂection and Transmission

Texture Coordinate Generation

10.2.1 10.2.2 10.2.3

2D (u, v) Mapping Spherical Mapping Cylindrical Mapping

597
598 599 604 605
608 610 611 612

CONTENTS

10.3 10.4 10.5 10.6

10.2.4 10.2.5

Planar Mapping 3D Mapping

Texture Interface and Basic Textures

10.3.1 10.3.2 10.3.3 10.3.4

Constant Texture Scale Texture Mix Textures Bilinear Interpolation

Image Texture

10.4.1 10.4.2 10.4.3 10.4.4 ∗ 10.4.5

Texture Memory Management ImageTexture Evaluation MIP Maps Isotropic Triangle Filter Elliptically Weighted Average

Solid and Procedural Texturing

10.5.1 10.5.2 10.5.3

UV Texture Checkerboard Solid Checkerboard

Noise

10.6.1 10.6.2 10.6.3 10.6.4 10.6.5 10.6.6

Perlin Noise Random Polka Dots Noise Idioms and Spectral Synthesis Bumpy and Wrinkled Textures Windy Waves Marble

Further Reading

Exercises

∗ CHAPTER 11. VOLUME SCATTERING

11.1
11.2 11.3 11.4

Volume Scattering Processes

11.1.1 11.1.2 11.1.3 11.1.4

Absorption Emission Out-Scattering and Attenuation In-scattering

Phase Functions

Media

11.3.1 11.3.2 11.3.3

Medium Interactions Homogeneous Medium 3D Grids

The BSSRDF

11.4.1 11.4.2 11.4.3

Separable BSSRDFs Tabulated BSSRDF Subsurface Scattering Materials

Further Reading

Exercises

xiii
613 613 614 615 615 616 617 618 620 622 623 632 634 640 641 642 646 648 649 653 655 660 662 663 664 667
671
672 673 674 676 678 680 684 687 688 690 692 693 696 700 702 703

xiv

CHAPTER 12. LIGHT SOURCES

12.1
12.2 12.3
12.4 12.5 12.6

Light Emission

12.1.1 12.1.2

Blackbody Emitters Standard Illuminants

Light Interface

12.2.1

Visibility Testing

Point Lights

12.3.1 12.3.2 12.3.3

Spotlights Texture Projection Lights Goniophotometric Diagram Lights

Distant Lights

Area Lights

Inﬁnite Area Lights

Further Reading

Exercises

CONTENTS
707
708 709 712 714 717 719 721 724 728 731 733 737 741 744

CHAPTER 13. MONTE CARLO INTEGRATION

13.1 13.2 13.3 ∗ 13.4
13.5 13.6
13.7

Background and Probability Review

13.1.1 13.1.2

Continuous Random Variables Expected Values and Variance

The Monte Carlo Estimator

Sampling Random Variables

13.3.1 13.3.2

The Inversion Method The Rejection Method

Metropolis Sampling

13.4.1 13.4.2 13.4.3 13.4.4 13.4.5

Basic Algorithm Choosing Mutation Strategies Start-up Bias 1D Setting Estimating Integrals with Metropolis Sampling

Transforming between Distributions

13.5.1 13.5.2 13.5.3

Transformation in Multiple Dimensions Polar Coordinates Spherical Coordinates

2D Sampling with Multidimensional Transformations

13.6.1 13.6.2 13.6.3 13.6.4 13.6.5 ∗ 13.6.6 13.6.7

Uniformly Sampling a Hemisphere Sampling a Unit Disk Cosine-Weighted Hemisphere Sampling Sampling a Cone Sampling a Triangle Sampling Cameras Piecewise-Constant 2D Distributions

Russian Roulette and Splitting

13.7.1

Splitting

747
748 749 750
751
753 753 760
762 763 764 766 766 771
771 772 772 773
773 774 776 779 781 781 783 784
787 788

CONTENTS

13.8
13.9 13.10

Careful Sample Placement

13.8.1 13.8.2 13.8.3

Stratiﬁed Sampling Quasi Monte Carlo Warping Samples and Distortion

Bias

Importance Sampling

13.10.1

Multiple Importance Sampling

Further Reading

Exercises

CHAPTER 14. LIGHT TRANSPORT I: SURFACE REFLECTION

14.1
14.2 14.3 14.4
14.5

Sampling Reﬂection Functions

14.1.1 14.1.2 14.1.3 ∗ 14.1.4 14.1.5 14.1.6

Microfacet BxDFs FresnelBlend Specular Reﬂection and Transmission Fourier BSDF Application: Estimating Reﬂectance Sampling BSDFs

Sampling Light Sources

14.2.1 14.2.2 14.2.3 14.2.4

Lights with Singularities Sampling Shapes Area Lights Inﬁnite Area Lights

Direct Lighting

14.3.1

Estimating the Direct Lighting Integral

The Light Transport Equation

14.4.1 14.4.2 14.4.3 14.4.4 14.4.5 14.4.6

Basic Derivation Analytic Solutions to the LTE The Surface Form of the LTE Integral over Paths Delta Distributions in the Integrand Partitioning the Integrand

Path Tracing

14.5.1 14.5.2 14.5.3 14.5.4

Overview Path Sampling Incremental Path Construction Implementation

Further Reading

Exercises

CHAPTER 15. LIGHT TRANSPORT II: VOLUME RENDERING

15.1

The Equation of Transfer

∗ 15.1.1

Generalized Path Space

xv
789 789 792 792 793 794 797 799 801
805
806 807 814 815 817 830 832 835 836 836 845 845 851 856 861 862 863 865 866 868 869 870 872 873 874 875 879 882
887
888 890

xvi
15.2
15.3 ∗ 15.4
∗ 15.5

Sampling Volume Scattering

15.2.1 15.2.2 15.2.3

Homogeneous Medium Heterogeneous Medium Sampling Phase Functions

Volumetric Light Transport

15.3.1

Path Tracing

Sampling Subsurface Reﬂection Functions

15.4.1 15.4.2 15.4.3

Sampling the SeparableBSSRDF Sampling the TabulatedBSSRDF Subsurface Scattering in the Path Tracer

Subsurface Scattering Using the Diffusion Equation

15.5.1 15.5.2 15.5.3 15.5.4 15.5.5 15.5.6 15.5.7 15.5.8 15.5.9

Principle of Similarity Diffusion Theory Monopole Solution Non-classical Diffusion Dipole Solution Beam Solution Single Scattering Term Filling the BSSRDFTable Setting Scattering Properties

Further Reading

Exercises

∗ CHAPTER 16. LIGHT TRANSPORT III: BIDIRECTIONAL METHODS

16.1 16.2 16.3 16.4

The Path-Space Measurement Equation

16.1.1 16.1.2 16.1.3

Sampling Cameras Sampling Light Rays Non-symmetric Scattering

Stochastic Progressive Photon Mapping

16.2.1 16.2.2 16.2.3 16.2.4 16.2.5 16.2.6

Theoretical Basis for Particle Tracing Photon Mapping SPPMIntegrator Accumulating Visible Points Visible Point Grid Construction Accumulating Photon Contributions

Bidirectional Path Tracing

16.3.1 16.3.2 16.3.3 16.3.4 16.3.5

Vertex Abstraction Layer Generating the Camera and Light Subpaths Subpath Connections Multiple Importance Sampling Inﬁnite Area Lights and BDPT

Metropolis Light Transport

16.4.1 16.4.2 16.4.3

Primary Sample Space MLT Multiplexed MLT Application to Rendering

CONTENTS
891 893 894 898 899 900 903 905 913 915 916 917 918 923 924 925 928 930 934 938 939 943
947
948 949 955 960 963 963 966 972 975 979 983 990 995 1003 1008 1012 1019 1022 1023 1025 1025

CONTENTS

16.4.4 16.4.5

Primary Sample Space Sampler MLT Integrator

Further Reading

Exercises

xvii
1028 1035 1042 1046

CHAPTER 17. RETROSPECTIVE AND THE FUTURE

17.1 17.2 17.3

Design Retrospective

17.1.1 17.1.2 17.1.3 17.1.4

Triangles Only Increased Scene Complexity Production Rendering Specialized Compilation

Alternative Hardware Architectures

17.2.1 17.2.2 17.2.3 17.2.4

GPU Ray Tracing Packet Tracing Ray-Tracing Hardware The Future

Conclusion

1051
1051 1052 1053 1054 1054
1055 1056 1057 1059 1059
1060

APPENDIXES

A

UTILITIES

B

SCENE DESCRIPTION INTERFACE

C

INDEX OF FRAGMENTS

D

INDEX OF CLASSES AND THEIR MEMBERS

E

INDEX OF MISCELLANEOUS IDENTIFIERS

1061 1103 1135 1151 1161

REFERENCES SUBJECT INDEX COLOPHON

1165 1213 1235

This page intentionally left blank

Preface
[Just as] other information should be available to those who want to learn and understand, program source code is the only means for programmers to learn the art from their predecessors. It would be unthinkable for playwrights not to allow other playwrights to read their plays [or to allow them] at theater performances where they would be barred even from taking notes. Likewise, any good author is well read, as every child who learns to write will read hundreds of times more than it writes. Programmers, however, are expected to invent the alphabet and learn to write long novels all on their own. Programming cannot grow and learn unless the next generation of programmers has access to the knowledge and information gathered by other programmers before them. —Erik Naggum
Rendering is a fundamental component of computer graphics. At the highest level of abstraction, rendering is the process of converting a description of a three-dimensional scene into an image. Algorithms for animation, geometric modeling, texturing, and other areas of computer graphics all must pass their results through some sort of rendering process so that they can be made visible in an image. Rendering has become ubiquitous; from movies to games and beyond, it has opened new frontiers for creative expression, entertainment, and visualization.
In the early years of the ﬁeld, research in rendering focused on solving fundamental problems such as determining which objects are visible from a given viewpoint. As effective solutions to these problems have been found and as richer and more realistic scene descriptions have become available thanks to continued progress in other areas of graphics, modern rendering has grown to include ideas from a broad range of disciplines, including physics and astrophysics, astronomy, biology, psychology and the study of perception, and pure and applied mathematics. The interdisciplinary nature of rendering is one of the reasons that it is such a fascinating area of study.
This book presents a selection of modern rendering algorithms through the documented source code for a complete rendering system. Nearly all of the images in this book, including the one on the front cover, were rendered by this software. All of the algorithms that came together to generate these images are described in these pages. The system, pbrt, is written using a programming methodology called literate programming that mixes prose describing the system with the source code that implements it. We believe that the literate programming approach is a valuable way to introduce ideas in computer graphics and computer science in general. Often, some of the subtleties of an algorithm can be unclear or hidden until it is implemented, so seeing an actual implementation is a good way to acquire a solid understanding of that algorithm’s details. Indeed, we believe that deep understanding of a small number of algorithms in this manner provides a stronger base for further study of computer graphics than does superﬁcial understanding of many.

xx

PREFACE

In addition to clarifying how an algorithm is implemented in practice, presenting these algorithms in the context of a complete and nontrivial software system also allows us to address issues in the design and implementation of medium-sized rendering systems. The design of a rendering system’s basic abstractions and interfaces has substantial implications for both the elegance of the implementation and the ability to extend it later, yet the trade-offs in this design space are rarely discussed.
pbrt and the contents of this book focus exclusively on photorealistic rendering, which can be deﬁned variously as the task of generating images that are indistinguishable from those that a camera would capture in a photograph or as the task of generating images that evoke the same response from a human observer as looking at the actual scene. There are many reasons to focus on photorealism. Photorealistic images are crucial for the movie special-effects industry because computer-generated imagery must often be mixed seamlessly with footage of the real world. In entertainment applications where all of the imagery is synthetic, photorealism is an effective tool for making the observer forget that he or she is looking at an environment that does not actually exist. Finally, photorealism gives a reasonably well-deﬁned metric for evaluating the quality of the rendering system’s output.

AUDIENCE
There are three main audiences that this book is intended for. The ﬁrst is students in graduate or upper-level undergraduate computer graphics classes. This book assumes existing knowledge of computer graphics at the level of an introductory college-level course, although certain key concepts such as basic vector geometry and transformations will be reviewed here. For students who do not have experience with programs that have tens of thousands of lines of source code, the literate programming style gives a gentle introduction to this complexity. We pay special attention to explaining the reasoning behind some of the key interfaces and abstractions in the system in order to give these readers a sense of why the system is structured in the way that it is.
The second audience is advanced graduate students and researchers in computer graphics. For those doing research in rendering, the book provides a broad introduction to the area, and the pbrt source code provides a foundation that can be useful to build upon (or at least to use bits of source code from). For those working in other areas, we believe that having a thorough understanding of rendering can be helpful context to carry along.
Our ﬁnal audience is software developers in industry. Although many of the ideas in this book will likely be familiar to this audience, seeing explanations of the algorithms presented in the literate style may provide new perspectives. pbrt includes implementations of a number of advanced and/or difﬁcult-to-implement algorithms and techniques, such as subdivision surfaces, Monte Carlo sampling algorithms, bidirectional path tracing, Metropolis sampling, and subsurface scattering; these should be of particular interest to experienced practitioners in rendering. We hope that delving into one particular organization of a complete and nontrivial rendering system will also be thought provoking to this audience.

PREFACE

xxi

OVERVIEW AND GOALS

pbrt is based on the ray-tracing algorithm. Ray tracing is an elegant technique that has its origins in lens making; Carl Friedrich Gauß traced rays through lenses by hand in the 19th century. Ray-tracing algorithms on computers follow the path of inﬁnitesimal rays of light through the scene until they intersect a surface. This approach gives a simple method for ﬁnding the ﬁrst visible object as seen from any particular position and direction and is the basis for many rendering algorithms.
pbrt was designed and implemented with three main goals in mind: it should be complete, it should be illustrative, and it should be physically based.
Completeness implies that the system should not lack key features found in high-quality commercial rendering systems. In particular, it means that important practical issues, such as antialiasing, robustness, numerical precision, and the ability to efﬁciently render complex scenes, should all be addressed thoroughly. It is important to consider these issues from the start of the system’s design, since these features can have subtle implications for all components of the system and can be quite difﬁcult to retroﬁt into the system at a later stage of implementation.
Our second goal means that we tried to choose algorithms, data structures, and rendering techniques with care and with an eye toward readability and clarity. Since their implementations will be examined by more readers than is the case for many other rendering systems, we tried to select the most elegant algorithms that we were aware of and implement them as well as possible. This goal also required that the system be small enough for a single person to understand completely. We have implemented pbrt using an extensible architecture, with the core of the system implemented in terms of a set of carefully designed abstract base classes, and as much of the speciﬁc functionality as possible in implementations of these base classes. The result is that one doesn’t need to understand all of the speciﬁc implementations in order to understand the basic structure of the system. This makes it easier to delve deeply into parts of interest and skip others, without losing sight of how the overall system ﬁts together.
There is a tension between the two goals of being complete and being illustrative. Implementing and describing every possible useful technique would not only make this book unacceptably long, but also would make the system prohibitively complex for most readers. In cases where pbrt lacks a particularly useful feature, we have attempted to design the architecture so that the feature could be added without altering the overall system design.
The basic foundations for physically based rendering are the laws of physics and their mathematical expression. pbrt was designed to use the correct physical units and concepts for the quantities it computes and the algorithms it implements. When conﬁgured to do so, pbrt can compute images that are physically correct; they accurately reﬂect the lighting as it would be in a real-world version of the scene. One advantage of the decision to use a physical basis is that it gives a concrete standard of program correctness: for simple scenes, where the expected result can be computed in closed form, if pbrt doesn’t compute the same result, we know there must be a bug in the implementation.

xxii

PREFACE

Similarly, if different physically based lighting algorithms in pbrt give different results for the same scene, or if pbrt doesn’t give the same results as another physically based renderer, there is certainly an error in one of them. Finally, we believe that this physically based approach to rendering is valuable because it is rigorous. When it is not clear how a particular computation should be performed, physics gives an answer that guarantees a consistent result.
Efﬁciency was given lower priority than these three goals. Since rendering systems often run for many minutes or hours in the course of generating an image, efﬁciency is clearly important. However, we have mostly conﬁned ourselves to algorithmic efﬁciency rather than low-level code optimization. In some cases, obvious micro-optimizations take a backseat to clear, well-organized code, although we did make some effort to optimize the parts of the system where most of the computation occurs.
In the course of presenting pbrt and discussing its implementation, we hope to convey some hard-learned lessons from years of rendering research and development. There is more to writing a good renderer than stringing together a set of fast algorithms; making the system both ﬂexible and robust is a difﬁcult task. The system’s performance must degrade gracefully as more geometry or light sources are added to it or as any other axis of complexity is pushed. Numerical stability must be handled carefully, and algorithms that don’t waste ﬂoating-point precision are critical.
The rewards for developing a system that addresses all these issues are enormous—it is a great pleasure to write a new renderer or add a new feature to an existing renderer and use it to create an image that couldn’t be generated before. Our most fundamental goal in writing this book was to bring this opportunity to a wider audience. Readers are encouraged to use the system to render the example scenes in the pbrt software distribution as they progress through the book. Exercises at the end of each chapter suggest modiﬁcations to the system that will help clarify its inner workings and more complex projects to extend the system by adding new features.
The Web site for this book is located at www.pbrt.org. The latest version of the pbrt source code is available from this site, and we will also post errata and bug ﬁxes, additional scenes to render, and supplemental utilities. Any bugs in pbrt or errors in this text that are not listed at the Web site can be reported to the email address bugs@pbrt.org. We greatly value your feedback!

CHANGES BETWEEN THE FIRST AND SECOND EDITIONS
Six years passed between the publication of the ﬁrst edition of this book in 2004 and the second edition in 2010. In that time, thousands of copies of the book were sold, and the pbrt software was downloaded thousands of times from the book’s Web site. The pbrt user base gave us a signiﬁcant amount of feedback and encouragement, and our experience with the system guided many of the decisions we made in making changes between the version of pbrt presented in the ﬁrst edition and the version in the second edition. In addition to a number of bug ﬁxes, we also made several signiﬁcant design changes and enhancements:

PREFACE

xxiii

. Removal of the plugin architecture. The ﬁrst version of pbrt used a run-time plugin architecture to dynamically load code for implementations of objects like shapes, lights, integrators, cameras, and other objects that were used in the scene currently being rendered. This approach allowed users to extend pbrt with new object types (e.g., new shape primitives) without recompiling the entire rendering system. This approach initially seemed elegant, but it complicated the task of supporting pbrt on multiple platforms and it made debugging more difﬁcult. The only new usage scenario that it truly enabled (binary-only distributions of pbrt or binary plugins) was actually contrary to our pedagogical and open-source goals. Therefore, the plugin architecture was dropped in this edition.
. Removal of the image-processing pipeline. The ﬁrst version of pbrt provided a tonemapping interface that converted high-dynamic-range (HDR) ﬂoating-point output images directly into low-dynamic-range TIFFs for display. This functionality made sense in 2004, as support for HDR images was still sparse. In 2010, however, advances in digital photography had made HDR images commonplace. Although the theory and practice of tone mapping are elegant and worth learning, we decided to focus the new book exclusively on the process of image formation and skip the topic of image display. Interested readers should read the book written by Reinhard et al. (2010) for a thorough and modern treatment of the HDR image display process.
. Task parallelism. Multicore architectures became ubiquitous, and we felt that pbrt would not remain relevant without the ability to scale to the number of locally available cores. We also hoped that the parallel programming implementation details documented in this book would help graphics programmers understand some of the subtleties and complexities in writing scalable parallel code (e.g., choosing appropriate task granularities), which is still a difﬁcult and too infrequently taught topic.
. Appropriateness for “production” rendering. The ﬁrst version of pbrt was intended exclusively as a pedagogical tool and a stepping-stone for rendering research. Indeed, we made a number of decisions in preparing the ﬁrst edition that were contrary to use in a production environment, such as limited support for image-based lighting, no support for motion blur, and a photon mapping implementation that wasn’t robust in the presence of complex lighting. With much improved support for these features as well as support for subsurface scattering and Metropolis light transport, we feel that with the second edition, pbrt became much more suitable for rendering very high-quality images of complex environments.

CHANGES BETWEEN THE SECOND AND THIRD EDITIONS
With the passage of another six years, it was time to update and extend the book and the pbrt system. We continued to learn from readers’ and users’ experiences to better understand which topics were most useful to cover. Further, rendering research continued apace; many parts of the book were due for an update to reﬂect current best practices. We made signiﬁcant improvements on a number of fronts:

xxiv

PREFACE

. Bidirectional light transport. The third version of pbrt now includes a full-featured bidirectional path tracer, including full support for volumetric light transport and multiple importance sampling to weight paths. An all-new Metropolis light transport integrator uses components of the bidirectional path tracer, allowing for a particularly succinct implementation of that algorithm. The foundations of these algorithms were established approximately ﬁfteen years ago; it’s overdue to have solid support for them in pbrt.
. Subsurface scattering. The appearance of many objects—notably, skin and translucent objects—is a result of subsurface light transport. Our implementation of subsurface scattering in the second edition reﬂected the state of the art in the early 2000s; we have thoroughly updated both our BSSRDF models and our subsurface light transport algorithms to reﬂect the progress made in ten subsequent years of research. We now use a considerably more accurate diffusion solution together with a ray-tracing-based sampling technique, removing the need for the costly preprocessing step used in the second edition.
. Numerically robust intersections. The effects of ﬂoating-point round-off error in geometric ray intersection calculations have been a long-standing challenge in ray tracing: they can cause small errors to be present throughout the image. We have focused on this issue and derived conservative (but tight) bounds of this error, which makes our implementation more robust to this issue than previous rendering systems.
. Participating media representation. We have signiﬁcantly improved the way that scattering media are described and represented in the system; this allows for more accurate results with nested scattering media. A new sampling technique enables unbiased rendering of heterogeneous media in a way that cleanly integrates with all of the other parts of the system.
. Measured materials. This edition includes a new technique to represent and evaluate measured materials using a sparse frequency-space basis. This approach is convenient because it allows for exact importance sampling, which was not possible with the representation used in the previous edition.
. Photon mapping. A signiﬁcant step forward for photon mapping algorithms has been the development of variants that don’t require storing all of the photons in memory. We have replaced pbrt’s photon mapping algorithm with an implementation based on stochastic progressive photon mapping, which efﬁciently renders many difﬁcult light transport effects.
. Sample generation algorithms. The distribution of sample values used for numerical integration in rendering algorithms can have a surprisingly large effect on the quality of the ﬁnal results. We have thoroughly updated our treatment of this topic, covering new approaches and efﬁcient implementation techniques in more depth than before.
Many other parts of the system have been improved and updated to reﬂect progress in the ﬁeld: microfacet reﬂection models are treated in more depth, with much better sampling techniques; a new “curve” shape has been added for modeling hair and other ﬁne geometry; and a new camera model that simulates realistic lens systems is now available. Throughout the book, we have made numerous smaller changes to more clearly explain and illustrate the key concepts in physically based rendering systems like pbrt.

PREFACE

xxv

ACKNOWLEDGMENTS

Pat Hanrahan has contributed to this book in more ways than we could hope to acknowledge; we owe a profound debt to him. He tirelessly argued for clean interfaces and ﬁnding the right abstractions to use throughout the system, and his understanding of and approach to rendering deeply inﬂuenced its design. His willingness to use pbrt and this manuscript in his rendering course at Stanford was enormously helpful, particularly in the early years of its life when it was still in very rough form; his feedback throughout this process has been crucial for bringing the text to its current state. Finally, the group of people that Pat helped assemble at the Stanford Graphics Lab, and the open environment that he fostered, made for an exciting, stimulating, and fertile environment. Matt and Greg both feel extremely privileged to have been there.
We owe a debt of gratitude to the many students who used early drafts of this book in courses at Stanford and the University of Virginia between 1999 and 2004. These students provided an enormous amount of feedback about the book and pbrt. The teaching assistants for these courses deserve special mention: Tim Purcell, Mike Cammarano, Ian Buck, and Ren Ng at Stanford, and Nolan Goodnight at Virginia. A number of students in those classes gave particularly valuable feedback and sent bug reports and bug ﬁxes; we would especially like to thank Evan Parker and Phil Beatty. A draft of the manuscript of this book was used in classes taught by Bill Mark and Don Fussell at the University of Texas, Austin, and Raghu Machiraju at Ohio State University; their feedback was invaluable, and we are grateful for their adventurousness in incorporating this system into their courses, even while it was still being edited and revised.
Matt Pharr would like to acknowledge colleagues and co-workers in rendering-related endeavors who have been a great source of education and who have substantially inﬂuenced his approach to writing renderers and his understanding of the ﬁeld. Particular thanks go to Craig Kolb, who provided a cornerstone of Matt’s early computer graphics education through the freely available source code to the rayshade ray-tracing system, and Eric Veach, who has also been generous with his time and expertise. Thanks also to Doug Shult and Stan Eisenstat for formative lessons in mathematics and computer science during high school and college, respectively, and most important to Matt’s parents, for the education they’ve provided and continued encouragement along the way. Finally, thanks also to Nick Triantos, Jayant Kolhe, and NVIDIA for their understanding and support through the ﬁnal stages of the preparation of the ﬁrst edition of the book.
Greg Humphreys is very grateful to all the professors and TAs who tolerated him when he was an undergraduate at Princeton. Many people encouraged his interest in graphics, speciﬁcally Michael Cohen, David Dobkin, Adam Finkelstein, Michael Cox, Gordon Stoll, Patrick Min, and Dan Wallach. Doug Clark, Steve Lyon, and Andy Wolfe also supervised various independent research boondoggles without even laughing once. Once, in a group meeting about a year-long robotics project, Steve Lyon became exasperated and yelled, “Stop telling me why it can’t be done, and ﬁgure out how to do it!”—an impromptu lesson that will never be forgotten. Eric Ristad ﬁred Greg as a summer research assistant after his freshman year (before the summer even began), pawning him off on an unsuspecting Pat Hanrahan and beginning an advising relationship that would span 10 years and both coasts. Finally, Dave Hanson taught Greg that literate programming was

xxvi

PREFACE

a great way to work and that computer programming can be a beautiful and subtle art form.
Wenzel Jakob was excited when the ﬁrst edition of pbrt arrived in his mail during his undergraduate studies in 2004. Needless to say, this had a lasting effect on his career— thus Wenzel would like to begin by thanking his co-authors for inviting him to become a part of third edition of this book. Wenzel is extremely indebted to Steve Marschner, who was his PhD advisor during a fulﬁlling ﬁve years at Cornell University. Steve brought him into the world of research and remains a continuous source of inspiration. Wenzel is also thankful for the guidance and stimulating research environment created by the other members of the graphics group, including Kavita Bala, Doug James, and Bruce Walter. Wenzel spent a wonderful postdoc with Olga Sorkine Hornung who introduced him to geometry processing. Olga’s support for Wenzel’s involvement in this book is deeply appreciated.
For the ﬁrst edition, we are also grateful to Don Mitchell, for his help with understanding some of the details of sampling and reconstruction; Thomas Kollig and Alexander Keller, for explaining the ﬁner points of low-discrepancy sampling; and Christer Ericson, who had a number of suggestions for improving our kd-tree implementation. For the second edition, we’re thankful to Christophe Hery and Eugene d’Eon for helping us with the nuances of subsurface scattering.
For the third edition, we’d especially like to thank Leo Gru¨nschloß for reviewing our sampling chapter; Alexander Keller for suggestions about topics for that chapter; Eric Heitz for extensive help with microfacets (and reviewing our text on that topic); Thiago Ize for thoroughly reviewing the text on ﬂoating-point error; Tom van Bussel for reporting a number of errors in our BSSRDF code; Ralf Habel for reviewing our BSSRDF text; and Toshiya Hachisuka and Anton Kaplanyan for extensive review and comments about our light transport chapters. Discussions with Eric Veach about ﬂoating-point round-off error and ray tracing were extremely helpful to our development of our approach to that topic. We’d also like to thank Per Christensen, Doug Epps, Luca Fascione, Marcos Fajardo, Christiphe Hery, John “Spike” Hughes, Andrew Kensler, Alan King, Chris Kulla, Morgan McGuire, Andy Selle, and Ingo Wald for helpful discussions, suggestions, and pointers to research.
We would also like to thank the book’s reviewers, all of whom had insightful and constructive feedback about the manuscript at various stages of its progress. We’d particularly like to thank the reviewers who provided feedback on both the ﬁrst and second editions of the book: Ian Ashdown, Per Christensen, Doug Epps, Dan Goldman, Eric Haines, Erik Reinhard, Pete Shirley, Peter-Pike Sloan, Greg Ward, and a host of anonymous reviewers. For the second edition, Janne Kontkanen, Nelson Max, Bill Mark, and Eric Tabellion also contributed numerous helpful suggestions.
Many people have contributed to not only pbrt but to our own better understanding of rendering through bug reports, patches, and suggestions about better implementation approaches. A few have made particularly substantial contributions over the years—we would especially like to thank Solomon Boulos, Stephen Chenney, John Danks, Kevin Egan, Volodymyr Kachurovskyi, and Ke Xu.

PREFACE

xxvii

In addition, we would like to thank Rachit Agrawal, Frederick Akalin, Mark Bolstad, Thomas de Bodt, Brian Budge, Mark Colbert, Yunjian Ding, Tao Du, Shaohua Fan, Etienne Ferrier, Nigel Fisher, Jeppe Revall Frisvad, Robert G. Graf, Asbjørn Heid, Keith Jeffery, Greg Johnson, Aaron Karp, Donald Knuth, Martin Kraus, Murat Kurt, Larry Lai, Craig McNaughton, Swaminathan Narayanan, Anders Nilsson, Jens Olsson, Vincent Pegoraro, Srinath Ravichandiran, Se´bastien Speierer, Nils Thuerey, Xiong Wei, Wei-Wei Xu, Arek Zimny, and Matthias Zwicker for their suggestions and bug reports. Finally, we would like to thank the LuxRender developers and the LuxRender community, particularly Terrence Vergauwen, Jean-Philippe Grimaldi, and Asbjørn Heid; it has been a delight to see the rendering system they have built from pbrt’s foundation, and we have learned from reading their source code and implementations of new rendering algorithms.
Special thanks to Martin Preston and Steph Bruning from Framestore for their help with our being able to use a frame from Gravity (image courtesy of Warner Bros. and Framestore), and to Joe Letteri, Dave Gouge, and Luca Fascione from Weta Digital for their help with the frame from The Hobbit: The Battle of the Five Armies (© 2014 Warner Bros. Entertainment Inc. and Metro-Goldwyn-Mayer Pictures Inc. (US, Canada & New Line Foreign Territories), © 2014 Metro-Goldwyn-Mayer Pictures Inc. and Warner Bros. Entertainment Inc. (all other territories). All Rights Reserved.

PRODUCTION
For the production of the ﬁrst edition, we would also like to thank Tim Cox (senior editor), for his willingness to take on this slightly unorthodox project and for both his direction and patience throughout the process. We are very grateful to Elisabeth Beller (project manager), who has gone well beyond the call of duty for this book; her ability to keep this complex project in control and on schedule has been remarkable, and we particularly thank her for the measurable impact she has had on the quality of the ﬁnal result. Thanks also to Rick Camp (editorial assistant) for his many contributions along the way. Paul Anagnostopoulos and Jacqui Scarlott at Windfall Software did the book’s composition; their ability to take the authors’ homebrew literate programming ﬁle format and turn it into high-quality ﬁnal output while also juggling the multiple unusual types of indexing we asked for is greatly appreciated. Thanks also to Ken DellaPenta (copyeditor) and Jennifer McClain (proofreader) as well as to Max Spector at Chen Design (text and cover designer), and Steve Rath (indexer).
For the second edition, we’d like to thank Greg Chalson who talked us into expanding and updating the book; Greg also ensured that Paul Anagnostopoulos at Windfall Software would again do the book’s composition. We’d like to thank Paul again for his efforts in working with this book’s production complexity. Finally, we’d also like to thank Todd Green, Paul Gottehrer, and Heather Scherer at Elsevier.
For the third edition, we’d like to thank Todd Green from Elsevier, who oversaw this goround, and Amy Invernizzi, who kept the train on the rails throughout the process. We were delighted to have Paul Anagnostopoulos at Windfall Software part of this process for a third time; his efforts have been critical to the book’s high production value, which is so important to us.

xxviii

PREFACE

SCENES AND MODELS
Many people and organizations have generously supplied us with scenes and models for use in this book and the pbrt distribution. Their generosity has been invaluable in helping us create interesting example images throughout the text.
The bunny, Buddha, and dragon models are courtesy of the Stanford Computer Graphics Laboratory’s scanning repository. The “killeroo” model is included with permission of Phil Dench and Martin Rezard (3D scan and digital representations by headus, design and clay sculpt by Rezard). The dragon model scan used in Chapters 8 and 9 is courtesy of Christian Schu¨ller, and thanks to Yasutoshi Mori for the sports car used in Chapters 7 and 12. The glass used to illustrate caustics in Figures 16.9 and 16.11 is thanks to Simon Wendsche, and the physically accurate smoke data sets were created by Duc Nguyen and Ron Fedkiw.
The head model used to illustrate subsurface scattering was made available by Inﬁnite Realities, Inc. under a Creative Commons Attribution 3.0 license. Thanks to “Wig42” for the breakfast table scene used in Figure 16.8 and “guismo” for the coffee splash scene used in Figure 15.5; both were was posted to blendswap.com also under a Creative Commons Attribution 3.0 license.
Nolan Goodnight created environment maps with a realistic skylight model, and Paul Debevec provided numerous high dynamic-range environment maps. Thanks also to Bernhard Vogl (dativ.at/lightprobes/) for environment maps that we used in numerous ﬁgures. Marc Ellens provided spectral data for a variety of light sources, and the spectral RGB measurement data for a variety of displays is courtesy of Tom Lianza at X-Rite.
We are most particularly grateful to Guillermo M. Leal Llaguno of Evolucio´ n Visual, www.evvisual.com, who modeled and rendered the San Miguel scene that was featured on the cover of the second edition and is still used in numerous ﬁgures in the book. We would also especially like to thank Marko Dabrovic (www.3lhd.com) and Mihovil Odak at RNA Studios (www.rna.hr), who supplied a bounty of excellent models and scenes, including the Sponza atrium, the Sibenik cathedral, and the Audi TT car model. Many thanks are also due to Florent Boyer (www.ﬂorentboyer.com), who provided the contemporary house scene used in some of the images in Chapter 16.

ABOUT THE COVER
The “Countryside” scene on the cover of the book was created by Jan-Walter Schliep, Burak Kahraman, and Timm Dapper of Laubwerk (www.laubwerk.com). The scene features 23,241 individual plants, with a total of 3.1 billion triangles. (Thanks to object instancing, only 24 million triangles need to be stored in memory.) The pbrt ﬁles that describe the scene geometry require 1.1 GB of on-disk storage. There are a total of 192 texture maps, representing 528 MB of texture data. The scene is one of the example scenes that are available from the pbrt Web site.

PREFACE

xxix

ADDITIONAL READING

Donald Knuth’s article Literate Programming (Knuth 1984) describes the main ideas behind literate programming as well as his web programming environment. The seminal TEX typesetting system was written with web and has been published as a series of books (Knuth 1986; Knuth 1993a). More recently, Knuth has published a collection of graph algorithms in literate format in The Stanford GraphBase (Knuth 1993b). These programs are enjoyable to read and are excellent presentations of their respective algorithms. The Web site www.literateprogramming.com has pointers to many articles about literate programming, literate programs to download, and a variety of literate programming systems; many reﬁnements have been made since Knuth’s original development of the idea.
The only other literate programs we know of that have been published as books are the implementation of the lcc compiler, which was written by Christopher Fraser and David Hanson and published as A Retargetable C Compiler: Design and Implementation (Fraser and Hanson 1995), and Martin Ruckert’s book on the mp3 audio format, Understanding MP3 (Ruckert 2005).

CHAPTER ONE

01 INTRODUCTION
Rendering is the process of producing an image from the description of a 3D scene. Obviously, this is a very broad task, and there are many ways to approach it. Physically based techniques attempt to simulate reality; that is, they use principles of physics to model the interaction of light and matter. While a physically based approach may seem to be the most obvious way to approach rendering, it has only been widely adopted in practice over the past 10 or so years. Section 1.7 at the end of this chapter gives a brief history of physically based rendering and its recent adoption for ofﬂine rendering for movies and for interactive rendering for games. This book describes pbrt, a physically based rendering system based on the ray-tracing algorithm. Most computer graphics books present algorithms and theory, sometimes combined with snippets of code. In contrast, this book couples the theory with a complete implementation of a fully functional rendering system. The source code to the system (as well as example scenes and a collection of data for rendering) can be found on the pbrt Web site, pbrt.org.
1.1 LITERATE PROGRAMMING
While writing the TEX typesetting system, Donald Knuth developed a new programming methodology based on the simple but revolutionary idea that programs should be written more for people’s consumption than for computers’ consumption. He named this methodology literate programming. This book (including the chapter you’re reading now) is a long literate program. This means that in the course of reading this book, you will read the full implementation of the pbrt rendering system, not just a high-level description of it. Literate programs are written in a metalanguage that mixes a document formatting language (e.g., TEX or HTML) and a programming language (e.g., C++). Two separate systems process the program: a “weaver” that transforms the literate program into a
Physically Based Rendering: From Theory To Implementation. http://dx.doi.org/10.1016/B978-0-12-800645-0.50001-4 Copyright © 2017 Elsevier Ltd. All rights reserved.

2

INTRODUCTION

CHAPTER 1

document suitable for typesetting and a “tangler” that produces source code suitable for compilation. Our literate programming system is homegrown, but it was heavily inﬂuenced by Norman Ramsey’s noweb system.
The literate programming metalanguage provides two important features. The ﬁrst is the ability to mix prose with source code. This feature makes the description of the program just as important as its actual source code, encouraging careful design and documentation. Second, the language provides mechanisms for presenting the program code to the reader in an order that is entirely different from the compiler input. Thus, the program can be described in a logical manner. Each named block of code is called a fragment, and each fragment can refer to other fragments by name.
As a simple example, consider a function InitGlobals() that is responsible for initializing all of a program’s global variables:1

void InitGlobals() { nMarbles = 25.7; shoeSize = 13; dielectric = true;
}

Despite its brevity, this function is hard to understand without any context. Why, for example, can the variable nMarbles take on ﬂoating-point values? Just looking at the code, one would need to search through the entire program to see where each variable is declared and how it is used in order to understand its purpose and the meanings of its legal values. Although this structuring of the system is ﬁne for a compiler, a human reader would much rather see the initialization code for each variable presented separately, near the code that actually declares and uses the variable.
In a literate program, one can instead write InitGlobals() like this:

Function Deﬁnitions ≡ void InitGlobals() { Initialize Global Variables 2 }

This deﬁnes a fragment, called Function Deﬁnitions , that contains the deﬁnition of the InitGlobals() function. The InitGlobals() function itself refers to another fragment, Initialize Global Variables . Because the initialization fragment has not yet been deﬁned, we don’t know anything about this function except that it will probably contain assignments to global variables. This is just the right level of abstraction for now, since no variables have been declared yet. When we introduce the global variable shoeSize somewhere later in the program, we can then write

Initialize Global Variables ≡

2

shoeSize = 13;

1 The example code in this section is merely illustrative and is not part of pbrt itself.

SECTION 1.1

LITERATE PROGRAMMING

3

Here we have started to deﬁne the contents of Initialize Global Variables . When the literate program is tangled into source code for compilation, the literate programming system will substitute the code shoeSize = 13; inside the deﬁnition of the InitGlobals() function. Later in the text, we may deﬁne another global variable, dielectric, and we can append its initialization to the fragment:

Initialize Global Variables +≡

2

dielectric = true;

The +≡ symbol after the fragment name shows that we have added to a previously deﬁned fragment. When tangled, the result of these three fragments is the code

void InitGlobals() { shoeSize = 13; dielectric = true;
}

In this way, we can decompose complex functions into logically distinct parts, making them much easier to understand. For example, we can write a complicated function as a series of fragments:

Function Deﬁnitions +≡ void complexFunc(int x, int y, double *values) { Check validity of arguments if (x < y) { Swap parameter values } Do precomputation before loop Loop through and update values array }

Again, the contents of each fragment are expanded inline in complexFunc() for compilation. In the document, we can introduce each fragment and its implementation in turn. This decomposition lets us present code a few lines at a time, making it easier to understand. Another advantage of this style of programming is that by separating the function into logical fragments, each with a single and well-delineated purpose, each one can then be written, veriﬁed, or read independently. In general, we will try to make each fragment less than 10 lines long.

In some sense, the literate programming system is just an enhanced macro substitution package tuned to the task of rearranging program source code. This may seem like a trivial change, but in fact literate programming is quite different from other ways of structuring software systems.

1.1.1 INDEXING AND CROSS-REFERENCING
The following features are designed to make the text easier to navigate. Indices in the page margins give page numbers where the functions, variables, and methods used on that page are deﬁned. Indices at the end of the book collect all of these identiﬁers so that it’s possible to ﬁnd deﬁnitions by name. Appendix C, “Index of Fragments,” lists the pages where each fragment is deﬁned and the pages where it is used. Within the text, a deﬁned

4

INTRODUCTION

CHAPTER 1

fragment name is followed by a list of page numbers on which that fragment is used. For example, a hypothetical fragment deﬁnition such as

A fascinating fragment ≡ nMarbles += .001;

184, 690

indicates that this fragment is used on pages 184 and 690. Occasionally we elide fragments from the printed book that are either boilerplate code or substantially the same as other fragments; when these fragments are used, no page numbers will be listed.

When a fragment is used inside another fragment, the page number on which it is ﬁrst deﬁned appears after the fragment name. For example,

Do something interesting + ≡

500

InitializeSomethingInteresting();

Do something else interesting 486

CleanUp();

indicates that the Do something else interesting fragment is deﬁned on page 486. If the deﬁnition of the fragment is not included in the book, no page number will be listed.

1.2 PHOTOREALISTIC RENDERING AND THE RAY-TRACING ALGORITHM
The goal of photorealistic rendering is to create an image of a 3D scene that is indistinguishable from a photograph of the same scene. Before we describe the rendering process, it is important to understand that in this context the word indistinguishable is imprecise because it involves a human observer, and different observers may perceive the same image differently. Although we will cover a few perceptual issues in this book, accounting for the precise characteristics of a given observer is a very difﬁcult and largely unsolved problem. For the most part, we will be satisﬁed with an accurate simulation of the physics of light and its interaction with matter, relying on our understanding of display technology to present the best possible image to the viewer.
Almost all photorealistic rendering systems are based on the ray-tracing algorithm. Ray tracing is actually a very simple algorithm; it is based on following the path of a ray of light through a scene as it interacts with and bounces off objects in an environment. Although there are many ways to write a ray tracer, all such systems simulate at least the following objects and phenomena:
. Cameras: A camera model determines how and from where the scene is being viewed, including how an image of the scene is recorded on a sensor. Many rendering systems generate viewing rays starting at the camera that are then traced into the scene.
. Ray–object intersections: We must be able to tell precisely where a given ray intersects a given geometric object. In addition, we need to determine certain properties of the object at the intersection point, such as a surface normal or its material. Most ray tracers also have some facility for testing the intersection of a ray with multiple objects, typically returning the closest intersection along the ray.

SECTION 1.2

P H O T O R E A L I S T I C R E N D E R I N G A N D T H E R A Y- T R A C I N G A L G O R I T H M

5

. Light sources: Without lighting, there would be little point in rendering a scene. A ray tracer must model the distribution of light throughout the scene, including not only the locations of the lights themselves but also the way in which they distribute their energy throughout space.
. Visibility: In order to know whether a given light deposits energy at a point on a surface, we must know whether there is an uninterrupted path from the point to the light source. Fortunately, this question is easy to answer in a ray tracer, since we can just construct the ray from the surface to the light, ﬁnd the closest ray–object intersection, and compare the intersection distance to the light distance.
. Surface scattering: Each object must provide a description of its appearance, including information about how light interacts with the object’s surface, as well as the nature of the reradiated (or scattered) light. Models for surface scattering are typically parameterized so that they can simulate a variety of appearances.
. Indirect light transport: Because light can arrive at a surface after bouncing off or passing through other surfaces, it is usually necessary to trace additional rays originating at the surface to fully capture this effect.
. Ray propagation: We need to know what happens to the light traveling along a ray as it passes through space. If we are rendering a scene in a vacuum, light energy remains constant along a ray. Although true vacuums are unusual on Earth, they are a reasonable approximation for many environments. More sophisticated models are available for tracing rays through fog, smoke, the Earth’s atmosphere, and so on.
We will brieﬂy discuss each of these simulation tasks in this section. In the next section, we will show pbrt’s high-level interface to the underlying simulation components and follow the progress of a single ray through the main rendering loop. We will also present the implementation of a surface scattering model based on Turner Whitted’s original raytracing algorithm.

1.2.1 CAMERAS
Nearly everyone has used a camera and is familiar with its basic functionality: you indicate your desire to record an image of the world (usually by pressing a button or tapping a screen), and the image is recorded onto a piece of ﬁlm or an electronic sensor. One of the simplest devices for taking photographs is called the pinhole camera. Pinhole cameras consist of a light-tight box with a tiny hole at one end (Figure 1.1). When the hole is uncovered, light enters this hole and falls on a piece of photographic paper that is afﬁxed to the other end of the box. Despite its simplicity, this kind of camera is still used today, frequently for artistic purposes. Very long exposure times are necessary to get enough light on the ﬁlm to form an image.
Although most cameras are substantially more complex than the pinhole camera, it is a convenient starting point for simulation. The most important function of the camera is to deﬁne the portion of the scene that will be recorded onto the ﬁlm. In Figure 1.1, we can see how connecting the pinhole to the edges of the ﬁlm creates a double pyramid that extends into the scene. Objects that are not inside this pyramid cannot be imaged onto the ﬁlm. Because actual cameras image a more complex shape than a pyramid, we will refer to the region of space that can potentially be imaged onto the ﬁlm as the viewing volume.

6

INTRODUCTION

CHAPTER 1

Film
Figure 1.1: A Pinhole Camera.

Pinhole

Viewing volume

Near plane

Far plane

Eye

Figure 1.2: When we simulate a pinhole camera, we place the ﬁlm in front of the hole at the near plane, and the hole is renamed the eye.
Another way to think about the pinhole camera is to place the ﬁlm plane in front of the pinhole but at the same distance (Figure 1.2). Note that connecting the hole to the ﬁlm deﬁnes exactly the same viewing volume as before. Of course, this is not a practical way to build a real camera, but for simulation purposes it is a convenient abstraction. When the ﬁlm (or image) plane is in front of the pinhole, the pinhole is frequently referred to as the eye.
Now we come to the crucial issue in rendering: at each point in the image, what color value does the camera record? If we recall the original pinhole camera, it is clear that only light rays that travel along the vector between the pinhole and a point on the ﬁlm can contribute to that ﬁlm location. In our simulated camera with the ﬁlm plane in front of the eye, we are interested in the amount of light traveling from the image point to the eye.
Therefore, an important task of the camera simulator is to take a point on the image and generate rays along which incident light will contribute to that image location. Because

SECTION 1.2

P H O T O R E A L I S T I C R E N D E R I N G A N D T H E R A Y- T R A C I N G A L G O R I T H M

7

a ray consists of an origin point and a direction vector, this task is particularly simple for the pinhole camera model of Figure 1.2: it uses the pinhole for the origin and the vector from the pinhole to the near plane as the ray’s direction. For more complex camera models involving multiple lenses, the calculation of the ray that corresponds to a given point on the image may be more involved. (Section 6.4 describes the implementation of such a model.)
With the process of converting image locations to rays completely encapsulated in the camera module, the rest of the rendering system can focus on evaluating the lighting along those rays, and a variety of camera models can be supported. pbrt’s camera abstraction is described in detail in Chapter 6.

1.2.2 RAY–OBJECT INTERSECTIONS
Each time the camera generates a ray, the ﬁrst task of the renderer is to determine which object, if any, that ray intersects ﬁrst and where the intersection occurs. This intersection point is the visible point along the ray, and we will want to simulate the interaction of light with the object at this point. To ﬁnd the intersection, we must test the ray for intersection against all objects in the scene and select the one that the ray intersects ﬁrst. Given a ray r, we ﬁrst start by writing it in parametric form:
r(t) = o + td,
where o is the ray’s origin, d is its direction vector, and t is a parameter whose legal range is (0, ∞). We can obtain a point along the ray by specifying its parametric t value and evaluating the above equation.
It is often easy to ﬁnd the intersection between the ray r and a surface deﬁned by an implicit function F (x , y , z) = 0. We ﬁrst substitute the ray equation into the implicit equation, producing a new function whose only parameter is t. We then solve this function for t and substitute the smallest positive root into the ray equation to ﬁnd the desired point. For example, the implicit equation of a sphere centered at the origin with radius r is
x2 + y2 + z2 − r2 = 0.
Substituting the ray equation, we have
ox + tdx 2 + oy + tdy 2 + oz + tdz 2 − r2 = 0.
All of the values besides t are known, giving us an easily solved quadratic equation in t. If there are no real roots, the ray misses the sphere; if there are roots, the smallest positive one gives the intersection point.
The intersection point alone is not enough information for the rest of the ray tracer; it needs to know certain properties of the surface at the point. First, a representation of the material at the point must be determined and passed along to later stages of the ray-tracing algorithm. Second, additional geometric information about the intersection point will also be required in order to shade the point. For example, the surface normal n is always required. Although many ray tracers operate with only n, more sophisticated rendering systems like pbrt require even more information, such as various partial

8

INTRODUCTION

CHAPTER 1

derivatives of position and surface normal with respect to the local parameterization of the surface.
Of course, most scenes are made up of multiple objects. The brute-force approach would be to test the ray against each object in turn, choosing the minimum positive t value of all intersections to ﬁnd the closest intersection. This approach, while correct, is very slow, even for scenes of modest complexity. A better approach is to incorporate an acceleration structure that quickly rejects whole groups of objects during the ray intersection process. This ability to quickly cull irrelevant geometry means that ray tracing frequently runs in O(I log N ) time, where I is the number of pixels in the image and N is the number of objects in the scene.2 (Building the acceleration structure itself is necessarily at least O(N) time, however.)
pbrt’s geometric interface and implementations of it for a variety of shapes is described in Chapter 3, and the acceleration interface and implementations are shown in Chapter 4.

1.2.3 LIGHT DISTRIBUTION
The ray–object intersection stage gives us a point to be shaded and some information about the local geometry at that point. Recall that our eventual goal is to ﬁnd the amount of light leaving this point in the direction of the camera. In order to do this, we need to know how much light is arriving at this point. This involves both the geometric and radiometric distribution of light in the scene. For very simple light sources (e.g., point lights), the geometric distribution of lighting is a simple matter of knowing the position of the lights. However, point lights do not exist in the real world, and so physically based lighting is often based on area light sources. This means that the light source is associated with a geometric object that emits illumination from its surface. However, we will use point lights in this section to illustrate the components of light distribution; rigorous discussion of light measurement and distribution is the topic of Chapters 5 and 12.
We frequently would like to know the amount of light power being deposited on the differential area surrounding the intersection point (Figure 1.3). We will assume that the point light source has some power associated with it and that it radiates light equally in all directions. This means that the power per area on a unit sphere surrounding the light is /(4π). (These measurements will be explained and formalized in Section 5.4.)
If we consider two such spheres (Figure 1.4), it is clear that the power per area at a point on the larger sphere must be less than the power at a point on the smaller sphere because the same total power is distributed over a larger area. Speciﬁcally, the power per area arriving at a point on a sphere of radius r is proportional to 1/r2. Furthermore, it can be shown that if the tiny surface patch dA is tilted by an angle θ away from the vector from the surface point to the light, the amount of power deposited on dA is proportional

2 Although ray tracing’s logarithmic complexity is often heralded as one of its key strengths, this complexity is typically only true on average. A number of ray-tracing algorithms that have guaranteed logarithmic running time have been published in the computational geometry literature, but these algorithms only work for certain types of scenes and have very expensive preprocessing and storage requirements. Szirmay-Kalos and Ma´ rton provide pointers to the relevant literature (Szirmay-Kalos and Ma´ rton 1998). One consolation is that scenes representing realistic environments generally don’t exhibit this worst-case behavior. In practice, the ray intersection algorithms presented in this book are sublinear, but without expensive preprocessing and huge memory usage it is always possible to construct worst-case scenes where ray tracing runs in O(I N ) time.

SECTION 1.2

P H O T O R E A L I S T I C R E N D E R I N G A N D T H E R A Y- T R A C I N G A L G O R I T H M

9

n r

p

Figure 1.3: Geometric construction for determining the power per area arriving at a point due to a point light source. The distance from the point to the light source is denoted by r.

r1 r2

Figure 1.4: Since the point light radiates light equally in all directions, the same total power is deposited on all spheres centered at the light.

to cos θ. Putting this all together, the differential power per area dE (the differential irradiance) is

dE =

cos θ 4π r2

.

Readers already familiar with basic lighting in computer graphics will notice two familiar laws encoded in this equation: the cosine falloff of light for tilted surfaces mentioned above, and the one-over-r-squared falloff of light with distance.

Scenes with multiple lights are easily handled because illumination is linear: the contribution of each light can be computed separately and summed to obtain the overall contribution.

10

INTRODUCTION

CHAPTER 1

p
Figure 1.5: A light source only deposits energy on a surface if the source is not obscured as seen from the receiving point. The light source on the left illuminates the point p, but the light source on the right does not.
1.2.4 VISIBILITY The lighting distribution described in the previous section ignores one very important component: shadows. Each light contributes illumination to the point being shaded only if the path from the point to the light’s position is unobstructed (Figure 1.5). Fortunately, in a ray tracer it is easy to determine if the light is visible from the point being shaded. We simply construct a new ray whose origin is at the surface point and whose direction points toward the light. These special rays are called shadow rays. If we trace this ray through the environment, we can check to see whether any intersections are found between the ray’s origin and the light source by comparing the parametric t value of any intersections found to the parametric t value along the ray of the light source position. If there is no blocking object between the light and the surface, the light’s contribution is included.
1.2.5 SURFACE SCATTERING We now are able to compute two pieces of information that are vital for proper shading of a point: its location and the incident lighting.3 Now we need to determine how the incident lighting is scattered at the surface. Speciﬁcally, we are interested in the amount of light energy scattered back along the ray that we originally traced to ﬁnd the intersection point, since that ray leads to the camera (Figure 1.6). Each object in the scene provides a material, which is a description of its appearance properties at each point on the surface. This description is given by the bidirectional reﬂectance distribution function (BRDF). This function tells us how much energy is reﬂected
3 Readers already familiar with rendering might object that the discussion in this section considers only direct lighting. Rest assured that pbrt does support global illumination.

SECTION 1.2

P H O T O R E A L I S T I C R E N D E R I N G A N D T H E R A Y- T R A C I N G A L G O R I T H M

11

n

p
Figure 1.6: The Geometry of Surface Scattering. Incident light arriving along direction ωi interacts with the surface at point p and is scattered back toward the camera along direction ωo. The amount of light scattered toward the camera is given by the product of the incident light energy and the BRDF.
from an incoming direction ωi to an outgoing direction ωo. We will write the BRDF at p as fr(p, ωo, ωi). Now, computing the amount of light L scattered back toward the camera is straightforward:
for each light: if light is not blocked: incident_light = light.L(point) amount_reflected = surface.BRDF(hit_point, camera_vector, light_vector) L += amount_reflected * incident_light
Here we are using the symbol L to represent the light; this represents a slightly different unit for light measurement than dE, which was used before. L represents radiance, a unit for measuring light that we will see much of in the following. It is easy to generalize the notion of a BRDF to transmitted light (obtaining a BTDF) or to general scattering of light arriving from either side of the surface. A function that describes general scattering is called a bidirectional scattering distribution function (BSDF). pbrt supports a variety of BSDF models; they are described in Chapter 8. More complex yet is the bidirectional subsurface scattering reﬂectance distribution function (BSSRDF), which models light that exits a surface at a different point than it enters. The BSSRDF is described in Sections 5.6.2, 11.4, and 15.5.
1.2.6 INDIRECT LIGHT TRANSPORT Turner Whitted’s original paper on ray tracing (1980) emphasized its recursive nature, which was the key that made it possible to include indirect specular reﬂection and transmission in rendered images. For example, if a ray from the camera hits a shiny object like a mirror, we can reﬂect the ray about the surface normal at the intersection point and recursively invoke the ray-tracing routine to ﬁnd the light arriving at the point on the

12

INTRODUCTION

CHAPTER 1

Figure 1.7: A Prototypical Example of Early Ray Tracing. Note the use of mirrored and glass objects, which emphasizes the algorithm’s ability to handle these kinds of surfaces.

mirror, adding its contribution to the original camera ray. This same technique can be used to trace transmitted rays that intersect transparent objects. For a long time, most early ray-tracing examples showcased mirrors and glass balls (Figure 1.7) because these types of effects were difﬁcult to capture with other rendering techniques.

In general, the amount of light that reaches the camera from a point on an object is given by the sum of light emitted by the object (if it is itself a light source) and the amount of reﬂected light. This idea is formalized by the light transport equation (also often known as the rendering equation), which says that the outgoing radiance Lo(p, ωo) from a point p in direction ωo is the emitted radiance at that point in that direction, Le(p, ωo), plus the incident radiance from all directions on the sphere S2 around p scaled by the BSDF f (p, ωo, ωi) and a cosine term:

Lo(p, ωo) = Le(p, ωo) + S2 f (p, ωo, ωi) Li(p, ωi) |cos θi| dωi.

(1.1)

We will show a more complete derivation of this equation in Sections 5.6.1 and 14.4. Solving this integral analytically is not possible except for the simplest of scenes, so we must either make simplifying assumptions or use numerical integration techniques.

Whitted’s algorithm simpliﬁes this integral by ignoring incoming light from most direc-
tions and only evaluating Li(p, ωi) for directions to light sources and for the directions of perfect reﬂection and refraction. In other words, it turns the integral into a sum over
a small number of directions.

Whitted’s method can be extended to capture more effects than just perfect mirrors and glass. For example, by tracing many recursive rays near the mirror-reﬂection direction and averaging their contributions, we obtain an approximation of glossy reﬂection. In

SECTION 1.2

P H O T O R E A L I S T I C R E N D E R I N G A N D T H E R A Y- T R A C I N G A L G O R I T H M

13

Diffuse

Shiny metal
Figure 1.8: Recursive ray tracing associates an entire tree of rays with each image location.
fact, we can always recursively trace a ray whenever we hit an object. For example, we can randomly choose a reﬂection direction ωi and weight the contribution of this newly spawned ray by evaluating the BRDF fr(p, ωo, ωi). This simple but powerful idea can lead to very realistic images because it captures all of the interreﬂection of light between objects. Of course, we need to know when to terminate the recursion, and choosing directions completely at random may make the rendering algorithm slow to converge to a reasonable result. These problems can be addressed, however; these issues are the topics of Chapters 13 through 16.
When we trace rays recursively in this manner, we are really associating a tree of rays with each image location (Figure 1.8), with the ray from the camera at the root of this tree. Each ray in this tree can have a weight associated with it; this allows us to model, for example, shiny surfaces that do not reﬂect 100% of the incoming light.
1.2.7 RAY PROPAGATION The discussion so far has assumed that rays are traveling through a vacuum. For example, when describing the distribution of light from a point source, we assumed that the light’s power was distributed equally on the surface of a sphere centered at the light without decreasing along the way. The presence of participating media such as smoke, fog, or dust can invalidate this assumption. These effects are important to simulate: even if we are not making a rendering of a smoke-ﬁlled room, almost all outdoor scenes are affected substantially by participating media. For example, Earth’s atmosphere causes objects that are farther away to appear less saturated (Figure 1.9).
There are two ways in which a participating medium can affect the light propagating along a ray. First, the medium can extinguish (or attenuate) light, either by absorbing it or by scattering it in a different direction. We can capture this effect by computing the transmittance T between the ray origin and the intersection point. The transmittance

14

INTRODUCTION

CHAPTER 1

Figure 1.9: Earth’s Atmosphere Decreases Saturation with Distance. The scene on the top is rendered without simulating this phenomenon, while the scene on the bottom includes an atmospheric model. This sort of atmospheric attenuation is an important depth cue when viewing real scenes and adds a sense of scale to the rendering on the bottom.

SECTION 1.3

pbrt: SYSTEM OVERVIEW

15

Figure 1.10: A Spotlight Shining on a Sphere through Fog. Notice that the shape of the spotlight’s lighting distribution and the sphere’s shadow are clearly visible due to the additional scattering in the participating medium.
tells us how much of the light scattered at the intersection point makes it back to the ray origin.
A participating medium can also add to the light along a ray. This can happen either if the medium emits light (as with a ﬂame) or if the medium scatters light from other directions back along the ray (Figure 1.10). We can ﬁnd this quantity by numerically evaluating the volume light transport equation, in the same way we evaluated the light transport equation to ﬁnd the amount of light reﬂected from a surface. We will leave the description of participating media and volume rendering until Chapters 11 and 15. For now, it will sufﬁce to say that we can compute the effect of participating media and incorporate its effect into the amount of light carried by the ray.
1.3 pbrt: SYSTEM OVERVIEW
pbrt is structured using standard object-oriented techniques: abstract base classes are deﬁned for important entities (e.g., a Shape abstract base class deﬁnes the interface that all geometric shapes must implement, the Light abstract base class acts similarly for lights, etc.). The majority of the system is implemented purely in terms of the interfaces provided by these abstract base classes; for example, the code that checks for occluding objects between a light source and a point being shaded calls the Shape intersection

16

INTRODUCTION

CHAPTER 1

Table 1.1: Main Interface Types. Most of pbrt is implemented in terms of 10 key abstract base classes, listed here. Implementations of each of these can easily be added to the system to extend its functionality.

Base class
Shape Aggregate Camera Sampler Filter Material Texture Medium Light Integrator

Directory
shapes/ accelerators/ cameras/ samplers/ filters/ materials/ textures/ media/ lights/ integrators/

Section
3.1 4.2 6.1 7.2 7.8 9.2 10.3 11.3 12.2 1.3.3

methods and doesn’t need to consider the particular types of shapes that are present in the scene. This approach makes it easy to extend the system, as adding a new shape only requires implementing a class that implements the Shape interface and linking it into the system.
pbrt is written using a total of 10 key abstract base classes, summarized in Table 1.1. Adding a new implementation of one of these types to the system is straightforward; the implementation must inherit from the appropriate base class, be compiled and linked into the executable, and the object creation routines in Appendix B must be modiﬁed to create instances of the object as needed as the scene description ﬁle is parsed. Section B.4 discusses extending the system in this manner in more detail.
The pbrt source code distribution is available from pbrt.org. (A large collection of example scenes is also available as a separate download.) All of the code for the pbrt core is in the src/core directory, and the main() function is contained in the short ﬁle src/main/pbrt.cpp. Various implementations of instances of the abstract base classes are in separate directories: src/shapes has implementations of the Shape base class, src/materials has implementations of Material, and so forth.
Throughout this section are a number of images rendered with extended versions of pbrt. Of them, Figures 1.11 through 1.14 are notable: not only are they visually impressive but also each of them was created by a student in a rendering course where the ﬁnal class project was to extend pbrt with new functionality in order to render an interesting image. These images are among the best from those courses. Figures 1.15 and 1.16 were rendered with LuxRender, a GPL-licensed rendering system originally based on the pbrt source code from the ﬁrst edition of the book. (See www.luxrender.net for more information about LuxRender.)

Aggregate 255 Camera 356 Filter 474 Integrator 25 Light 714 main() 21 Material 577 Medium 684 Sampler 421 Shape 123 Texture 614

SECTION 1.3

pbrt: SYSTEM OVERVIEW

17

Figure 1.11: Guillaume Poncin and Pramod Sharma extended pbrt in numerous ways, implementing a number of complex rendering algorithms, to make this prize-winning image for Stanford’s cs348b rendering competition. The trees are modeled procedurally with L-systems, a glow image processing ﬁlter increases the apparent realism of the lights on the tree, snow was modeled procedurally with metaballs, and a subsurface scattering algorithm gave the snow its realistic appearance by accounting for the effect of light that travels beneath the snow for some distance before leaving it.

Integrator 25 Scene 23

1.3.1 PHASES OF EXECUTION
pbrt can be conceptually divided into two phases of execution. First, it parses the scene description ﬁle provided by the user. The scene description is a text ﬁle that speciﬁes the geometric shapes that make up the scene, their material properties, the lights that illuminate them, where the virtual camera is positioned in the scene, and parameters to all of the individual algorithms used throughout the system. Each statement in the input ﬁle has a direct mapping to one of the routines in Appendix B; these routines comprise the procedural interface for describing a scene. The scene ﬁle format is documented on the pbrt Web site, pbrt.org.
The end results of the parsing phase are an instance of the Scene class and an instance of the Integrator class. The Scene contains a representation of the contents of the scene (geometric objects, lights, etc.), and the Integrator implements an algorithm to render it. The integrator is so-named because its main task is to evaluate the integral from Equation (1.1).

18

INTRODUCTION

CHAPTER 1

Figure 1.12: Abe Davis, David Jacobs, and Jongmin Baek rendered this amazing image of an ice cave to take the grand prize in the 2009 Stanford CS348b rendering competition. They ﬁrst implemented a simulation of the physical process of glaciation, the process where snow falls, melts, and refreezes over the course of many years, forming stratiﬁed layers of ice. They then simulated erosion of the ice due to melted water runoff before generating a geometric model of the ice. Scattering of light inside the volume was simulated with volumetric photon mapping; the blue color of the ice is entirely due to modeling the wavelength-dependent absorption of light in the ice volume.

Once the scene has been speciﬁed, the second phase of execution begins, and the main rendering loop executes. This phase is where pbrt usually spends the majority of its running time, and most of this book describes code that executes during this phase. The rendering loop is performed by executing an implementation of the Integrator::Render() method, which is the focus of Section 1.3.4.
This chapter will describe a particular Integrator subclass named SamplerIntegrator, whose Render() method determines the light arriving at a virtual ﬁlm plane for a large number of rays that model the process of image formation. After the contributions of all of these ﬁlm samples have been computed, the ﬁnal image is written to a ﬁle. The scene

Integrator 25 Integrator::Render() 25 SamplerIntegrator 25

SECTION 1.3

pbrt: SYSTEM OVERVIEW

19

main() 21

Figure 1.13: Lingfeng Yang implemented a bidirectional texture function to simulate the appearance of cloth, adding an analytic self-shadowing model, to render this image that took ﬁrst prize in the 2009 Stanford CS348b rendering competition.
description data in memory are deallocated, and the system then resumes processing statements from the scene description ﬁle until no more remain, allowing the user to specify another scene to be rendered, if desired.
1.3.2 SCENE REPRESENTATION pbrt’s main() function can be found in the ﬁle main/pbrt.cpp. This function is quite simple; it ﬁrst loops over the provided command-line arguments in argv, initializing values in the Options structure and storing the ﬁlenames provided in the arguments. Running pbrt with --help as a command-line argument prints all of the options that can be speciﬁed on the command line. The fragment that parses the command-line argu-

20

INTRODUCTION

CHAPTER 1

Figure 1.14: Jared Jacobs and Michael Turitzin added an implementation of Kajiya and Kay’s texelbased fur rendering algorithm (Kajiya and Kay 1989) to pbrt and rendered this image, where both the fur on the dog and the shag carpet are rendered with the texel fur algorithm.

ments, Process command-line arguments , is straightforward and therefore not included in the book here.
The options structure is then passed to the pbrtInit() function, which does systemwide initialization. The main() function then parses the given scene description(s), leading to the creation of a Scene and an Integrator. After all rendering is done, pbrtCleanup() does ﬁnal cleanup before the system exits.
The pbrtInit() and pbrtCleanup() functions appear in a mini-index in the page margin, along with the number of the page where they are actually deﬁned. The mini-indices have pointers to the deﬁnitions of almost all of the functions, classes, methods, and member variables used or referred to on each page.

Integrator 25 pbrtCleanup() 1109 pbrtInit() 1109 Scene 23

SECTION 1.3

pbrt: SYSTEM OVERVIEW

21

Options 1109 ParseFile() 21 pbrtCleanup() 1109 pbrtInit() 1109

Figure 1.15: This contemporary indoor scene was modeled and rendered by Florent Boyer (www.ﬂorentboyer.com). The image was rendered using LuxRender , a GPL-licensed physicallybased rendering system originally based on pbrt’s source code. Modeling and texturing were done using Blender.

Main program ≡ int main(int argc, char *argv[]) { Options options; std::vector<std::string> filenames; Process command-line arguments pbrtInit(options); Process scene description 21 pbrtCleanup(); return 0; }

If pbrt is run with no input ﬁlenames provided, then the scene description is read from standard input. Otherwise it loops through the provided ﬁlenames, processing each ﬁle in turn.

Process scene description ≡

21

if (filenames.size() == 0) {

Parse scene from standard input 22

} else {

Parse scene from input ﬁles 22

}

The ParseFile() function parses a scene description ﬁle, either from standard input or from a ﬁle on disk; it returns false if it was unable to open the ﬁle. The mechanics of parsing scene description ﬁles will not be described in this book; the parser implementation can be found in the lex and yacc ﬁles core/pbrtlex.ll and core/pbrtparse.y,

22

INTRODUCTION

CHAPTER 1

Figure 1.16: Martin Lubich modeled this scene of the Austrian Imperial Crown and rendered it using LuxRender , an open source fork of the pbrt codebase. The scene was modeled in Blender and consists of approximately 1.8 million vertices. It is illuminated by six area light sources with emission spectra based on measured data from a real-world light source and was rendered with 1280 samples per pixel in 73 hours of computation on a quad-core CPU. See Martin’s Web site, www.loramel.net, for more information, including downloadable Blender scene ﬁles.

respectively. Readers who want to understand the parsing subsystem but are not familiar with these tools may wish to consult Levine, Mason, and Brown (1992).
We use the common UNIX idiom that a ﬁle named “-” represents standard input:

Parse scene from standard input ≡

21

ParseFile("-");

If a particular input ﬁle can’t be opened, the Error() routine reports this information to the user. Error() uses the same format string semantics as printf().

Parse scene from input ﬁles ≡

21

for (const std::string &f : filenames)

if (!ParseFile(f))

Error("Couldn’t open scene file \"%s\"", f.c_str());

As the scene ﬁle is parsed, objects are created that represent the lights and geometric primitives in the scene. These are all stored in the Scene object, which is created by the

Error() 1068 ParseFile() 21 Scene 23

SECTION 1.3

pbrt: SYSTEM OVERVIEW

23

Light 714 Material 577 Primitive 248 RenderOptions::MakeScene()
1130 Scene 23 Scene::aggregate 23 Scene::lights 23 Shape 123

RenderOptions::MakeScene() method in Section B.3.7 in Appendix B. The Scene class is declared in core/scene.h and deﬁned in core/scene.cpp.
Scene Declarations ≡ class Scene { public: Scene Public Methods 23 Scene Public Data 23 private: Scene Private Data 23 };

Scene Public Methods ≡

23

Scene(std::shared_ptr<Primitive> aggregate,

const std::vector<std::shared_ptr<Light>> &lights)

: lights(lights), aggregate(aggregate) {

Scene Constructor Implementation 24

}

Each light source in the scene is represented by a Light object, which speciﬁes the shape of a light and the distribution of energy that it emits. The Scene stores all of the lights using a vector of shared_ptr instances from the C++ standard library. pbrt uses shared pointers to track how many times objects are referenced by other instances. When the last instance holding a reference (the Scene in this case) is destroyed, the reference count reaches zero and the Light can be safely freed, which happens automatically at that point.

While some renderers support separate light lists per geometric object, allowing a light to illuminate only some of the objects in the scene, this idea does not map well to the physically based rendering approach taken in pbrt, so pbrt only supports a single global per-scene list. Many parts of the system need access to the lights, so the Scene makes them available as a public member variable.

Scene Public Data ≡

23

std::vector<std::shared_ptr<Light>> lights;

Each geometric object in the scene is represented by a Primitive, which combines two objects: a Shape that speciﬁes its geometry, and a Material that describes its appearance (e.g., the object’s color, whether it has a dull or glossy ﬁnish). All of the geometric primitives are collected into a single aggregate Primitive in the Scene member variable Scene::aggregate. This aggregate is a special kind of primitive that itself holds references to many other primitives. Because it implements the Primitive interface it appears no different from a single primitive to the rest of the system. The aggregate implementation stores all the scene’s primitives in an acceleration data structure that reduces the number of unnecessary ray intersection tests with primitives that are far away from a given ray.

Scene Private Data ≡

23

std::shared_ptr<Primitive> aggregate;

The constructor caches the bounding box of the scene geometry in the worldBound member variable.

24

INTRODUCTION

CHAPTER 1

Scene Constructor Implementation ≡

23

worldBound = aggregate->WorldBound();

Scene Private Data +≡

23

Bounds3f worldBound;

The bound is made available via the WorldBound() method.

Scene Public Methods +≡

23

const Bounds3f &WorldBound() const { return worldBound; }

Some Light implementations ﬁnd it useful to do some additional initialization after the scene has been deﬁned but before rendering begins. The Scene constructor calls their Preprocess() methods to allow them to do so.

Scene Constructor Implementation +≡

23

for (const auto &light : lights)

light->Preprocess(*this);

The Scene class provides two methods related to ray–primitive intersection. Its Intersect() method traces the given ray into the scene and returns a Boolean value indicating whether the ray intersected any of the primitives. If so, it ﬁlls in the provided SurfaceInteraction structure with information about the closest intersection point along the ray. The SurfaceInteraction structure is deﬁned in Section 4.1.
Scene Method Deﬁnitions ≡ bool Scene::Intersect(const Ray &ray, SurfaceInteraction *isect) const { return aggregate->Intersect(ray, isect); }

A closely related method is Scene::IntersectP(), which checks for the existence of intersections along the ray but does not return any information about those intersections. Because this routine doesn’t need to search for the closest intersection or compute any additional information about intersections, it is generally more efﬁcient than Scene::Intersect(). This routine is used for shadow rays.
Scene Method Deﬁnitions +≡ bool Scene::IntersectP(const Ray &ray) const { return aggregate->IntersectP(ray); }
1.3.3 INTEGRATOR INTERFACE AND SamplerIntegrator
Rendering an image of the scene is handled by an instance of a class that implements the Integrator interface. Integrator is an abstract base class that deﬁnes the Render() method that must be provided by all integrators. In this section, we will deﬁne one Integrator implementation, the SamplerIntegrator. The basic integrator interfaces are deﬁned in core/integrator.h, and some utility functions used by integrators are in core/integrator.cpp. The implementations of the various integrators are in the integrators directory.

Bounds3f 76 Integrator 25 Light::Preprocess() 717 Primitive::Intersect() 249 Primitive::IntersectP() 249 Primitive::WorldBound() 249 Ray 73 SamplerIntegrator 25 Scene 23 Scene::aggregate 23 Scene::Intersect() 24 Scene::IntersectP() 24 Scene::lights 23 Scene::worldBound 24 SurfaceInteraction 116

SECTION 1.3

pbrt: SYSTEM OVERVIEW

25

Camera 356 Film 484 Integrator 25 Sampler 421 SamplerIntegrator 25 Scene 23 WhittedIntegrator 32

Integrator Declarations ≡ class Integrator { public: Integrator Interface 25 };

The method that Integrators must provide is Render(); it is passed a reference to the Scene to use to compute an image of the scene or more generally, a set of measurements of the scene lighting. This interface is intentionally kept very general to permit a wide range of implementations—for example, one could implement an Integrator that takes measurements only at a sparse set of positions distributed through the scene rather than generating a regular 2D image.

Integrator Interface ≡

25

virtual void Render(const Scene &scene) = 0;

In this chapter, we’ll focus on SamplerIntegrator, which is an Integrator subclass, and the WhittedIntegrator, which implements the SamplerIntegrator interface. (Implementations of other SamplerIntegrators will be introduced in Chapters 14 and 15; the integrators in Chapter 16 inherit directly from Integrator.) The name of the Sampler Integrator derives from the fact that its rendering process is driven by a stream of samples from a Sampler; each such sample identiﬁes a point on the image at which the integrator should compute the arriving light to form the image.

SamplerIntegrator Declarations ≡ class SamplerIntegrator : public Integrator { public: SamplerIntegrator Public Methods 26 protected: SamplerIntegrator Protected Data 26 private: SamplerIntegrator Private Data 25 };

The SamplerIntegrator stores a pointer to a Sampler. The role of the sampler is subtle, but its implementation can substantially affect the quality of the images that the system generates. First, the sampler is responsible for choosing the points on the image plane from which rays are traced. Second, it is responsible for supplying the sample positions used by integrators for estimating the value of the light transport integral, Equation (1.1). For example, some integrators need to choose random points on light sources to compute illumination from area lights. Generating a good distribution of these samples is an important part of the rendering process that can substantially affect overall efﬁciency; this topic is the main focus of Chapter 7.

SamplerIntegrator Private Data ≡

25

std::shared_ptr<Sampler> sampler;

The Camera object controls the viewing and lens parameters such as position, orientation, focus, and ﬁeld of view. A Film member variable inside the Camera class handles image storage. The Camera classes are described in Chapter 6, and Film is described in

26

INTRODUCTION

CHAPTER 1

Section 7.9. The Film is responsible for writing the ﬁnal image to a ﬁle and possibly displaying it on the screen as it is being computed.

SamplerIntegrator Protected Data ≡

25

std::shared_ptr<const Camera> camera;

The SamplerIntegrator constructor stores pointers to these objects in member variables. The SamplerIntegrator is created in the RenderOptions::MakeIntegrator() method, which is in turn called by pbrtWorldEnd(), which is called by the input ﬁle parser when it is done parsing a scene description from an input ﬁle and is ready to render the scene.

SamplerIntegrator Public Methods ≡

25

SamplerIntegrator(std::shared_ptr<const Camera> camera,

std::shared_ptr<Sampler> sampler)

: camera(camera), sampler(sampler) { }

SamplerIntegrator implementations may optionally implement the Preprocess() method. It is called after the Scene has been fully initialized and gives the integrator a chance to do scene-dependent computation, such as allocating additional data structures that are dependent on the number of lights in the scene, or precomputing a rough representation of the distribution of radiance in the scene. Implementations that don’t need to do anything along these lines can leave this method unimplemented.

SamplerIntegrator Public Methods +≡

25

virtual void Preprocess(const Scene &scene, Sampler &sampler) { }

1.3.4 THE MAIN RENDERING LOOP
After the Scene and the Integrator have been allocated and initialized, the Integrator:: Render() method is invoked, starting the second phase of pbrt’s execution: the main rendering loop. In the SamplerIntegrator’s implementation of this method, at each of a series of positions on the image plane, the method uses the Camera and the Sampler to generate a ray into the scene and then uses the Li() method to determine the amount of light arriving at the image plane along that ray. This value is passed to the Film, which records the light’s contribution. Figure 1.17 summarizes the main classes used in this method and the ﬂow of data among them.
SamplerIntegrator Method Deﬁnitions ≡ void SamplerIntegrator::Render(const Scene &scene) { Preprocess(scene, *sampler); Render image tiles in parallel 27 Save ﬁnal image after rendering 32 }
So that rendering can proceed in parallel on systems with multiple processing cores, the image is decomposed into small tiles of pixels. Each tile can be processed independently and in parallel. The ParallelFor() function, which is described in more detail in Section A.6, implements a parallel for loop, where multiple iterations may run in parallel. A C++ lambda expression provides the loop body. Here, a variant of ParallelFor() that loops over a 2D domain is used to iterate over the image tiles.

Camera 356 Film 484 Integrator 25 Integrator::Render() 25 pbrtWorldEnd() 1129
RenderOptions:: MakeIntegrator() 1130
Sampler 421 SamplerIntegrator 25 SamplerIntegrator::camera 26
SamplerIntegrator:: Preprocess() 26
SamplerIntegrator::sampler 25 Scene 23

SECTION 1.3

pbrt: SYSTEM OVERVIEW

27

SSaammpplelerr Sample

Camera
Sample Ray

SamplerIntegrator::Render()
Sample, Radiance

Ray
SamplerIntegrator::Li()
Radiance

Figure 1.17: Class Relationships for the Main Rendering Loop in the SamplerIntegrator:: Render() Method in core/integrator.cpp. The Sampler provides a sequence of sample values, one for each image sample to be taken. The Camera turns a sample into a corresponding ray from the ﬁlm plane, and the Li() method implementation computes the radiance along that ray arriving at the ﬁlm. The sample and its radiance are given to the Film, which stores their contribution in an image. This process repeats until the Sampler has provided as many samples as are necessary to generate the ﬁnal image.

Camera 356 Film 484 Film::GetSampleBounds() 487 ParallelFor2D() 1093 Point2i 68 Sampler 421 SamplerIntegrator::Render()
26

Render image tiles in parallel ≡

26

Compute number of tiles, nTiles, to use for parallel rendering 28

ParallelFor2D(

[&](Point2i tile) {

Render section of image corresponding to tile 28

}, nTiles);

There are two factors to trade off in deciding how large to make the image tiles: loadbalancing and per-tile overhead. On one hand, we’d like to have signiﬁcantly more tiles than there are processors in the system: consider a four-core computer with only four tiles. In general, it’s likely that some of the tiles will take less processing time than others; the ones that are responsible for parts of the image where the scene is relatively simple will usually take less processing time than parts of the image where the scene is relatively complex. Therefore, if the number of tiles was equal to the number of processors, some processors would ﬁnish before others and sit idle while waiting for the processor that had the longest running tile. Figure 1.18 illustrates this issue; it shows the distribution of execution time for the tiles used to render the shiny sphere scene in Figure 1.7. The longest running one took 151 times longer than the shortest one.
On the other hand, having tiles that are too small is also inefﬁcient. There is a small ﬁxed overhead for a processing core to determine which loop iteration it should run next; the more tiles there are, the more times this overhead must be paid.
For simplicity, pbrt always uses 16 × 16 tiles; this granularity works well for almost all images, except for very low-resolution ones. We implicitly assume that the small image case isn’t particularly important to render at maximum efﬁciency. The Film’s GetSampleBounds() method returns the extent of pixels over which samples must be generated for the image being rendered. The addition of tileSize - 1 in the computation of nTiles results in a number of tiles that is rounded to the next higher integer when the

28
150 100 50

INTRODUCTION

CHAPTER 1

0.02

0.04

0.06

0.08

0.10

0.12

Figure 1.18: Histogram of Time Spent Rendering Each Tile for the Scene in Figure 1.7. The horizontal axis measures time in seconds. Note the wide variation in execution time, illustrating that different parts of the image required substantially different amounts of computation.

sample bounds along an axis are not exactly divisible by 16. This means that the lambda function invoked by ParallelFor() must be able to deal with partial tiles containing some unused pixels.

Compute number of tiles, nTiles, to use for parallel rendering ≡

27

Bounds2i sampleBounds = camera->film->GetSampleBounds();

Vector2i sampleExtent = sampleBounds.Diagonal();

const int tileSize = 16;

Point2i nTiles((sampleExtent.x + tileSize - 1) / tileSize,

(sampleExtent.y + tileSize - 1) / tileSize);

When the parallel for loop implementation that is deﬁned in Appendix A.6.4 decides to run a loop iteration on a particular processor, the lambda will be called with the tile’s coordinates. It starts by doing a little bit of setup work, determining which part of the ﬁlm plane it is responsible for and allocating space for some temporary data before using the Sampler to generate image samples, the Camera to determine corresponding rays leaving the ﬁlm plane, and the Li() method to compute radiance along those rays arriving at the ﬁlm.

Render section of image corresponding to tile ≡

27

Allocate MemoryArena for tile 29

Get sampler instance for tile 29

Compute sample bounds for tile 29

Get FilmTile for tile 30

Loop over pixels in tile to render them 30

Merge image tile into Film 32

Bounds2::Diagonal() 80 Bounds2i 76 Camera 356 Camera::film 356 Film::GetSampleBounds() 487 ParallelFor() 1088 Point2i 68 Sampler 421 SamplerIntegrator::camera 26 Vector2i 60

SECTION 1.3

pbrt: SYSTEM OVERVIEW

29

Bounds2i 76 Film 484 FilmTile 489 MemoryArena 1074 Point2i 68 Sampler 421 Sampler::Clone() 424

Implementations of the Li() method will generally need to temporarily allocate small amounts of memory for each radiance computation. The large number of resulting allocations can easily overwhelm the system’s regular memory allocation routines (e.g., malloc() or new), which must maintain and synchronize elaborate internal data structures to track sets of free memory regions among processors. A naive implementation could potentially spend a fairly large fraction of its computation time in the memory allocator.
To address this issue, we will pass an instance of the MemoryArena class to the Li() method. MemoryArena instances manage pools of memory to enable higher performance allocation than what is possible with the standard library routines.
The arena’s memory pool is always released in its entirety, which removes the need for complex internal data structures. Instances of this class can only be used by a single thread—concurrent access without additional synchronization is not permitted. We create a unique MemoryArena for each loop iteration that can be used directly, which also ensures that the arena is only accessed by a single thread.

Allocate MemoryArena for tile ≡

28

MemoryArena arena;

Most Sampler implementations ﬁnd it useful to maintain some state, such as the coordinates of the current pixel being sampled. This means that multiple processing threads cannot use a single Sampler concurrently. Therefore, Samplers provide a Clone() method to create a new instance of a given Sampler; it takes a seed that is used by some implementations to seed a pseudo-random number generator so that the same sequence of pseudo-random numbers isn’t generated in every tile. (Note that not all Samplers use pseudo-random numbers; those that don’t just ignore the seed.)

Get sampler instance for tile ≡

28

int seed = tile.y * nTiles.x + tile.x;

std::unique_ptr<Sampler> tileSampler = sampler->Clone(seed);

Next, the extent of pixels to be sampled in this loop iteration is computed based on the tile indices. Two issues must be accounted for in this computation: ﬁrst, the overall pixel bounds to be sampled may not be equal to the full image resolution. For example, the user may have speciﬁed a “crop window” of just a subset of pixels to sample. Second, if the image resolution isn’t an exact multiple of 16, then the tiles on the right and bottom images won’t be a full 16 × 16.

Compute sample bounds for tile ≡

28

int x0 = sampleBounds.pMin.x + tile.x * tileSize;

int x1 = std::min(x0 + tileSize, sampleBounds.pMax.x);

int y0 = sampleBounds.pMin.y + tile.y * tileSize;

int y1 = std::min(y0 + tileSize, sampleBounds.pMax.y);

Bounds2i tileBounds(Point2i(x0, y0), Point2i(x1, y1));

Finally, a FilmTile is acquired from the Film. This class provides a small buffer of memory to store pixel values for the current tile. Its storage is private to the loop iteration, so

30

INTRODUCTION

CHAPTER 1

pixel values can be updated without worrying about other threads concurrently modifying the same pixels. The tile is merged into the ﬁlm’s storage once the work for rendering it is done; serializing concurrent updates to the image is handled then.

Get FilmTile for tile ≡

28

std::unique_ptr<FilmTile> filmTile =

camera->film->GetFilmTile(tileBounds);

Rendering can now proceed. The implementation loops over all of the pixels in the tile using a range-based for loop that automatically uses iterators provided by the Bounds2 class. The cloned Sampler is notiﬁed that it should start generating samples for the current pixel, and samples are processed in turn until StartNextSample() returns false. (As we’ll see in Chapter 7, taking multiple samples per pixel can greatly improve ﬁnal image quality.)

Loop over pixels in tile to render them ≡

28

for (Point2i pixel : tileBounds) {

tileSampler->StartPixel(pixel);

do {

Initialize CameraSample for current sample 30

Generate camera ray for current sample 31

Evaluate radiance along camera ray 31

Add camera ray’s contribution to image 32

Free MemoryArena memory from computing image sample value 32

} while (tileSampler->StartNextSample());

}

The CameraSample structure records the position on the ﬁlm for which the camera should generate the corresponding ray. It also stores time and lens position sample values, which are used when rendering scenes with moving objects and for camera models that simulate non-pinhole apertures, respectively.

Initialize CameraSample for current sample ≡

30

CameraSample cameraSample = tileSampler->GetCameraSample(pixel);

The Camera interface provides two methods to generate rays: Camera::GenerateRay(), which returns the ray for a given image sample position, and Camera::GenerateRay Differential(), which returns a ray differential, which incorporates information about the rays that the Camera would generate for samples that are one pixel away on the image plane in both the x and y directions. Ray differentials are used to get better results from some of the texture functions deﬁned in Chapter 10, making it possible to compute how quickly a texture varies with respect to the pixel spacing, a key component of texture antialiasing.

After the ray differential has been returned, the ScaleDifferentials() method scales the differential rays to account for the actual spacing between samples on the ﬁlm plane for the case where multiple samples are taken per pixel.

The camera also returns a ﬂoating-point weight associated with the ray. For simple camera models, each ray is weighted equally, but camera models that more accurately model the process of image formation by lens systems may generate some rays that

Bounds2 76 Camera 356 Camera::film 356 Camera::GenerateRay() 357 Camera::
GenerateRayDifferential() 357 CameraSample 357 Film::GetFilmTile() 488 FilmTile 489 MemoryArena 1074 Point2i 68 Sampler::GetCameraSample() 423 Sampler::StartNextSample() 424 Sampler::StartPixel() 422 SamplerIntegrator::camera 26

SECTION 1.3

pbrt: SYSTEM OVERVIEW

31

Camera:: GenerateRayDifferential() 357
Float 1062 MemoryArena 1074 RayDifferential 75
RayDifferential:: ScaleDifferentials() 75
Sampler 421 Sampler::samplesPerPixel 422 SamplerIntegrator 25 SamplerIntegrator::camera 26 SamplerIntegrator::Li() 31 Scene 23 Spectrum 315

contribute more than others. Such a camera model might simulate the effect of less light arriving at the edges of the ﬁlm plane than at the center, an effect called vignetting. The returned weight will be used later to scale the ray’s contribution to the image.

Generate camera ray for current sample ≡

30

RayDifferential ray;

Float rayWeight = camera->GenerateRayDifferential(cameraSample, &ray);

ray.ScaleDifferentials(1 / std::sqrt(tileSampler->samplesPerPixel));

Note the capitalized ﬂoating-point type Float: depending on the compilation ﬂags of pbrt, this is an alias for either float or double. More detail on this design choice is provided in Section A.1.
Given a ray, the next task is to determine the radiance arriving at the image plane along that ray. The Li() method takes care of this task.

Evaluate radiance along camera ray ≡

30

Spectrum L(0.f);

if (rayWeight > 0)

L = Li(ray, scene, *tileSampler, arena);

Issue warning if unexpected radiance value is returned

Li() is a pure virtual method that returns the incident radiance at the origin of a given ray; each subclass of SamplerIntegrator must provide an implementation of this method. The parameters to Li() are the following:

. ray: the ray along which the incident radiance should be evaluated. . scene: the Scene being rendered. The implementation will query the scene for infor-
mation about the lights and geometry, and so on. . sampler: a sample generator used to solve the light transport equation via Monte
Carlo integration. . arena: a MemoryArena for efﬁcient temporary memory allocation by the integrator.
The integrator should assume that any memory it allocates with the arena will be freed shortly after the Li() method returns and thus should not use the arena to allocate any memory that must persist for longer than is needed for the current ray. . depth: the number of ray bounces from the camera that have occurred up until the current call to Li().

The method returns a Spectrum that represents the incident radiance at the origin of the ray:

SamplerIntegrator Public Methods +≡

25

virtual Spectrum Li(const RayDifferential &ray, const Scene &scene,

Sampler &sampler, MemoryArena &arena, int depth = 0) const = 0;

A common side effect of bugs in the rendering process is that impossible radiance values are computed. For example, division by zero results in radiance values equal either to the IEEE ﬂoating-point inﬁnity or “not a number” value. The renderer looks for this possibility, as well as for spectra with negative contributions, and prints an error message when it encounters them. Here we won’t include the fragment that does this, Issue warning if

32

INTRODUCTION

CHAPTER 1

unexpected radiance value is returned . See the implementation in core/integrator.cpp if you’re interested in its details.
After the radiance arriving at the ray’s origin is known, the image can be updated: the FilmTile::AddSample() method updates the pixels in the tile’s image given the results from a sample. The details of how sample values are recorded in the ﬁlm are explained in Sections 7.8 and 7.9.

Add camera ray’s contribution to image ≡

30

filmTile->AddSample(cameraSample.pFilm, L, rayWeight);

After processing a sample, all of the allocated memory in the MemoryArena is freed together when MemoryArena::Reset() is called. (See Section 9.1.1 for an explanation of how the MemoryArena is used to allocate memory to represent BSDFs at intersection points.)

Free MemoryArena memory from computing image sample value ≡

30

arena.Reset();

Once radiance values for all of the samples in a tile have been computed, the FilmTile is handed off to the Film’s MergeFilmTile() method, which handles adding the tile’s pixel contributions to the ﬁnal image. Note that the std::move() function is used to transfer ownership of the unique_ptr to MergeFilmTile().

Merge image tile into Film ≡

28

camera->film->MergeFilmTile(std::move(filmTile));

After all of the loop iterations have ﬁnished, the SamplerIntegrator’s Render() method calls the Film’s WriteImage() method to write the image out to a ﬁle.

Save ﬁnal image after rendering ≡

26

camera->film->WriteImage();

1.3.5 AN INTEGRATOR FOR WHITTED RAY TRACING
Chapters 14 and 15 include the implementations of many different integrators, based on a variety of algorithms with differing levels of accuracy. Here we will present an integrator based on Whitted’s ray-tracing algorithm. This integrator accurately computes reﬂected and transmitted light from specular surfaces like glass, mirrors, and water, although it doesn’t account for other types of indirect lighting effects like light bouncing off a wall and illuminating a room. The WhittedIntegrator class can be found in the integrators/whitted.h and integrators/whitted.cpp ﬁles in the pbrt distribution.
WhittedIntegrator Declarations ≡ class WhittedIntegrator : public SamplerIntegrator { public: WhittedIntegrator Public Methods 33 private: WhittedIntegrator Private Data 33 };

Camera::film 356 CameraSample::pFilm 357 Film 484 Film::MergeFilmTile() 493 Film::WriteImage() 494 FilmTile::AddSample() 490 MemoryArena 1074 MemoryArena::Reset() 1076 SamplerIntegrator 25 SamplerIntegrator::camera 26 WhittedIntegrator 32 WriteImage() 1068

SECTION 1.3

pbrt: SYSTEM OVERVIEW

33

Scene

Ray

Interaction

Ray

BSDF

SurfaceInteraction::

Integrator::Li()

ComputeScatteringFunctions()

Radiance

Radiance

Radiance

Light::Sample_Li()

Light::Sample_Li()

Figure 1.19: Class Relationships for Surface Integration. The main rendering loop in the SamplerIntegrator computes a camera ray and passes it to the Li() method, which returns the radiance along that ray arriving at the ray’s origin. After ﬁnding the closest intersection, it computes the material properties at the intersection point, representing them in the form of a BSDF. It then uses the Lights in the Scene to determine the illumination there. Together, these give the information needed to compute the radiance reﬂected back along the ray at the intersection point.

Camera 356 MemoryArena 1074 RayDifferential 75 Sampler 421 SamplerIntegrator 25 Scene 23 Scene::Intersect() 24 Spectrum 315 WhittedIntegrator 32 WhittedIntegrator::maxDepth
33

WhittedIntegrator Public Methods ≡

32

WhittedIntegrator(int maxDepth, std::shared_ptr<const Camera> camera,

std::shared_ptr<Sampler> sampler)

: SamplerIntegrator(camera, sampler), maxDepth(maxDepth) { }

The Whitted integrator works by recursively evaluating radiance along reﬂected and refracted ray directions. It stops the recursion at a predetermined maximum depth, WhittedIntegrator::maxDepth. By default, the maximum recursion depth is ﬁve. Without this termination criterion, the recursion might never terminate (imagine, e.g., a hall-of-mirrors scene). This member variable is initialized in the WhittedIntegrator constructor (not included here), based on parameters set in the scene description ﬁle.

WhittedIntegrator Private Data ≡

32

const int maxDepth;

As a SamplerIntegrator implementation, the WhittedIntegrator must provide an implementation of the Li() method, which returns the radiance arriving at the origin of the given ray. Figure 1.19 summarizes the data ﬂow among the main classes used during integration at surfaces.

WhittedIntegrator Method Deﬁnitions ≡ Spectrum WhittedIntegrator::Li(const RayDifferential &ray, const Scene &scene, Sampler &sampler, MemoryArena &arena, int depth) const { Spectrum L(0.); Find closest ray intersection or return background radiance 34 Compute emitted and reﬂected light at ray intersection point 34 return L; }

The ﬁrst step is to ﬁnd the ﬁrst intersection of the ray with the shapes in the scene. The Scene::Intersect() method takes a ray and returns a Boolean value indicating whether

34

INTRODUCTION

CHAPTER 1

it intersected a shape. For rays where an intersection was found, it initializes the provided SurfaceInteraction with geometric information about the intersection.
If no intersection was found, radiance may be carried along the ray due to light sources that don’t have associated geometry. One example of such a light is the InfiniteArea Light, which can represent illumination from the sky. The Light::Le() method allows such lights to return their radiance along a given ray.

Find closest ray intersection or return background radiance ≡

33

SurfaceInteraction isect;

if (!scene.Intersect(ray, &isect)) {

for (const auto &light : scene.lights)

L += light->Le(ray);

return L;

}

Otherwise a valid intersection has been found. The integrator must determine how light is scattered by the surface of the shape at the intersection point, determine how much illumination is arriving from light sources at the point, and apply an approximation to Equation (1.1) to compute how much light is leaving the surface in the viewing direction. Because this integrator ignores the effect of participating media like smoke or fog, the radiance leaving the intersection point is the same as the radiance arriving at the ray’s origin.

Compute emitted and reﬂected light at ray intersection point ≡

33

Initialize common variables for Whitted integrator 34

Compute scattering functions for surface interaction 35

Compute emitted light if ray hit an area light source 35

Add contribution of each light source 36

if (depth + 1 < maxDepth) {

Trace rays for specular reﬂection and refraction 37

}

Figure 1.20 shows a few quantities that will be used frequently in the fragments to come. n is the surface normal at the intersection point and the normalized direction from the hit point back to the ray origin is stored in wo; Cameras are responsible for normalizing the direction component of generated rays, so there’s no need to renormalize it here. Normalized directions are denoted by the ω symbol in this book, and in pbrt’s code we will use wo to represent ωo, the outgoing direction of scattered light.

Initialize common variables for Whitted integrator ≡

34

Normal3f n = isect.shading.n;

Vector3f wo = isect.wo;

If an intersection was found, it’s necessary to determine how the surface’s material scatters light. The ComputeScatteringFunctions() method handles this task, evaluating texture functions to determine surface properties and then initializing a representation of the BSDF (and possibly BSSRDF) at the point. This method generally needs to allocate memory for the objects that constitute this representation; because this memory only

Camera 356 InfiniteAreaLight 737 Interaction::wo 115 Light::Le() 741 Normal3f 71 Scene::Intersect() 24 Scene::lights 23 SurfaceInteraction 116
SurfaceInteraction::shading 118
SurfaceInteraction:: shading::n 118
Vector3f 60
WhittedIntegrator::maxDepth 33

SECTION 1.3

pbrt: SYSTEM OVERVIEW

35

n

p

Figure 1.20: Geometric Setting for the Whitted Integrator. p is the ray intersection point and n
is its surface normal. The direction in which we’d like to compute reﬂected radiance is ωo; it is the vector pointing in the opposite direction of the incident ray.

Integrator 25 Light::Sample_Li() 716 MemoryArena 1074
SurfaceInteraction:: ComputeScatteringFunctions() 578
SurfaceInteraction::Le() 734 VisibilityTester 717

needs to be available for the current ray, the MemoryArena is provided for it to use for its allocations.

Compute scattering functions for surface interaction ≡

34

isect.ComputeScatteringFunctions(ray, arena);

In case the ray happened to hit geometry that is emissive (such as an area light source), the integrator accounts for any emitted radiance by calling the SurfaceInteraction:: Le() method. This gives the ﬁrst term of the light transport equation, Equation (1.1) on page 12. If the object is not emissive, this method returns a black spectrum.

Compute emitted light if ray hit an area light source ≡

34

L += isect.Le(wo);

For each light, the integrator calls the Light::Sample_Li() method to compute the radiance from that light falling on the surface at the point being shaded. This method also returns the direction vector from the point being shaded to the light source, which is stored in the variable wi (denoting an incident direction ωi).4
The spectrum returned by this method does not account for the possibility that some other shape may block light from the light and prevent it from reaching the point being shaded. Instead, it returns a VisibilityTester object that can be used to determine if any primitives block the surface point from the light source. This test is done by tracing a shadow ray between the point being shaded and the light to verify that the path is

4 When considering light scattering at a surface location, pbrt uses the convention that ωi always refers to the direction from which the quantity of interest (radiance in this case) arrives, rather than the direction from which the Integrator reached the
surface.

36

INTRODUCTION

CHAPTER 1

clear. pbrt’s code is organized in this way so that it can avoid tracing the shadow ray unless necessary: this way it can ﬁrst make sure that the light falling on the surface would be scattered in the direction ωo if the light isn’t blocked. For example, if the surface is not transmissive, then light arriving at the back side of the surface doesn’t contribute to reﬂection.
The Sample_Li() method also returns the probability density for the light to have sampled the direction wi in the pdf variable. This value is used for Monte Carlo integration with complex area light sources where light is arriving at the point from many directions even though just one direction is sampled here; for simple lights like point lights, the value of pdf is one. The details of how this probability density is computed and used in rendering are the topics of Chapters 13 and 14; in the end, the light’s contribution must be divided by pdf, so this is done by the implementation here.
If the arriving radiance is nonzero and the BSDF indicates that some of the incident light from the direction ωi is in fact scattered to the outgoing direction ωo, then the integrator multiplies the radiance value Li by the value of the BSDF f and the cosine term. The cosine term is computed using the AbsDot() function, which returns the absolute value of the dot product between two vectors. If the vectors are normalized, as both wi and n are here, this is equal to the absolute value of the cosine of the angle between them (Section 2.2.1).
This product represents the light’s contribution to the light transport equation integral, Equation (1.1), and it is added to the total reﬂected radiance L. After all lights have been considered, the integrator has computed the total contribution of direct lighting—light that arrives at the surface directly from emissive objects (as opposed to light that has reﬂected off other objects in the scene before arriving at the point).

Add contribution of each light source ≡

34

for (const auto &light : scene.lights) {

Vector3f wi;

Float pdf;

VisibilityTester visibility;

Spectrum Li = light->Sample_Li(isect, sampler.Get2D(), &wi,

&pdf, &visibility);

if (Li.IsBlack() || pdf == 0) continue;

Spectrum f = isect.bsdf->f(wo, wi);

if (!f.IsBlack() && visibility.Unoccluded(scene))

L += f * Li * AbsDot(wi, n) / pdf;

}

This integrator also handles light scattered by perfectly specular surfaces like mirrors or glass. It is fairly simple to use properties of mirrors to ﬁnd the reﬂected directions (Figure 1.21) and to use Snell’s law to ﬁnd the transmitted directions (Section 8.2). The integrator can then recursively follow the appropriate ray in the new direction and add its contribution to the reﬂected radiance at the point originally seen from the camera. The computation of the effect of specular reﬂection and transmission is handled in separate utility methods so these functions can easily be reused by other SamplerIntegrator implementations.

AbsDot() 64 BSDF::f() 575 Float 1062 Light::Sample_Li() 716 Sampler::Get2D() 422 SamplerIntegrator 25 Scene::lights 23 Spectrum 315 Spectrum::IsBlack() 317 SurfaceInteraction::bsdf 250 Vector3f 60 VisibilityTester 717 VisibilityTester::
Unoccluded() 718

SECTION 1.3

pbrt: SYSTEM OVERVIEW

37

n

Figure 1.21: Reﬂected rays due to perfect specular reﬂection make the same angle with the surface normal as the incident ray.

Trace rays for specular reﬂection and refraction ≡

34

L += SpecularReflect(ray, isect, scene, sampler, arena, depth);

L += SpecularTransmit(ray, isect, scene, sampler, arena, depth);

BSDF 572 BSDF::Sample_f() 832 MemoryArena 1074 RayDifferential 75 Sampler 421 SamplerIntegrator::Li() 31
SamplerIntegrator:: SpecularReflect() 37
SamplerIntegrator:: SpecularTransmit() 38
Scene 23 Spectrum 315 SurfaceInteraction 116 WhittedIntegrator::Li() 33

SamplerIntegrator Method Deﬁnitions +≡ Spectrum SamplerIntegrator::SpecularReflect(const RayDifferential &ray, const SurfaceInteraction &isect, const Scene &scene, Sampler &sampler, MemoryArena &arena, int depth) const { Compute specular reﬂection direction wi and BSDF value 38 Return contribution of specular reﬂection 38 }
In the SpecularReflect() and SpecularTransmit() methods, the BSDF::Sample_f() method returns an incident ray direction for a given outgoing direction and a given mode of light scattering. This method is one of the foundations of the Monte Carlo light transport algorithms that will be the subject of the last few chapters of this book. Here, we will use it to ﬁnd only outgoing directions corresponding to perfect specular reﬂection or refraction, using ﬂags to indicate to BSDF::Sample_f() that other types of reﬂection should be ignored. Although BSDF::Sample_f() can sample random directions leaving the surface for probabilistic integration algorithms, the randomness is constrained to be consistent with the BSDF’s scattering properties. In the case of perfect specular reﬂection or refraction, only one direction is possible, so there is no randomness at all.
The calls to BSDF::Sample_f() in these functions initialize wi with the chosen direction and return the BSDF’s value for the directions (ωo, ωi). If the value of the BSDF is nonzero, the integrator uses the SamplerIntegrator::Li() method to get the incoming radiance along ωi, which in this case will in turn resolve to the WhittedIntegrator::Li() method.

38

INTRODUCTION

CHAPTER 1

Compute specular reﬂection direction wi and BSDF value ≡

37

Vector3f wo = isect.wo, wi;

Float pdf;

BxDFType type = BxDFType(BSDF_REFLECTION | BSDF_SPECULAR);

Spectrum f = isect.bsdf->Sample_f(wo, &wi, sampler.Get2D(), &pdf, type);

In order to use ray differentials to antialias textures that are seen in reﬂections or refractions, it is necessary to know how reﬂection and transmission affect the screen-space footprint of rays. The fragments that compute the ray differentials for these rays are deﬁned later, in Section 10.1.3. Given the fully initialized ray differential, a recursive call to Li() provides incident radiance, which is scaled by the value of the BSDF, the cosine term, and divided by the PDF, as per Equation (1.1).

Return contribution of specular reﬂection ≡

37

const Normal3f &ns = isect.shading.n;

if (pdf > 0 && !f.IsBlack() && AbsDot(wi, ns) != 0) {

Compute ray differential rd for specular reﬂection 607

return f * Li(rd, scene, sampler, arena, depth + 1) * AbsDot(wi, ns) /

pdf;

}

else

return Spectrum(0.f);

The SpecularTransmit() method is essentially the same as SpecularReflect() but just requests the BSDF_TRANSMISSION specular component of the BSDF, if any, rather than the BSDF_REFLECTION component used by SpecularReflect(). We therefore won’t include its implementation in the text of the book here.

1.4 PARALLELIZATION OF pbrt
It’s now nearly impossible to buy a new laptop or desktop computer with only one processing core. (The same even holds for mobile phones, for that matter.) The computer systems of today and of the future will increasingly feature multiple processing cores, both on CPUs and on highly parallel throughput processors like GPUs. At the same time, the computational capabilities of their cores are only improving slowly; as such, signiﬁcant increases in performance over time are now only available to programs that can run in parallel, with many separate threads of execution performing computation simultaneously on multiple cores.
Writing a parallel program is more difﬁcult than writing a serial program. When two computations that the programmer believes are independent are executing simultaneously but then interact unexpectedly, the program may crash or generate unexpected results. However, such a bug may not manifest itself again if the program is run again, perhaps due to those particular computations not running simultaneously during the

AbsDot() 64 BSDF::Sample_f() 832 BSDF_REFLECTION 513 BSDF_SPECULAR 513 BSDF_TRANSMISSION 513 BxDFType 513 Float 1062 Interaction::wo 115 Normal3f 71 Sampler::Get2D() 422 SamplerIntegrator::Li() 31 Spectrum 315 Spectrum::IsBlack() 317 SurfaceInteraction::shading
118 SurfaceInteraction::
shading::n 118 Vector3f 60

SECTION 1.4

PARALLELIZATION OF pbrt

39

next run. Fortunately, increasingly good tools to help developers ﬁnd these sorts of interactions are increasingly available.5

For a parallel program to scale well to many processors, it needs to be able to provide a substantial amount of independent computation: any computation dependent on results of prior computation can’t be run concurrently with the computation it depends on. Fortunately, most rendering algorithms based on ray tracing have abundant parallelism; for the SamplerIntegrator, each image sample is independent of all of the other ones, and many millions of samples may be used for high-quality images.

One of the biggest challenges with parallel ray tracing is the impact of non-parallel phases of computation. For example, it’s not as easy to effectively parallelize the construction of many acceleration structures while the scene is being constructed than it is to parallelize rendering. While this may seem like a minor issue, Amdahl’s law, which describes the speedup of a workload that has both serial and parallel phases, points to the challenge. Given n cores performing computation and a workload where the fraction s of its overall computation is inherently serial, then the maximum speedup possible is

1

s

+

1 n

(1

−

. s)

Thus, even with an inﬁnite number of cores, the maximum speedup is 1/s. If, for example, a seemingly innocuous 5% of the run time is spent in a serial phase of parsing the scene ﬁle and building acceleration structures, the maximum speedup possible is 1/0.05 = 20×, no matter how quickly the parallel phase executes.

1.4.1 DATA RACES AND COORDINATION
In pbrt, we assume that the computation is running on processors that provide coherent shared memory. The main idea of coherent shared memory is that all threads can read and write to a common set of memory locations and that changes to memory made by one thread will eventually be seen by other threads. These properties greatly simplify the implementation of the system as there’s no need to explicitly communicate data between cores. (Coherent shared memory is generally available on today’s CPUs and is likely to continue to be on future CPUs. On the other hand, if one is running a computation across multiple computers in a cluster, coherent shared memory is generally not available.)
Although coherent shared memory relieves the need for separate threads to explicitly communicate data with each other, they still need to coordinate their access to shared data; a danger of coherent shared memory is data races. If two threads modify the same memory location without coordination between the two of them, the program will almost certainly compute incorrect results or even crash. Consider the example of two processors simultaneously running the following innocuous-looking code, where globalCounter starts with a value of two:

SamplerIntegrator 25

5 We found the open-source tool helgrind, part of the valgrind suite of tools, instrumental for helping to ﬁnd bugs in pbrt’s parallel code as we were developing it. “Thread sanitizer” is also well regarded.

40

INTRODUCTION

CHAPTER 1

extern int globalCounter; if (--globalCounter == 0)
printf("done!\n");

Because the two threads don’t coordinate their reading and writing of globalCounter, it is possible that “done” will be printed zero, one, or even two times! The assembly instructions generated by the compiler are likely to correspond to steps something like the following:

extern int globalCounter; int temp = globalCounter; temp = temp - 1; globalCounter = temp; if (globalCounter == 0)
printf("done!\n");

Now, consider different ways this code could be executed on two processors. For example, the second processor could start executing slightly after the ﬁrst one, but the ﬁrst one could go idle for a few cycles after executing the ﬁrst few instructions:

Thread A int temp = globalCounter; temp = temp - 1; globalCounter = temp;
// (idle)
if (globalCounter == 0) printf("done!\n");

Thread B
(idle)
int temp = globalCounter; temp = temp - 1; globalCounter = temp; if (globalCounter == 0)
printf("done!\n");

(Many unpredictable events can cause these sorts of execution bubbles, ranging from the OS interrupting the thread to cache misses.) In this ordering, both threads read the value of zero from globalCounter, and both execute the printf() call. In this case, the error is not fatal, but if instead the system was freeing resources in the if block, then it would attempt to free the same resources twice, which would very likely cause a crash. Consider now this potential execution order:

Thread A int temp = globalCounter; temp = temp - 1; globalCounter = temp; // (idle) if (globalCounter == 0)
printf("done!\n");

Thread B int temp = globalCounter; temp = temp - 1; // (idle) globalCounter = temp; if (globalCounter == 0)
printf("done!\n");

In this case, globalCounter ends up with a value of one, and neither thread executes the if block. These examples illustrate the principle that when multiple threads of execution are accessing shared modiﬁed data, they must somehow synchronize their access.

Two main mechanisms are available today for doing this type of synchronization: mutual exclusion and atomic operations. Mutual exclusion is implemented with std::mutex

SECTION 1.4

PARALLELIZATION OF pbrt

41

objects in pbrt. A std::mutex can be used to protect access to some resource, ensuring that only one thread can access it at a time. Consider the following updated version of the previous computation; here a std::lock_guard object acquires a lock on the mutex and releases it when it goes out of scope at the ﬁnal brace.

extern int globalCounter; extern std::mutex globalCounterMutex; { std::lock_guard<std::mutex> lock(globalCounterMutex);
int temp = globalCounter; temp = temp - 1; globalCounter = temp; if (globalCounter == 0)
printf("done!\n"); }

If two threads are executing this code and try to acquire the mutex at the same time, then the mutex will allow only one of them to proceed, stalling the other one in the std::lock_guard constructor. Only when the ﬁrst thread has ﬁnished the computation and its std::lock_guard goes out of scope, releasing the lock on the mutex, is the second thread able to acquire the mutex itself and continue the computation.

Thread A { std::lock_guard<std::mutex> lock(
globalCounterMutex); int temp = globalCounter; . . .
} // (mutex released)

Thread B { std::lock_guard<std::mutex> lock(
globalCounterMutex); // (stalled by mutex)
// (mutex acquired) int temp = globalCounter; . . . } // (mutex released)

With correct mutual exclusion here, the printf() will only be executed once, no matter what the ordering of execution between the two threads is.
Atomic memory operations (or atomics) are the other option for correctly performing this type of memory update with multiple threads. Atomics are machine instructions that guarantee that their respective memory updates will be performed in a single transaction. (Atomic in this case refers to the notion that the memory updates are indivisible.) The implementations of atomic operations in pbrt are from the C++11 standard library and are further discussed in Appendix A.6.2. Using atomics, the computation above could be written to use the std::atomic<int> type, which has overloaded add, subtract, increment, and decrement operations, as below:

extern std::atomic<int> globalCounter; if (--globalCounter == 0)
printf("done!\n");

42

INTRODUCTION

CHAPTER 1

The std::atomic -- operator subtracts 1 from the given variable, globalCounter, and returns the previous value of the variable. Using an atomic operation ensures that if two threads simultaneously try to update the variable then not only will the ﬁnal value of the variable be the expected value but each thread will be returned the value of the variable after its update alone. In this example, then, globalCounter will end up with a value of zero, as expected, with one thread guaranteed to have the value one returned from the atomic subtraction and the other thread guaranteed to have zero returned.
An additional option, transactional memory, is just starting to become available in CPUs as of this writing. With transactional memory, a set of memory writes are bundled as a transaction; if no other threads access those memory locations while the transaction is executing, then all of the writes are committed in a single atomic operation. Otherwise, it is rolled back and none of the writes reach memory, and thus the computation has had no effect; the transaction must then be tried again. Transactional memory helps bridge the ﬁne-grained operation of atomics and the higher overhead of mutexes. However, because it isn’t yet widely available, transactional memory isn’t currently used in pbrt.
Section A.6 in Appendix A has more information about parallel programming, with additional details on performance issues and pitfalls, as well as the various routines used in the parallel implementation of pbrt.

1.4.2 CONVENTIONS IN pbrt
In pbrt (as is the case for most ray tracers) the vast majority of data at render time is read only (e.g., the scene description and texture maps). Almost all of the parsing of the scene ﬁle and creation of the scene representation in memory is done with a single thread of execution, so there are no synchronization issues during that phase of execution.6 During rendering, concurrent read access to all of the read-only data by multiple threads works with no problems; we only need to be concerned with situations where data in memory is being modiﬁed.
When adding new code to pbrt, it’s important to make sure to not inadvertently add code that modiﬁes shared data without proper synchronization. This is usually straightforward; for example, when adding a new Shape implementation, the Shape will normally only perform read accesses to its member variables after it has been created. Sometimes, however, shared data may be inadvertently introduced. Consider the following code idiom, often seen in single-threaded code:

static bool firstCall = true;

if (firstCall) {

.

. .

additional initialization

firstCall = false;

}

6 The two exceptions are some image resampling performed on image texture maps, and construction of one variant of the BVHAccel, though both of these are highly localized.

BVHAccel 256 Shape 123

SECTION 1.4

PARALLELIZATION OF pbrt

43

This code is unsafe with multiple threads of execution, as multiple threads may see the value of firstCall as true and all execute the initialization code. Writing this safely requires either atomic operations or mutexes. (This particular idiom can also be implemented safely using the std::call_once() function.)

Light 714 MemoryArena 1074 Normal3f 71 ParallelFor() 1088 Point3f 68 Quaternion 99 RNG 1065 Sampler 421 SamplerIntegrator 25 SamplerIntegrator::Render()
26 Spectrum 315 SurfaceInteraction 116 Transform 83 Vector3f 60

1.4.3 THREAD SAFETY EXPECTATIONS IN pbrt
Many class methods in pbrt are required to be safe for multiple concurrent threads of execution. Particular instances of these methods must either be safe naturally due to not updating shared global data or due to using mutexes or atomic operations to safely perform any updates that are needed.
As a general rule, the low-level classes and structures in the system are not thread-safe. For example, the Point3f class, which stores three float values to represent a point in 3D space, is not safe for multiple threads to call methods that modify it at the same time. (Multiple threads can use Point3fs as read-only data simultaneously, of course.) The run-time overhead to make Point3f thread-safe would have a substantial effect on performance with little beneﬁt in return.
The same is true for classes like Vector3f, Normal3f, Spectrum, Transform, Quaternion, and SurfaceInteraction. These classes are usually either created at scene construction time and then used as read-only data or allocated on the stack during rendering and used only by a single thread.
The utility classes MemoryArena (used for high-performance temporary memory allocation) and RNG (pseudo-random number generation) are also not safe for use by multiple threads; these classes store state that is modiﬁed when their methods are called, and the overhead from protecting modiﬁcation to their state with mutual exclusion would be excessive relative to the amount of computation they perform. Consequently, in code like the SamplerIntegrator::Render() method above, the implementation allocates perthread instances of these classes on the stack.
With two exceptions, implementations of the higher level abstract base classes listed in Table 1.1 are all expected to be safe for multiple threads to use simultaneously. With a little care, it is usually straightforward to implement speciﬁc instances of these base classes so they don’t modify any shared state in their methods.
The ﬁrst exceptions are the SamplerIntegrator and Light Preprocess() methods. These are called by the system during scene construction, and implementations of them generally modify shared state in their implementations—for example, by building data structures that represent the distribution of illumination in the scene. Therefore, it’s helpful to allow the implementer to assume that only a single thread will call into these methods. (This is a separate issue from the consideration that implementations of these methods that are computationally intensive may use ParallelFor() to parallelize their computation.)
The second exception is the Sampler; its methods are also not expected to be thread safe. This is another instance where this requirement would impose an excessive performance and scalability impact; many threads simultaneously trying to get samples from a single

44

INTRODUCTION

CHAPTER 1

Sampler would limit the system’s overall performance. Therefore, as described in Section 1.3.4, a unique Sampler is created for each image tile using Sampler::Clone(); this sampler can then be used for just the one tile, without any mutual exclusion overhead.
All stand-alone functions in pbrt are thread-safe (as long as multiple threads don’t pass pointers to the same data to them).

1.5 HOW TO PROCEED THROUGH THIS BOOK

We wrote this book assuming it will be read in roughly front-to-back order. Generally, we tried to minimize the number of forward references to ideas and interfaces that haven’t yet been introduced, but we do assume that the reader is acquainted with the previous content at any particular point in the text. However, some sections go into depth about advanced topics that some readers may wish to skip over (particularly on ﬁrst reading); each advanced section is identiﬁed by an asterisk in its title.
Because of the modular nature of the system, the main requirement is that the reader be familiar with the low-level classes like Point3f, Ray, and Spectrum; the interfaces deﬁned by the abstract base classes listed in Table 1.1; and the rendering loop in Sampler Integrator::Render(). Given that knowledge, for example, the reader who doesn’t care about precisely how a camera model based on a perspective projection matrix maps CameraSamples to rays can skip over the implementation of that camera and can just remember that the Camera::GenerateRayDifferential() method somehow turns a CameraSample into a RayDifferential.
The rest of this book is divided into four main parts of a few chapters each. First, Chapters 2 through 4 deﬁne the main geometric functionality in the system. Chapter 2 has the low-level classes like Point3f, Ray, and Bounds3f. Chapter 3 deﬁnes the Shape interface, gives implementations of a number of shapes, and shows how to perform ray-shape intersection tests. Chapter 4 has the implementations of the acceleration structures for speeding up ray tracing by skipping tests with primitives that a ray can be shown to deﬁnitely not intersect.
The second part covers the image formation process. First, Chapter 5 introduces the physical units used to measure light and the Spectrum class that represents wavelengthvarying distributions (i.e., color). Chapter 6 deﬁnes the Camera interface and has a few different camera implementations. The Sampler classes that place samples on the image plane are the topic of Chapter 7, and the overall process of turning radiance values on the ﬁlm into images suitable for display is explained in Section 7.9.
The third part of the book is about light and how it scatters from surfaces and participating media. Chapter 8 includes a set of building-block classes that deﬁne a variety of types of reﬂection from surfaces. Materials, described in Chapter 9, use these reﬂection functions to implement a number of different surface materials, such as plastic, glass, and metal. Chapter 10 introduces texture, which describes variation in material properties (color, roughness, etc.) over surfaces, and Chapter 11 has the abstractions that describe how light is scattered and absorbed in participating media. Finally, Chapter 12 has the interface for light sources and a number of light source implementations.

Bounds3f 76 Camera 356 Camera::
GenerateRayDifferential() 357 CameraSample 357 Point3f 68 Ray 73 RayDifferential 75 Sampler 421 Sampler::Clone() 424 SamplerIntegrator::Render() 26 Shape 123 Spectrum 315

SECTION 1.6

USING AND UNDERSTANDING THE CODE

45

The last part brings all of the ideas from the rest of the book together to implement a number of interesting light transport algorithms. Chapter 13 introduces the theory of Monte Carlo integration, a statistical technique for estimating the value of complex integrals, and describes low-level routines for applying Monte Carlo to illumination and light scattering. The integrators in Chapters 14, 15, and 16 use Monte Carlo integration to compute more accurate approximations of the light transport equation than the WhittedIntegrator, using techniques like path tracing, bidirectional path tracing, Metropolis light transport, and photon mapping.
Chapter 17, the last chapter of the book, provides a brief retrospective and discussion of system design decisions along with a number of suggestions for more far-reaching projects than those in the exercises. Appendices describe utility functions and details of how the scene description is created as the input ﬁle is parsed.
1.5.1 THE EXERCISES
At the end of each chapter you will ﬁnd exercises related to the material covered in that chapter. Each exercise is marked as one of three levels of difﬁculty:
An exercise that should take only an hour or two A reading and/or implementation task that would be suitable for a course assignment and should take between 10 and 20 hours of work A suggested ﬁnal project for a course that will likely take 40 hours or more to complete

WhittedIntegrator 32

1.6 USING AND UNDERSTANDING THE CODE
We wrote pbrt in C++ but focused on readability for non-C++ experts by limiting usage of esoteric features of the language. Staying close to the core language features also helps with the system’s portability. In particular, we avoid multiple inheritance, run-time exception handling, and excessive use of C++11 and C++14 features. We also use only a small subset of C++’s extensive standard library.
We will occasionally omit short sections of pbrt’s source code from the book. For example, when there are a number of cases to be handled, all with nearly identical code, we will present one case and note that the code for the remaining cases has been omitted from the text. Of course, all the omitted code can be found in the pbrt source code distribution.
1.6.1 POINTER OR REFERENCE?
C++ provides two different mechanisms for passing the address of a data structure to a function or method: pointers and references. If a function argument is not intended as an output variable, either can be used to save the expense of passing the entire structure on the stack. By convention, pbrt uses pointers when the argument will be completely changed by the function or method, references when some of its internal state will be changed but it won’t be fully re-initialized, and const references when it won’t be changed at all. One important exception to this rule is that we will always use a pointer when we

46

INTRODUCTION

CHAPTER 1

want to be able to pass nullptr to indicate that a parameter is not available or should not be used.

1.6.2 ABSTRACTION VERSUS EFFICIENCY
One of the primary tensions when designing interfaces for software systems is making a reasonable trade-off between abstraction and efﬁciency. For example, many programmers religiously make all data in all classes private and provide methods to obtain or modify the values of the data items. For simple classes (e.g., Vector3f), we believe that approach needlessly hides a basic property of the implementation—that the class holds three ﬂoating-point coordinates—that we can reasonably expect to never change. Of course, using no information hiding and exposing all details of all classes’ internals leads to a code maintenance nightmare. Yet, we believe that there is nothing wrong with judiciously exposing basic design decisions throughout the system. For example, the fact that a Ray is represented with a point, a vector, and values that give its extent, time, and recursion depth is a decision that doesn’t need to be hidden behind a layer of abstraction. Code elsewhere is shorter and easier to understand when details like these are exposed.
An important thing to keep in mind when writing a software system and making these sorts of trade-offs is the expected ﬁnal size of the system. The core of pbrt (excluding the implementations of speciﬁc shapes, lights, and so forth), where all of the basic interfaces, abstractions, and policy decisions are deﬁned, is under 20,000 lines of code. Adding additional functionality to the system will generally only increase the amount of code in the implementations of the various abstract base classes. pbrt is never going to grow to be a million lines of code; this fact can and should be reﬂected in the amount of information hiding used in the system. It would be a waste of programmer time (and likely a source of run-time inefﬁciency) to design the interfaces to accommodate a system of that level of complexity.

1.6.3 CODE OPTIMIZATION
We tried to make pbrt efﬁcient through the use of well-chosen algorithms rather than through local micro-optimizations, so that the system can be more easily understood. However, we applied some local optimizations to the parts of pbrt that account for the most execution time, as long as doing so didn’t make the code too hard to understand. There are two main local optimization principles used throughout the code:
. On current CPU architectures, the slowest mathematical operations are divides, square roots, and trigonometric functions. Addition, subtraction, and multiplication are generally 10 to 50 times faster than those operations. Reducing the number of slow mathematical operations can help performance substantially; for example, instead of repeatedly dividing by some value v, we will often precompute the reciprocal 1/v and multiply by that instead.
. The speed of CPUs continues to grow more quickly than the speed at which data can be loaded from main memory into the CPU. This means that waiting for values to be fetched from memory can be a major performance limitation. Organizing algorithms and data structures in ways that give good performance from memory caches can speed up program execution much more than reducing the total number of instructions executed. Section A.4 in Appendix A discusses general principles

Ray 73 Vector3f 60

SECTION 1.6

USING AND UNDERSTANDING THE CODE

47

for memory-efﬁcient programming; these ideas are mostly applied in the ray intersection acceleration structures of Chapter 4 and the image map representation in Section 10.4.3, although they inﬂuence many of the design decisions throughout the system.

1.6.4 THE BOOK WEB SITE
We created a companion Web site for this book, located at pbrt.org. The Web site includes the system’s source code, documentation, images rendered with pbrt, example scenes, errata, and links to a bug reporting system. We encourage you to visit the Web site and subscribe to the pbrt mailing list.
1.6.5 EXTENDING THE SYSTEM
One of our goals in writing this book and building the pbrt system was to make it easier for developers and researchers to experiment with new (or old!) ideas in rendering. One of the great joys in computer graphics is writing new software that makes a new image; even small changes to the system can be fun to experiment with. The exercises throughout the book suggest many changes to make to the system, ranging from small tweaks to major open-ended research projects. Section B.4 in Appendix B has more information about the mechanics of adding new implementations of the abstract base classes listed in Table 1.1.
1.6.6 BUGS
Although we made every effort to make pbrt as correct as possible through extensive testing, it is inevitable that some bugs are still present.
If you believe you have found a bug in the system, please do the following:
1. Reproduce the bug with an unmodiﬁed copy of the latest version of pbrt. 2. Check the online discussion forum and the bug-tracking system at pbrt.org. Your
issue may be a known bug, or it may be a commonly misunderstood feature. 3. Try to ﬁnd the simplest possible test case that demonstrates the bug. Many bugs
can be demonstrated by scene description ﬁles that are just a few lines long, and debugging is much easier with a simple scene than a complex one. 4. Submit a detailed bug report using our online bug-tracking system. Make sure that you include the scene ﬁle that demonstrates the bug and a detailed description of why you think pbrt is not behaving correctly with the scene. If you can provide a patch to the code that ﬁxes the bug, all the better!
We will periodically release updated versions of pbrt with bug ﬁxes and minor enhancements. (Be aware that we often let bug reports accumulate for a few months before going through them; don’t take this as an indication that we don’t value them!) However, we will not make major changes to the pbrt source code so that it doesn’t diverge from the system described here in the book.

48

INTRODUCTION

CHAPTER 1

1.7 A BRIEF HISTORY OF PHYSICALLY BASED RENDERING

Through the early years of computer graphics in the 1970s, the most important problems to solve were fundamental issues like visibility algorithms and geometric representations. When a megabyte of RAM was a rare and expensive luxury and when a computer capable of a million ﬂoating-point operations per second cost hundreds of thousands of dollars, the complexity of what was possible in computer graphics was correspondingly limited, and any attempt to accurately simulate physics for rendering was infeasible.
As computers have become more capable and less expensive, it became possible to consider more computationally demanding approaches to rendering, which in turn has made physically based approaches viable. This progression is neatly explained by Blinn’s law: “as technology advances, rendering time remains constant.”
Jim Blinn’s simple statement captures an important constraint: given a certain number of images that must be rendered (be it a handful for a research paper or over a hundred thousand for a feature ﬁlm), it’s only possible to take so much processing time for each one. One has a certain amount of computation available and one has some amount of time available before rendering must be ﬁnished, so the maximum computation per image is necessarily limited.
Blinn’s law also expresses the observation that there remains a gap between the images people would like to be able to render and the images that they can render: as computers have become faster, content creators have continued to be use increased computational capability to render more complex scenes with more sophisticated rendering algorithms, rather than rendering the same scenes as before, just more quickly. Rendering continues to consume all of the computational capabilities made available to it.
1.7.1 RESEARCH
Physically based approaches to rendering started to be seriously considered by graphics researchers in the 1980s. Whitted’s paper (1980) introduced the idea of using ray tracing for global lighting effects, opening the door to accurately simulating the distribution of light in scenes. The rendered images his approach produced were markedly different from any that had been seen before, which spurred excitement about this approach.
Another notable early advancement in physically based rendering was Cook and Torrance’s reﬂection model (1981, 1982), which introduced microfacet reﬂection models to graphics. Among other contributions, they showed that accurately modeling microfacet reﬂection made it possible to render metal surfaces accurately; metal was not well rendered by earlier approaches.
Shortly afterward, Goral et al. (1984) made connections between the thermal transfer literature and rendering, showing how to incorporate global diffuse lighting effects using a physically based approximation of light transport. This method was based on ﬁniteelement methods, where areas of surfaces in the scene exchanged energy with each other. This approach came to be referred to as “radiosity,” after a related physical unit. Following work by Cohen and Greenberg (1985) and Nishita and Nakamae (1985) introduced important improvements. Once again, a physically based approach led to images with

SECTION 1.7

A BRIEF HISTORY OF PHYSICALLY BASED RENDERING

49

lighting effects that hadn’t previously been seen in rendered images, which led to many researchers pursuing improvements in this area.
While the radiosity approach was strongly based on physical units and conservation of energy, in time it became clear that it didn’t lead to viable rendering algorithms: the asymptotic computational complexity was a difﬁcult-to-manage O(n2), and it was necessary to be able to re-tessellate geometric models along shadow boundaries for good results; researchers had difﬁculty developing robust and efﬁcient tessellation algorithms for this purpose and radiosity’s adoption in practice was limited.
During the radiosity years, a small group of researchers pursued physically based approaches to rendering that were based on ray tracing and Monte Carlo integration. At the time, many looked at their work with skepticism; objectionable noise in images due to variance from Monte Carlo integration seemed unavoidable, while radiosity-based methods quickly gave visually pleasing results, at least on relatively simple scenes.
In 1984, Cook, Porter, and Carpenter introduced distributed ray tracing, which generalized Whitted’s algorithm to compute motion blur and defocus blur from cameras, blurry reﬂection from glossy surfaces, and illumination from area light sources (Cook et al. 1984), showing that ray tracing was capable of generating a host of important lighting effects.
Shortly afterward, Kajiya (1986) introduced path tracing; he set out a rigorous formulation of the rendering problem (the light transport integral equation) and showed how to apply Monte Carlo integration to solve it. This work required immense amounts of computation: to render a 256 × 256 pixel image of two spheres with path tracing required 7 hours of computation on an IBM 4341 computer, which cost roughly $280,000 when it was ﬁrst released (Farmer 1981). With von Herzen, Kajiya also introduced the volumerendering equation to graphics (Kajiya and von Herzen 1984); this equation rigorously describes the scattering of light in participating media.
Both Cook et al.’s and Kajiya’s work once again led to images unlike any that had been seen before, demonstrating the value of physically based methods. In subsequent years, important work on Monte Carlo for realistic image synthesis was described in papers by Arvo and Kirk (1990) and Kirk and Arvo (1991). Shirley’s Ph.D. dissertation (1990) and follow-on work by Shirley et al. (1996) were important contributions to Monte Carlo–based efforts. Hall’s book, Illumination and Color in Computer Generated Imagery, (1989) is one of the ﬁrst books to present rendering in a physically based framework, and Andrew Glassner’s Principles of Digital Image Synthesis rigorously laid out foundations of the ﬁeld (1995). Ward’s Radiance rendering system was an early open source physically based rendering system, focused on lighting design (Ward 1994), and Slusallek’s Vision renderer was deisgned to bridge the gap between physically based approaches and the then widely used RenderMan interface, which wasn’t physically based (Slusallek 1996).
Following Torrance and Cook’s work, much of the research in the Program of Computer Graphics at Cornell University investigated physically based approaches. The motivations for this work were summarized by Greenberg et al. (1997), who made a strong argument for a physically accurate rendering based on measurements of the material properties of real-world objects and on deep understanding of the human visual system.

50

INTRODUCTION

CHAPTER 1

A crucial step forward for physically based rendering was Veach’s work, described in detail in his dissertation (Veach 1997). Veach advanced key theoretical foundations of Monte Carlo rendering while also developing new algorithms like multiple importance sampling, bidirectional path tracing, and Metropolis light transport that greatly improved its efﬁciency. Using Blinn’s law as a guide, we believe that these signiﬁcant improvements in efﬁciency were critical to practical adoption of these approaches.
Around this time, as computers became faster and more parallel, a number of researchers started pursuing real-time ray tracing; Wald, Slusallek, and Benthin wrote an inﬂuential paper that described a highly optimized ray tracer that was much more efﬁcient than previous ray tracers (Wald et al. 2001b). Many subsequent papers introduced increasingly more efﬁcient ray-tracing algorithms. Though most of this work wasn’t physically based, the results led to great progress in ray-tracing acceleration structures and performance of the geometric components of ray tracing. Because physically based rendering generally makes substantial use of ray tracing, this work has in turn had the same helpful effect as faster computers have, making it possible to render more complex scenes with physical approaches.
At this point, we’ll end our summary of the key steps in the research progress of physically based rendering; much more has been done. The “Further Reading” sections in all of the subsequent chapters of this book cover this work in detail.
1.7.2 PRODUCTION
With more capable computers in the 1980s, computer graphics could start to be used for animation and ﬁlm production. Early examples include Jim Blinn’s rendering of the Voyager 2 Flyby of Saturn in 1981 and visual effects in the movies Star Trek II: The Wrath of Khan (1982), Tron (1982), and The Last Starﬁghter (1984).
In early production use of computer-generated imagery, rasterization-based rendering (notably, the Reyes algorithm (Cook et al. 1987)) was the only viable option. One reason was that not enough computation was available for complex reﬂection models or for the global lighting effects that physically based ray tracing could provide. More signiﬁcantly, rasterization had the important advantage that it didn’t require that the entire scene representation ﬁt into main memory.
When RAM was much less plentiful, almost any interesting scene was too large to ﬁt into main memory. Rasterization-based algorithms made it possible to render scenes while having only a small subset of the full scene representation in memory at any time. Global lighting effects are difﬁcult to achieve if the whole scene can’t ﬁt into main memory; for many years, with limited computer systems, content creators effectively decided that geometric and texture complexity was more important to visual realism than lighting complexity (and in turn physical accuracy).
Many practitioners at this time also believed that physically based approaches were undesirable for production: one of the great things about computer graphics is that one can cheat reality with impunity to achieve a desired artistic effect. For example, lighting designers on regular movies often struggle to place light sources so that they aren’t visible to the camera or spend a lot of effort placing a light to illuminate an actor without shin-

SECTION 1.7

A BRIEF HISTORY OF PHYSICALLY BASED RENDERING

51

ing too much light on the background. Computer graphics offers the opportunity to, for example, implement a light source model that shines twice as much light on a character as on a background object, in a fairly straightforward manner. For many years, this capability seemed much more useful than physical accuracy.
Visual effects practitioners who had the speciﬁc need to match rendered imagery to ﬁlmed real-world environments pioneered capturing real-world lighting and shading effects and were early adopters of physically based approaches in the late 1990s and early 2000s. (See Snow (2010) for a history of ILM’s early work in this area, for example.)
During this time, the Blue Sky studio adopted a physically based pipeline early in their history (Ohmer 1997). The photorealism of an advertisement they made for a Braun shaver in 1992 caught the attenation of many, and their short ﬁlm, Bunny, shown in 1998, was an early example of Monte Carlo global illumination used in production. Its visual look was substantially different from those of ﬁlms and shorts rendered with Reyes and was widely noted. Subsequent feature ﬁlms from Blue Sky also followed this approach. Unfortunately, Blue Sky never published signiﬁcant technical details of their approach, limiting their wider inﬂuence.
During the early 2000s, the mental ray ray-tracing system was used by a number of studios, mostly for visual effects. It was a very efﬁcient ray tracer with sophisticated global illumination algorithm implementations. The main focus of its developers was computer-aided design and product design applications, so it lacked features like the ability to handle extremely complex scenes and the enormous numbers of texture maps that ﬁlm production demanded.
After Bunny, another watershed moment came in 2001, when Marcos Fajardo came to the SIGGRAPH with an early version of his Arnold renderer. He showed images in the Monte Carlo image synthesis course that not only had complex geometry, textures, and global illumination but also were rendered in tens of minutes. While these scenes weren’t of the complexity of those used in ﬁlm production at the time, his results showed many the creative opportunities from global illumination in complex scenes.
Fajardo brought Arnold to Sony Pictures Imageworks, where work started to transform it to a production-capable physically based rendering system. Work on efﬁcient motion blur, programmable shading, support for massively complex scenes and deferred loading of scene geometry, and support for texture caching, where only a small subset of the texture in the scene is kept in memory, were all important areas to be addressed. Arnold was ﬁrst used on the movie Monster House and is now generally available as a product.
In the mid-2000s, Pixar’s RenderMan renderer started to support hybrid rasterization and ray-tracing algorithms and included a number of innovative algorithms for computing global illumination solutions in complex scenes. RenderMan was recently rewritten to be a physically based ray tracer, following the general system architecture of pbrt (Christensen 2015).
One of the main reasons that physically based Monte Carlo approaches to rendering have been successful in production is that they end up improving the productivity of artists. Some of the important factors have been:

52

INTRODUCTION

CHAPTER 1

Figure 1.22: Gravity (2013) featured spectacular computer-generated imagery of a realistic space environment with volumetric scattering and large numbers of anisotropic metal surfaces. The image was generated using Arnold, a physically based rendering system that accounts for global illumination. Image courtesy of Warner Bros. and Framestore.
. The algorithms involved have essentially just a single quality knob: how many samples to take per pixel; this is extremely helpful for artists. Ray-tracing algorithms are also suited to both progressive reﬁnement and quickly computing rough previews by taking just a few samples per pixel; rasterization-based renderers don’t have equivalent capabilities.
. Adopting physically based reﬂection models has made it easier to design surface materials. Earlier, when reﬂection models that didn’t necessarily conserve energy were used, an object might be placed in a single lighting environment while its surface reﬂection parameters were adjusted. The object might look great in that environment, but it would often appear completely wrong when moved to another lighting environment because surfaces were actually reﬂecting too little or too much energy: surface properties had been set to unreasonable values.
. The quality of shadows computed with ray tracing is much better than it is with rasterization. Eliminating the need to tweak shadow map resolutions, biases, and other parameters has eliminated an unpleasant task of lighting artists. Further, physically based methods bring with them bounce lighting and other soft-lighting effects from the method itself, rather than as an artistically tuned manual process.
As of this writing, physically based rendering is used widely for producing computergenerated imagery for movies; Figures 1.22 and 1.23 show images from two recent movies that used physically based approaches.

FURTHER READING

53

Figure 1.23: This image from The Hobbit: The Battle of the Five Armies (2014) was also rendered using a physically based rendering system; the characters feature heterogeneous subsurface scattering and vast amounts of geometric detail. Image by Weta Digital, courtesy of Warner Bros. and Metro-Goldwyn-Mayer.
FURTHER READING
In a seminal early paper, Arthur Appel (1968) ﬁrst described the basic idea of ray tracing to solve the hidden surface problem and to compute shadows in polygonal scenes. Goldstein and Nagel (1971) later showed how ray tracing could be used to render scenes with quadric surfaces. Kay and Greenberg (1979) described a ray-tracing approach to rendering transparency, and Whitted’s seminal CACM article described the general recursive ray-tracing algorithm that is implemented in this chapter, accurately simulating reﬂection and refraction from specular surfaces and shadows from point light sources (Whitted 1980). Heckbert (1987) was the ﬁrst to explore realistic rendering of dessert.
Notable early books on physically based rendering and image synthesis include Cohen and Wallace’s Radiosity and Realistic Image Synthesis (1993), Sillion and Puech’s Radiosity and Global Illumination (1994), and Ashdown’s Radiosity: A Programmer’s Perspective (1994), all of which primarily describe the ﬁnite-element radiosity method.
In a paper on ray-tracing system design, Kirk and Arvo (1988) suggested many principles that have now become classic in renderer design. Their renderer was implemented as a core kernel that encapsulated the basic rendering algorithms and interacted with primitives and shading routines via a carefully constructed object-oriented interface. This approach made it easy to extend the system with new primitives and acceleration methods. pbrt’s design is based on these ideas.
Another good reference on ray-tracer design is Introduction to Ray Tracing (Glassner 1989a), which describes the state of the art in ray tracing at that time and has a chapter by Heckbert that sketches the design of a basic ray tracer. More recently, Shirley and Morley’s Realistic Ray Tracing (2003) gives an easy-to-understand introduction to ray

54

INTRODUCTION

CHAPTER 1

tracing and includes the complete source code to a basic ray tracer. Suffern’s book (2007) also provides a gentle introduction to ray tracing.
Researchers at Cornell University have developed a rendering testbed over many years; its design and overall structure were described by Trumbore, Lytle, and Greenberg (1993). Its predecessor was described by Hall and Greenberg (1983). This system is a loosely coupled set of modules and libraries, each designed to handle a single task (ray–object intersection acceleration, image storage, etc.) and written in a way that makes it easy to combine appropriate modules to investigate and develop new rendering algorithms. This testbed has been quite successful, serving as the foundation for much of the rendering research done at Cornell.
Radiance was the ﬁrst widely available open source renderer based fundamentally on physical quantities. It was designed to perform accurate lighting simulation for architectural design. Ward described its design and history in a paper and a book (Ward 1994; Larson and Shakespeare 1998). Radiance is designed in the UNIX style, as a set of interacting programs, each handling a different part of the rendering process. This general type of rendering architecture was ﬁrst described by Duff (1985).
Glassner’s (1993) Spectrum rendering architecture also focuses on physically based rendering, approached through a signal-processing-based formulation of the problem. It is an extensible system built with a plug-in architecture; pbrt’s approach of using parameter/value lists for initializing implementations of the main abstract interfaces is similar to Spectrum’s. One notable feature of Spectrum is that all parameters that describe the scene can be functions of time.
Slusallek and Seidel (1995, 1996; Slusallek 1996) described the Vision rendering system, which is also physically based and designed to support a wide variety of light transport algorithms. In particular, it had the ambitious goal of supporting both Monte Carlo and ﬁnite-element-based light transport algorithms.
Many papers have been written that describe the design and implementation of other rendering systems, including renderers for entertainment and artistic applications. The Reyes architecture, which forms the basis for Pixar’s RenderMan renderer, was ﬁrst described by Cook et al. (1987), and a number of improvements to the original algorithm have been summarized by Apodaca and Gritz (2000). Gritz and Hahn (1996) described the BMRT ray tracer. The renderer in the Maya modeling and animation system was described by Sung et al. (1998), and some of the internal structure of the mental ray renderer is described in Driemeyer and Herken’s book on its API (Driemeyer and Herken 2002). The design of the high-performance Manta interactive ray tracer is described by Bigler et al. (2006).
The source code to pbrt is licensed under the BSD License; this has made it possible for other developers to use pbrt code as a basis for their efforts. LuxRender, available from www.luxrender.net, is a physically based renderer built using pbrt as a starting point; it offers a number of additional features and has a rich set of scene export plugins for modeling systems.
Ray Tracing News, an electronic newsletter compiled by Eric Haines dates to 1987 and is occasionally still published. It’s a very good resource for general ray-tracing informa-

EXERCISE

55

tion and has particularly useful discussions about intersection acceleration approaches, implementation issues, and tricks of the trade. More recently, the forums at ompf2.com have been frequented by many experienced ray-tracer developers.
The object-oriented approach used to structure pbrt makes the system easy to understand but is not the only way to structure rendering systems. An important counterpoint to the object-oriented approach is data-oriented design (DoD), a way of programming that has notably been advocated by a number of game developers (for whom performance is critical). The key tenet behind DoD is that many principles of traditional objectoriented design are incompatible with high-performance software systems as they lead to cache-inefﬁcient layout of data in memory. Instead, its proponents argue for driving system design ﬁrst from considerations of the layout of data in memory and how those data are transformed by the program. See, for example, Mike Acton’s keynote at the C++ Conference (Acton 2014).

EXERCISE
1.1 A good way to gain an understanding of pbrt is to follow the process of computing the radiance value for a single ray in a debugger. Build a version of pbrt with debugging symbols and set up your debugger to run pbrt with the killeroo-simple.pbrt scene from the scenes directory. Set breakpoints in the SamplerIntegrator::Render() method and trace through the process of how a ray is generated, how its radiance value is computed, and how its contribution is added to the image. The ﬁrst time you do this, you may want to specify that only a single thread of execution should be used by providing --nthreads 1 as command-line arguments to pbrt; doing so ensures that all computation is done in the main processing thread, which may make it easier to understand what is going on, depending on how easy your debugger makes it to step through the program when it is running multiple threads.
As you gain more understanding about the details of the system later in the book, repeat this process and trace through particular parts of the system more carefully.

SamplerIntegrator::Render() 26

CHAPTER TWO

02 GEOMETRY AND
TRANSFORMATIONS
Almost all nontrivial graphics programs are built on a foundation of geometric classes. These classes represent mathematical constructs like points, vectors, and rays. Because these classes are ubiquitous throughout the system, good abstractions and efﬁcient implementations are critical. This chapter presents the interface to and implementation of pbrt’s geometric foundation. Note that these are not the classes that represent the actual scene geometry (triangles, spheres, etc.); those classes are the topic of Chapter 3. The geometric classes in this chapter are deﬁned in the ﬁles core/geometry.h and core/geometry.cpp in the pbrt distribution, and the implementations of transformation matrices (Section 2.7) are in the ﬁles core/transform.h and core/transform.cpp.
2.1 COORDINATE SYSTEMS
As is typical in computer graphics, pbrt represents three-dimensional points, vectors, and normal vectors with three coordinate values: x, y, and z. These values are meaningless without a coordinate system that deﬁnes the origin of the space and gives three linearly independent vectors that deﬁne the x, y, and z axes of the space. Together, the origin and three vectors are called the frame that deﬁnes the coordinate system. Given an arbitrary point or direction in 3D, its (x , y , z) coordinate values depend on its relationship to the frame. Figure 2.1 shows an example that illustrates this idea in 2D. In the general n-dimensional case, a frame’s origin po and its n linearly independent basis vectors deﬁne an n-dimensional afﬁne space. All vectors v in the space can be expressed as a linear combination of the basis vectors. Given a vector v and the basis vectors vi, there is a unique set of scalar values si such that
v = s1v1 + . . . + snvn.
Physically Based Rendering: From Theory To Implementation. http://dx.doi.org/10.1016/B978-0-12-800645-0.50002-6 Copyright © 2017 Elsevier Ltd. All rights reserved.

58

GEOMETRY AND TRANSFORMATIONS

CHAPTER 2

2
p –4
8
8
Figure 2.1: In 2D, the (x , y) coordinates of a point p are deﬁned by the relationship of the point to a particular 2D coordinate system. Here, two coordinate systems are shown; the point might have coordinates (8, 8) with respect to the coordinate system with its coordinate axes drawn in solid lines but have coordinates (2, −4) with respect to the coordinate system with dashed axes. In either case, the 2D point p is at the same absolute position in space.

The scalars si are the representation of v with respect to the basis {v1, v2, . . . , vn} and are the coordinate values that we store with the vector. Similarly, for all points p, there are unique scalars si such that the point can be expressed in terms of the origin po and the basis vectors
p = po + s1v1 + . . . + snvn.
Thus, although points and vectors are both represented by x, y, and z coordinates in 3D, they are distinct mathematical entities and are not freely interchangeable.
This deﬁnition of points and vectors in terms of coordinate systems reveals a paradox: to deﬁne a frame we need a point and a set of vectors, but we can only meaningfully talk about points and vectors with respect to a particular frame. Therefore, in three dimensions we need a standard frame with origin (0, 0, 0) and basis vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1). All other frames will be deﬁned with respect to this canonical coordinate system, which we call world space.
2.1.1 COORDINATE SYSTEM HANDEDNESS
There are two different ways that the three coordinate axes can be arranged, as shown in Figure 2.2. Given perpendicular x and y coordinate axes, the z axis can point in one of two directions. These two choices are called left-handed and right-handed. The choice between the two is arbitrary but has a number of implications for how some of the geometric operations throughout the system are implemented. pbrt uses a left-handed coordinate system.

SECTION 2.2

VECTORS

59

y

y

z

x

x

z

(a)

(b)

Figure 2.2: (a) In a left-handed coordinate system, the z axis points into the page when the x and y axes are oriented with x pointing to the right and y pointing up. (b) In a right-handed system, the z axis points out of the page.

2.2 VECTORS

pbrt provides both 2D and 3D vector classes. Both are parameterized by the type of the underlying vector element, thus making it easy to instantiate vectors of both integer and ﬂoating-point types.

Vector Declarations ≡ template <typename T> class Vector2 { public: Vector2 Public Methods Vector2 Public Data 59 };

Vector Declarations +≡ template <typename T> class Vector3 { public: Vector3 Public Methods 60 Vector3 Public Data 59 };

In the following, we will generally only include implementations of Vector3 methods; all have Vector2 parallels that have straightforward implementation differences.

A vectors is represented with a tuple of components that gives its representation in terms of the x, y, z (in 3D) axes of the space it is deﬁned in. The individual components of a 3D vector v will be written vx, vy, and vz.

Vector2 Public Data ≡

59

T x, y;

Vector3 Public Data ≡

59

T x, y, z;

60

GEOMETRY AND TRANSFORMATIONS

CHAPTER 2

An alternate implementation would be to have a single template class that is also parameterized with an integer number of dimensions and to represent the coordinates with an array of that many T values. While this approach would reduce the total amount of code, individual components of the vector couldn’t be accessed as v.x and so forth. We believe that in this case, a bit more code in the vector implementations is worthwhile in return for more transparent access to elements.

However, some routines do ﬁnd it useful to be able to easily loop over the components of vectors; the vector classes also provide a C++ operator to index into the components so that, given a vector v, v[0] == v.x and so forth.

Vector3 Public Methods ≡

59

T operator[](int i) const {

Assert(i >= 0 && i <= 2);

if (i == 0) return x;

if (i == 1) return y;

return z;

}

T &operator[](int i) {

Assert(i >= 0 && i <= 2);

if (i == 0) return x;

if (i == 1) return y;

return z;

}

For convenience, a number of widely used types of vectors are given a typedef, so that they have more concise names in code elsewhere.

Vector Declarations +≡ typedef Vector2<Float> Vector2f; typedef Vector2<int> Vector2i; typedef Vector3<Float> Vector3f; typedef Vector3<int> Vector3i;

Readers who have been exposed to object-oriented design may question our decision to make the vector element data publicly accessible. Typically, data members are only accessible inside their class, and external code that wishes to access or modify the contents of a class must do so through a well-deﬁned API of selector and mutator functions. Although we generally agree with this design principle (though see the discussion of data-oriented design in the “Further Reading” section of Chapter 1), it is not appropriate here. The purpose of selector and mutator functions is to hide the class’s internal implementation details. In the case of vectors, hiding this basic part of their design gains nothing and adds bulk to code that uses them.

By default, the (x , y , z) values are set to zero, although the user of the class can optionally supply values for each of the components. If the user does supply values, we check that none of them has the ﬂoating-point “not a number” (NaN) value using the Assert() macro. When compiled in optimized mode, this macro disappears from the compiled code, saving the expense of verifying this case. NaNs almost certainly indicate a bug in the system; if a NaN is generated by some computation, we’d like to catch it as soon as

Assert() 1069 Float 1062 Vector2 59 Vector3 59

SECTION 2.2

VECTORS

61

possible in order to make isolating its source easier. (See Section 3.9.1 for more discussion of NaN values.)

Vector3 Public Methods +≡

59

Vector3() { x = y = z = 0; }

Vector3(T x, T y, T z)

: x(x), y(y), z(z) {

Assert(!HasNaNs());

}

The code to check for NaNs just calls the std::isnan() function on each of the x, y, and z components.

Vector3 Public Methods +≡

59

bool HasNaNs() const {

return std::isnan(x) || std::isnan(y) || std::isnan(z);

}

Addition and subtraction of vectors are done component-wise. The usual geometric interpretation of vector addition and subtraction is shown in Figures 2.3 and 2.4.

Vector3 Public Methods +≡

59

Vector3<T> operator+(const Vector3<T> &v) const {

return Vector3(x + v.x, y + v.y, z + v.z);

}

Vector3<T>& operator+=(const Vector3<T> &v) {

x += v.x; y += v.y; z += v.z;

return *this;

}

The code for subtracting two vectors is similar and therefore not shown here.

Assert() 1069 Vector3 59 Vector3::HasNaNs() 61

w v + w

v v + w
w w

v

v

(a)

(b)

Figure 2.3: (a) Vector addition: v + w. (b) Notice that the sum v + w forms the diagonal of the parallelogram formed by v and w, which shows the commutativity of vector addition: v + w = w + v.

62
w w – v

GEOMETRY AND TRANSFORMATIONS

CHAPTER 2

w

v

w

w – v

v

v

(a)

(b)

Figure 2.4: (a) Vector subtraction. (b) The difference −v − w is the other diagonal of the parallelogram formed by v and w.

A vector can be multiplied component-wise by a scalar, thereby changing its length. Three functions are needed in order to cover all of the different ways that this operation may be written in source code (i.e., v*s, s*v, and v *= s):

Vector3 Public Methods +≡

59

Vector3<T> operator*(T s) const { return Vector3<T>(s*x, s*y, s*z); }

Vector3<T> &operator*=(T s) {

x *= s; y *= s; z *= s;

return *this;

}

Geometry Inline Functions ≡ template <typename T> inline Vector3<T> operator*(T s, const Vector3<T> &v) { return v * s; }
Similarly, a vector can be divided component-wise by a scalar. The code for scalar division is similar to scalar multiplication, although division of a scalar by a vector is not well deﬁned and so is not permitted.
In the implementation of these methods, we use a single division to compute the scalar’s reciprocal and then perform three component-wise multiplications. This is a useful trick for avoiding division operations, which are generally much slower than multiplies on modern CPUs.1
We use the Assert() macro to make sure that the provided divisor is not zero; this should never happen and would indicate a bug elsewhere in the system.

1 It is a common misconception that these sorts of optimizations are unnecessary because the compiler will perform the necessary analysis. Compilers are generally restricted from performing many transformations of this type. For division, the IEEE ﬂoating-point standard requires that x/x = 1 for all x, but if we compute 1/x and store it in a variable and then multiply x by that value, it is not guaranteed that 1 will be the result. In this case, we are willing to lose that guarantee in exchange for higher performance. See Section 3.9 for more discussion of these issues.

Assert() 1069 Vector3 59

SECTION 2.2

VECTORS

63

Vector3 Public Methods +≡

59

Vector3<T> operator/(T f) const {

Assert(f != 0);

Float inv = (Float)1 / f;

return Vector3<T>(x * inv, y * inv, z * inv);

}

Vector3<T> &operator/=(T f) { Assert(f != 0); Float inv = (Float)1 / f; x *= inv; y *= inv; z *= inv; return *this;
}

The Vector3 class also provides a unary negation operator that returns a new vector pointing in the opposite direction of the original one:

Vector3 Public Methods +≡

59

Vector3<T> operator-() const { return Vector3<T>(-x, -y, -z); }

Finally, Abs() returns a vector with the absolute value operation applied to its components.

Geometry Inline Functions +≡ template <typename T> Vector3<T> Abs(const Vector3<T> &v) { return Vector3<T>(std::abs(v.x), std::abs(v.y), std::abs(v.z)); }

Assert() 1069 Float 1062 Vector3 59

2.2.1 DOT AND CROSS PRODUCT
Two useful operations on vectors are the dot product (also known as the scalar or inner product) and the cross product. For two vectors v and w, their dot product (v · w) is deﬁned as:
vxwx + vywy + vzwz.

Geometry Inline Functions +≡ template <typename T> inline T Dot(const Vector3<T> &v1, const Vector3<T> &v2) { return v1.x * v2.x + v1.y * v2.y + v1.z * v2.z; }

The dot product has a simple relationship to the angle between the two vectors:

(v · w) = v w cos θ ,

(2.1)

where θ is the angle between v and w, and v denotes the length of the vector v. It follows from this that (v · w) is zero if and only if v and w are perpendicular, provided that neither v nor w is degenerate—equal to (0, 0, 0). A set of two or more mutually perpendicular vectors is said to be orthogonal. An orthogonal set of unit vectors is called orthonormal.

64

GEOMETRY AND TRANSFORMATIONS

CHAPTER 2

It immediately follows from Equation (2.1) that if v and w are unit vectors, their dot product is the cosine of the angle between them. As the cosine of the angle between two vectors often needs to be computed for rendering, we will frequently make use of this property.
A few basic properties directly follow from the deﬁnition. For example, if u, v, and w are vectors and s is a scalar value, then:
(u · v) = (v · u) (su · v) = s(v · u) (u · (v + w)) = (u · v) + (u · w).

We will frequently need to compute the absolute value of the dot product as well. The AbsDot() function does this for us so that a separate call to std::abs() isn’t necessary.

Geometry Inline Functions +≡ template <typename T> inline T AbsDot(const Vector3<T> &v1, const Vector3<T> &v2) { return std::abs(Dot(v1, v2)); }

The cross product is another useful operation for 3D vectors. Given two vectors in 3D, the cross product v×w is a vector that is perpendicular to both of them. Given orthogonal vectors v and w, then v×w is deﬁned to be a vector such that (v, w, v×w) form a coordinate system.
The cross product is deﬁned as:
(v×w)x = vywz − vzwy (v×w)y = vzwx − vxwz (v×w)z = vxwy − vywx .
A way to remember this is to compute the determinant of the matrix:
ijk v×w = vx vy vz ,
wx wy wz
where i, j , and k represent the axes (1, 0, 0), (0, 1, 0), and (0, 0, 1), respectively. Note that this equation is merely a memory aid and not a rigorous mathematical construction, since the matrix entries are a mix of scalars and vectors.
In the implementation here, the vector elements are converted to double-precision (regardless of the type of Float) before the subtractions in the Cross() function. Using extra precision for 32-bit ﬂoating-point values here protects against error from catastrophic cancellation, a type of ﬂoating-point error that can happen when subtracting two values that are very close together. This isn’t a theoretical concern: this change was necessary to ﬁx bugs that came up from this issue previously. See Section 3.9 for more information on ﬂoating-point rounding error.

AbsDot() 64 Dot() 63 Float 1062 Vector3 59

SECTION 2.2

VECTORS

65

v2 h

v1 v2

v1
Figure 2.5: The area of a parallelogram with edges given by vectors v1 and v2 is equal to v1h. From Equation (2.2), the length of the cross product of v1 and v2 is equal to the product of the two vector lengths times the sine of the angle between them—the parallelogram area.

Float 1062 Vector3 59

Geometry Inline Functions +≡ template <typename T> inline Vector3<T> Cross(const Vector3<T> &v1, const Vector3<T> &v2) { double v1x = v1.x, v1y = v1.y, v1z = v1.z; double v2x = v2.x, v2y = v2.y, v2z = v2.z; return Vector3<T>((v1y * v2z) - (v1z * v2y), (v1z * v2x) - (v1x * v2z), (v1x * v2y) - (v1y * v2x)); }

From the deﬁnition of the cross product, we can derive

v×w = v w sin θ ,

(2.2)

where θ is the angle between v and w. An important implication of this is that the cross product of two perpendicular unit vectors is itself a unit vector. Note also that the result of the cross product is a degenerate vector if v and w are parallel.
This deﬁnition also shows a convenient way to compute the area of a parallelogram (Figure 2.5). If the two edges of the parallelogram are given by vectors v1 and v2, and it has height h, the area is given by v1 h. Since h = sin θ v2 , we can use Equation (2.2) to see that the area is v1×v2 .

2.2.2 NORMALIZATION

It is often necessary to normalize a vector—that is, to compute a new vector pointing in the same direction but with unit length. A normalized vector is often called a unit vector. The notation used in this book for normalized vectors is that vˆ is the normalized version of v. To normalize a vector, it’s ﬁrst useful to be able to compute its length.

Vector3 Public Methods +≡

59

Float LengthSquared() const { return x * x + y * y + z * z; }

Float Length() const { return std::sqrt(LengthSquared()); }

66

GEOMETRY AND TRANSFORMATIONS

CHAPTER 2

Normalize() normalizes a vector. It divides each component by the length of the vector, v . It returns a new vector; it does not normalize the vector in place:
Geometry Inline Functions +≡ template <typename T> inline Vector3<T> Normalize(const Vector3<T> &v) { return v / v.Length(); }

2.2.3 MISCELLANEOUS OPERATIONS
A few additional operations are useful when working with vectors. The MinComponent() and MaxComponent() methods return the smallest and largest coordinate value, respectively.

Geometry Inline Functions +≡ template <typename T> T MinComponent(const Vector3<T> &v) { return std::min(v.x, std::min(v.y, v.z)); } template <typename T> T MaxComponent(const Vector3<T> &v) { return std::max(v.x, std::max(v.y, v.z)); }

Related, MaxDimension() returns the index of the component with the largest value.

Geometry Inline Functions +≡ template <typename T> int MaxDimension(const Vector3<T> &v) { return (v.x > v.y) ? ((v.x > v.z) ? 0 : 2) : ((v.y > v.z) ? 1 : 2); }

Component-wise minimum and maximum operations are also available.

Geometry Inline Functions +≡ template <typename T> Vector3<T> Min(const Vector3<T> &p1, const Vector3<T> &p2) { return Vector3<T>(std::min(p1.x, p2.x), std::min(p1.y, p2.y), std::min(p1.z, p2.z)); } template <typename T> Vector3<T> Max(const Vector3<T> &p1, const Vector3<T> &p2) { return Vector3<T>(std::max(p1.x, p2.x), std::max(p1.y, p2.y), std::max(p1.z, p2.z)); }
Finally, Permute() permutes the coordinate values according to the index values provided.

Vector3 59

SECTION 2.3

POINTS

67

Geometry Inline Functions +≡ template <typename T> Vector3<T> Permute(const Vector3<T> &v, int x, int y, int z) { return Vector3<T>(v[x], v[y], v[z]); }
2.2.4 COORDINATE SYSTEM FROM A VECTOR
We will frequently want to construct a local coordinate system given only a single 3D vector. Because the cross product of two vectors is orthogonal to both, we can apply the cross product two times to get a set of three orthogonal vectors for the coordinate system. Note that the two vectors generated by this technique are unique only up to a rotation about the given vector.
The implementation of this function assumes that the vector passed in, v1, has already been normalized. It ﬁrst constructs a perpendicular vector by zeroing one of the components of the original vector, swapping the remaining two, and negating one of them. Inspection of the two cases should make clear that v2 will be normalized and that the dot product (v1 · v2) must be equal to zero. Given these two perpendicular vectors, a single cross product gives the third, which by deﬁnition will be perpendicular to the ﬁrst two.
Geometry Inline Functions +≡ template <typename T> inline void CoordinateSystem(const Vector3<T> &v1, Vector3<T> *v2, Vector3<T> *v3) { if (std::abs(v1.x) > std::abs(v1.y)) *v2 = Vector3<T>(-v1.z, 0, v1.x) / std::sqrt(v1.x * v1.x + v1.z * v1.z); else *v2 = Vector3<T>(0, v1.z, -v1.y) / std::sqrt(v1.y * v1.y + v1.z * v1.z); *v3 = Cross(v1, *v2); }

Cross() 65 Point3 68 Vector3 59

2.3 POINTS
A point is a zero-dimensional location in 2D or 3D space. The Point2 and Point3 classes in pbrt represent points in the obvious way: using x, y, z (in 3D) coordinates with respect to a coordinate system. Although the same representation is used for vectors, the fact that a point represents a position whereas a vector represents a direction leads to a number of important differences in how they are treated. Points are denoted in text by p.
In this section, we’ll continue the approach of only including implementations of the 3D point methods in the Point3 class here.
Point Declarations ≡ template <typename T> class Point2 { public: Point2 Public Methods 68 Point2 Public Data 68 };

68

GEOMETRY AND TRANSFORMATIONS

CHAPTER 2

Point Declarations +≡ template <typename T> class Point3 { public: Point3 Public Methods 68 Point3 Public Data 68 };

As with vectors, it’s helpful to have shorter type names for commonly used point types.
Point Declarations +≡ typedef Point2<Float> Point2f; typedef Point2<int> Point2i; typedef Point3<Float> Point3f; typedef Point3<int> Point3i;

Point2 Public Data ≡

67

T x, y;

Point3 Public Data ≡

68

T x, y, z;

Also like vectors, a Point3 constructor takes parameters to set the x, y, and z coordinate values.

Point3 Public Methods ≡

68

Point3() { x = y = z = 0; }

Point3(T x, T y, T z) : x(x), y(y), z(z) {

Assert(!HasNaNs());

}

It can be useful to convert a Point3 to a Point2 by dropping the z coordinate. The constructor that does this conversion has the explicit qualiﬁer so that this conversion can’t happen without an explicit cast, lest it happen unintentionally.

Point2 Public Methods ≡

67

explicit Point2(const Point3<T> &p) : x(p.x), y(p.y) {

Assert(!HasNaNs());

}

It’s also useful to be able to convert a point with one element type (e.g., a Point3f) to a point of another one (e.g., Point3i) as well as to be able to convert a point to a vector with a different underlying element type. The following constructor and conversion operator provide these conversions. Both also require an explicit cast, to make it clear in source code when they are being used.

Float 1062 Point2 67 Point3 68

