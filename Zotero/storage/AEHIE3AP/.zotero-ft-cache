Optimizing the Graphics Pipeline with Compute   
Graham Wihlidal  Sr. Rendering Engineer, Frostbite
Hello, my name is Graham Wihlidal, a senior rendering engineer on the Frostbite team at EA. Today I am talking about op>mizing the graphics pipeline with compute, or more speciﬁcally, how to render triangles fast, by not rendering so many triangles.

Acronyms
u Optimizations and algorithms presented are AMD GCN-centric [1][8]
VGT Vertex Grouper \ Tessellator PA Primitive Assembly CP Command Processor IA Input Assembly SE Shader Engine CU Compute Unit LDS Local Data Share
HTILE Hi-Z Depth Compression GCN Graphics Core Next SGPR Scalar General-Purpose Register VGPR Vector General-Purpose Register
ALU Arithmetic Logic Unit SPI Shader Processor Interpolator
To start things oﬀ, I’d like to list a few acronyms. The op>miza>ons and algorithms presented are speciﬁc to AMD GCN hardware, as this ﬁrst version was aimed at “geLng it right” on consoles and high end AMD PCs. The following deﬁni>ons map to GCN hardware concepts, and are used throughout this presenta>on.

During an onsite at MicrosoO with the Advanced Technology Group, while op>mizing Dragon Age: Inquisi>on, it was clear that our displacement mapping was performing poorly on the hardware. Despite small improvements done to the shaders to reduce LDS usage, it was obvious that we were at the mercy of various boTlenecks.
I began experimen>ng with oﬄoading the hull shader adap>ve tessella>on factor calcula>ons to a compute shader and reading the results back in the hull shader. This prototype was quite successful, and included interes>ng approaches like HTILE sourced Hi-Z culling of the triangle patches.

libEdge
This then led me to build a new prototype for regular triangle and patch culling, as a spiritual successor to libEdge on PS3. This prototype was interes>ng; some scenes would be a win, and some would be a complete loss. I played around with various GCN instruc>ons, memory op>miza>ons, and asynchronous compute, and found scenes that showed a signiﬁcant win using compute based triangle culling. At a conference, I started chaLng with Alex Nankervis and James Stanard from MicrosoO, and learned they had been doing their own inves>ga>ons for this [18]. We did a lot of excellent idea sharing back and forth, and my technology deﬁnitely improved thanks to their help. The ﬁnal advancement, was when I met MaThäus Chajdas at AMD Munich, whom was also experimen>ng with compute triangle culling, now >tled GeometryFX [20], and released open source as part of AMD GPUOpen. This sparked a great collabora>on to push this technology further…

… which is now integrated into the Frostbite Engine, on the majority of our >tles moving forward.

/ clock / clock / clock
Lets break down peak triangle rate for our target plahorms, and prove how we can beat it. Both Xbox One and PlaySta>on 4 have 2 shader engines, where each can issue 1 triangle per clock. Newer AMD GPUs on PC have 4 shader engines, so a total of 4 triangles per clock.

12 CU * 64 ALU * 2 FLOPs 1,536 ALU ops / cy
18 CU * 64 ALU * 2 FLOPs 2,304 ALU ops / cy
64 CU * 64 ALU * 2 FLOPs 8,192 ALU ops / cy
If we mul>ply the number of Compute Units (CUs) by the number of ALUs per CU, and mul>ply that value by 2 ﬂoa>ng point opera>ons, since one CU can execute 64 FMA in one cycle, we get the number of ALU ops that can be executed per cycle. There is a technical caveat to men>on with this math. There are actually 4x as many waves running, but each ALU takes 4 clocks, so those two factors of 4 cancel out.

1,536 ALU ops / 2 engines 768 ALU ops per triangle
2,304 ALU ops / 2 engines 1,017 ALU ops per triangle
8,192 ALU ops / 4 engines 2,048 ALU ops per triangle
If we take the number of ALU ops that can be executed per cycle, and divide that by the number of available shader engines, we get the number of ALU ops that can be executed per triangle.

768 ALU ops / 2 ALU per cy = 384 instruction limit
1,017 ALU ops / 2 ALU per cy = 508 instruction limit
2,048 ALU ops / 2 ALU per cy = 1024 instruction limit
Finally, if we divide the number of ALU ops per triangle by the number of ALU ops per clock, we get a ﬁnal instruc>on upper limit that we have to cull a triangle with, and s>ll beat the ﬁxed func>on primi>ve setup and scan converter.

Can anyone here cull a triangle in less than 384 instructions on Xbox One?
… I sure hope so ☺
So the ques>on I propose to the audience, is whether or not you can write a compute shader within 384 instruc>ons that can eﬃciently cull a triangle. I sure hope so ☺

Motivation – Death By 1000 Draws
u DirectX 12 promised millions of draws!
u Great CPU performance advancements u Low overhead u Power in the hands of (experienced) developers u Console hardware is a fixed target 
u GPU still chokes on tiny draws
u Common to see 2nd half of base pass barely utilizing the GPU u Lots of tiny details or distant objects – most are Hi-Z culled u Still have to run mostly empty vertex wavefronts 
u More draws not necessarily a good thing
We now have DirectX 12, and we were promised millions of draws. The new API has givennd great CPU performance advancements through low overhead, and places the power in the hands of experienced developers. However, the GPU s>ll chokes on >ny draws; it is quite common to see the 2 half of the base pass barely u>lizing the GPU. Typically there are lots of >ny details or distant objects, of which most are Hi-Z culled. The eﬃciency loss comes from the GPU s>ll having to run mostly empty vertex wavefronts. More draws are not necessarily a good thing..

Motivation – Death By 1000 Draws
In this GPU capture, you can see on the leO that we start out alright, but very quickly on the right we end up spinning on vertex shader wavefronts that that don’t result in any pixels.

Motivation – Primitive Rate
u Wildly optimistic to assume we get close to 2 prims per cy – Getting 0.9 prim / cy 
u If you are doing anything useful, you will be bound elsewhere in the pipeline 
u You need good balance and lucky scheduling between the VGTs and PAs 
u Depth of FIFO between VGT and PA
u Need positions of a VS back in < 4096 cy, or reduces primitive rate 
u Some games hit close to peak perf (95+% range) in shadow passes
u Usually slower regions in there due to large triangles u Coarse raster only does 1 super-tile per clock u Triangles with bounding rectangle larger than 32x32?
u Multi-cycle on coarse raster, reduces primitive rate
I’ve shown how easy it can be to beat the peak primi>ve rate at a cursory glance. The GPU has a lot more going on, so we s>ll have to proﬁle and op>mize our culling aggressively, especially bandwidth usage. A saving grace is that it is wildly op>mis>c to expect that we’ll get 2 triangles per clock cycle on consoles, as the rasterizer is subjected to other pipeline boTlenecks; on Xbox One I measured an actual rate of 0.9 triangles per clock with regular rendering, which is really quite healthy, as prim rate is not where you want to be bound.
In prac>ce, if you are actually submiLng geometry this fast, and doing any useful rendering, then you will be bound elsewhere in the pipeline, at least during some intervals. Also, you need good balance and lucky scheduling between the two VGTs and PAs to get max rate on each. For instance, the same vertex in two diﬀerent waves might have to be shaded twice, because the waves alternate between PAs, and the PAs operate independently.
Due to the depth of the FIFO between VGT and PA, you need to get the posi>ons of a VS back in less than 4096 cycles, coun>ng from the moment the vertex goes into the FIFO. This leaves you with slightly fewer cycles than that to compute your posi>ons. If your VS takes longer, prim rate goes down linearly. Some games hit very close to peak perf (in the 95+% range) in shadow passes. There are usually some slower regions in there due to large triangles. The coarse rasterizer only does 1 super->le per clock, so triangles with a bounding rectangle larger than 32x32 will need to mul>-cycle on the coarse rasterizer, reducing primi>ve rate.

Motivation – Primitive Rate
u Benchmarks that get 2 prims / cy (around 1.97) have these characteristics:
u VS reads nothing u VS writes only SV_Position u VS always outputs 0.0f for position - Trivially cull all primitives u Index buffer is all 0s - Every vertex is a cache hit u Every instance is a multiple of 64 vertices – Less likely to have unfilled VS waves u No PS bound – No parameter cache usage
u Requires that nothing after VS causes a stall
u Parameter size <= 4 * PosSize u Pixels drain faster than they are generated u No scissoring occurs 
u PA can receive work faster than VS can possibly generate it
u Often see tessellation achieve peak VS primitive throughout; one SE at a time
Benchmarks that get very close to 2 prims/clock (around 1.97) have these characteris>cs: VS reads nothing VS writes only SV_Posi>on VS always outputs 0.0f for posi>on - So every primi>ve is trivially culled Index buﬀer is all 0's So every vertex is a cache hit. Cache hits don't count as verts for purposes of peak vertex rate. That's the only way we can get near 2 prims/clock without hiLng 2 ver>ces/clock ﬁrst. Every instance is a mul>ple of 64 ver>ces - Makes unﬁlled VS waves less likely No PS bound - So no parameter cache usage
The peak primi>ve rate also requires that nothing aOer VS causes a stall. - ParamSize <= 4 * PosSize - Pixels drain faster than they are generated - No scissoring occurs
Apart from that, the PA can receive work faster than VS can possibly generate it. We oOen see tessella>on achieve peak VS primi>ve throughput - for one SE at a >me.

Motivation – Opportunity
u Coarse cull on CPU, refine on GPU  u Latency between CPU and GPU prevents optimizations  u GPGPU Submission!
u Depth-aware culling
u Tighten shadow bounds \ sample distribution shadow maps [21] u Cull shadow casters without contribution [4] u Cull hidden objects from color pass 
u VR late-latch culling
u CPU submits conservative frustum and GPU refines 
u Triangle and cluster culling
u Covered by this presentation
Engines typically do various methods of coarse culling on the CPU, prior to GPU submission. Due to latency between CPU and GPU, many op>miza>ons are inappropriate, or It would mean >ght lock stepping. The CPU is a limited resource on consoles, and this isn’t a great use of a CPU core. On PC you have to get the data over PCIE which would be prohibi>ve. Because of this, we want the culling to happen on the GPU’s >meline, so the solu>on is to do GPGPU submission. GPU based approaches include depth-aware culling, VR late-latch culling, or triangle and cluster culling, which is covered by this presenta>on.

Motivation – Opportunity
u Maps directly to graphics pipeline
u Offload tessellation hull shader work u Offload entire tessellation pipeline! [16][17] u Procedural vertex animation (wind, cloth, etc.) u Reusing results between multiple passes & frames
u Maps indirectly to graphics pipeline
u Bounding volume generation u Pre-skinning u Blend shapes u Generating GPU work from the GPU [4] [13] u Scene and visibility determination 
u Treat your draws as data!
u Pre-build u Cache and reuse u Generate on GPU
Compute shader mesh processing opens up opportuni>es for more eﬃciently suppor>ng a variety of high ﬁdelity features and improvements. BeTer yet, by reusing post-shader results between mul>ple passes, and doing less draw setup work on the CPU, there is increased op>miza>on poten>al.
The mantra is to treat all your draws as regular data. The data can be pre-built, cached and reused, and generated on the GPU. This approach allows us increased ﬂexibility, including the ability to work around various ﬁxed func>on boTlenecks.

Culling Overview
I’m going to start oﬀ by giving an overview of the culling methods

Culling Overview
u Consists of:
u Collection of meshes u Specific view
u Camera, light, etc.

Scene

Lets ﬁrst deﬁne some terms to reduce confusion. A scene consists of a collec>on of meshes, displayed from a speciﬁc view

Culling Overview
u Configurable subset of meshes in a scene 
u Meshes within a batch share the same shader and strides (vertex/index)

Batch

u Near 1:1 with DirectX 12 PSO
(Pipeline State Object)

Then we have a batch, which is a conﬁgurable subset of meshes in a scene. Except on Xbox One, we require all meshes in a batch to share the same shader, and also that all meshes share the same vertex and index strides. These requirements are due to the way that GPU driven rendering works currently, at least on PC. A batch here can be thought of as a near 1 to 1 with DirectX 12’s Pipeline State Object concept, or PSO.

Culling Overview
Mesh Section
u Represents an indexed draw call (triangle list) 
u Has its own:
u Vertex buffer(s) u Index buffer u Primitive count u Etc.
We also have a mesh sec>on, which represents an indexed draw call. A mesh sec>on has its own vertex buﬀers, index buﬀer, primi>ve count, and other proper>es.

Culling Overview
Work Item
u Optimal number of triangles for processing in a wavefront
u AMD GCN has 64 threads per wavefront
u Each culling thread processes 1 triangle
u Work item processes 256 triangles
Lastly, we have a work item, which represents a subset of triangles in a batch that will be processed by a compute shader wavefront. The number of triangles has been chosen based on the underlying hardware, and characteris>cs of the algorithm. AMD GCN has 64 threads per wavefront (which includes both consoles), each culling thread processes 1 triangle, and each work item processes 256 triangles.

Culling Overview

Scene

Batch

Batch

Mesh Section

Mesh Section

Mesh Section

Mesh Section

Work Item Work Item Work Item Work Item Work Item Work Item Work Item Work Item

Culling Culling Culling Culling Culling Culling

Draw Args

Draw Args

Draw Args

Draw Call Compaction (No Zero Size Draws)

Draw Args

Draw Args Draw Args

Multi Draw Indirect

…

Culling Culling
Draw Args

Here is a high level overview of how a scene breaks down into work items that ﬁrst undergo coarse view culling, and then surviving clusters undergo triangle culling, with a variety of tests. We run a quick compac>on pass that ensures we do not have zero size draws if a mesh sec>on is en>rely culled (like in the case of occlusion or frustum culling).
At the end of the pipeline, we have a group of indexed draw arguments that we can kick from the GPU with ExecuteIndirect on DirectX 12, or via the AMD_mul>_draw_indirect extension on OpenGL.
On Xbox One, ExecuteIndirect has some incredible extensions where PSOs can be switched by indirect arguments, meaning we can issue a single ExecuteIndirect for our en>re scene, regardless of state or resource changes.

Mapping Mesh ID to MultiDraw ID
u Indirect draws no longer know the mesh section or instance they came from
u Important for loading various constants, etc.
u A DirectX 12 trick is to create a custom command signature
u Allows for parsing a custom indirect arguments buffer format u We can store the mesh section id along with each draw argument block u PC drivers use compute shader patching u Xbox One has custom command processor microcode support 
u OpenGL has gl_DrawId which can be used for this
u SPI Loads StartInstanceLocation into reserved SGPR and adds to SV_InstanceID
u A fallback approach can be an instancing buffer with a step rate of 1 which maps from instance id to draw id
Construc>ng each draw argument block is fairly straight forward, it’s mostly a maTer of determining what star>ng index and count each block is responsible for during rendering. However, things get complicated when you try to load constants or other resource data from a regular vertex or pixel shader, unaware that this culling pass has occurred. In order to avoid state changes, we have an instancing buﬀer that contains the transforms, colors, etc. per instance, but in this case it’s no longer one to one with a draw call. Essen>ally, we need to add a custom 32 bit word to the argument buﬀer that tracks what original draw index it is associated with. A DirectX 12 trick is to create a custom command signature. Doing so allows for parsing a custom indirect arguments buﬀer format, where we can store a custom id packed alongside the other hardcoded draw indexed argument values. On PC, drivers use compute shader patching, where the id is loaded into a register for a shader to reference per invoca>on. On OpenGL, you can use gl_DrawId for this purpose. The command processor microcode on Xbox One handles indirect draws without intermediate steps or patching, which is extremely op>mal. An alterna>ve would be to bind a buﬀer with per-instance step rate of 1 which maps from instance id to draw id. Depending on the driver implementa>on, this might be faster than the root constant approach for the >me being while drivers mature.

Mapping Mesh ID to MultiDraw ID
Draw Args
Mesh Section Id Index Count Per Instance
Instance Count Start Index Location Base Vertex Location Start Instance Location
Here you can see the appropriate command signature descrip>on to deﬁne the custom format that’s displayed on the right. Argument 0 deﬁnes the 32 bitthmesh sec>on id, including the parameter index into the root signature. Argument 1 then follows, which is the ﬁxed list of 5 arguments that make up a draw indexed packet. This mapping will cause the 0 word of your argument block to be loaded into an SGPR register for use by the shader. On PC, having a command signature with complex commands will cause ExecuteIndirect processing to go through a compute shader. However, having a single extra word to represent the draw id will remain on a fast path – similar to AGS Mul>DrawIndirect or gl_DrawId.

De-Interleaved Vertex Buffers

P0 N0 TC0 P1 N1 TC1 P2 N2 TC2 … Do This!

P0 P1 P2 P3 … N0 N1 N2 N3 … TC0 TC1 TC2 TC3 …

Draw Call

Draw Call

De-Interleaved vertex buffers are optimal on GCN architectures They also make compute processing easier!

Another architectural note is that we have de-interleaved our vertex buﬀers. This can be a substan>al win on GCN architectures, and it also makes the task of compute mesh processing much easier.

De-Interleaved Vertex Buffers
u Helpful for minimizing state changes for compute processing
u Constant vertex position stride 
u Cleaner separation of volatile vs. non-volatile data  u Lower memory usage overall  u More optimal for regular GPU rendering  u Evict cache lines as quickly as possible!
There are a number of reasons that de-interleaving your vertex data is beneﬁcial. In terms of compute processing performance, having culling data like posi>on in its own stream away from other aTributes like UVs, colors, TSB, etc. means that we have an almost never changing stride. The only >me you would need to break batching would be 16 bit vs 32 bit precision. Consoles and DirectX 12 placement resources can be spanned across all the geometry data, meaning that with a constant stride, and some pointer arithme>c to determine the right start vertex and index loca>on for each draw, we can completely avoid binding varying buﬀers throughout rendering! In addi>on to algorithmic beneﬁts, de-interleaving your vertex data is more op>mal for regular GPU rendering on GCN architectures, so there’s really no excuse. We want to evict cache lines as quickly as possible. With interleaved data, the cache line needs to be kept between the ﬁrst and the last read. With de-interleaved data and inlined fetch shaders, the wavefront fetches a cache line, consumes it, and it throws the cache line away immediately. An addi>onal beneﬁt, is that de-interleaving delivers faster processing on the CPU, as the data will be SoA instead of AoS, making it easier to process with SSE/AVX, etc., and the same advantage applies on the GPU. If you want to be the most op>mal across mobile, AMD and other IHVs, it is common to at least have mul>ple interleaved streams of mutable vs immutable data, posi>ons in their own streams (op>onally with UVs in the case of alpha tested shadows), skinning data, and other common data grouped together. Another advantage of de-interleaved vertex buﬀers is that you can create separate index buﬀers per pass. A depth-only pass (like for culling) can have more vertex re-use than a full pass, because you oOen need to duplicate ver>ces for full rendering (same posi>on but diﬀerent texcoord, or same posi>on but diﬀerent normal).

Cluster Culling
Before geLng into the per triangle culling, it’s important to touch on coarse culling of triangle clusters

Cluster Culling
u Generate triangle clusters using spatially coherent bucketing in spherical coordinates  u Optimize each triangle cluster to be cache coherent  u Generate optimal bounding cone of each cluster [19]
u Project normals on to the unit sphere u Calculate minimum enclosing circle u Diameter is the cone angle u Center is projected back to Cartesian for cone normal 
u Store cone in 8:8:8:8 SNORM  u Cull if dot(cone.Normal, -view) < -sin(cone.angle)
In order to make eﬃcient use of the GPU, we ﬁrst do a coarse GPU culling pass of our mesh data. An oﬄine process par>>ons meshes into 256 triangle clusters using a greedy spa>ally and cache coherent bucke>ng algorithm. For each cluster, we generate an op>mal bounding cone. The general idea is to project each triangle normal on to the unit sphere, and take this 256 projected normal collec>on and calculate a minimum enclosing circle against the phi and theta pairs. Since the algorithm is opera>ng on a diﬀerence of angles, we can use the circle diameter as the cone angle, and project the center back to Cartesian for the cone normal. <Commence primary igni>on> 4 component 8bit SNORM has enough precision to store this cone, which can be culled on the GPU by taking the dot product of the cone normal and a conserva>ve cluster-centroid view vector and comparing it to the nega>ve sine cone angle. The obvious op>miza>on is to store the cone angle with the nega>ve sine calculated in to the value. You’ll want to make an allowance for rounding, like slightly enlarging the cone angle to avoid false rejec>on. The cone normal will quan>ze as well. Any of the gbuﬀer encoding to improve normal accuracy would be applicable here, as long as they are not the ones that bias depth precision towards viewer facing.

Cluster Culling
u 64 is convenient on consoles
u Opens up intrinsic optimizations u Not optimal, as the CP bottlenecks on too many draws u Not LDS bound 
u 256 seems to be the sweet spot
u More vertex reuse u Fewer atomic operations 
u Larger than 256?
u 2x VGTs alternate back and forth (256 triangles) u Vertex re-use does not survive the flip
For determining the ideal cluster size, I did a lot of proﬁling of various conﬁgura>ons. 64 is a convenient size on consoles, as this opens up intrinsic op>miza>ons. However, I found this to be sub-op>mal, as the CP boTlenecks on too many draws, and we were never bound by LDS atomics. Based on proﬁling, a cluster size of 256 seems to be the sweet spot. The 2 VGTs ﬂip back and forth every 256 triangles, and vertex re-use does not survive the ﬂip, making a cluster size of 256 a wise choice.

Cluster Culling
u Coarse reject clusters of triangles [4]  u Cull against:
u View (Bounding Cone) u Frustum (Bounding Sphere) u Hi-Z Depth (Screen Space Bounding Box)
u Be careful of perspective distortion! [22] u Spheres become ellipsoids under projection
This approach allows for us to coarse reject en>re clusters of triangles prior to the per triangle culling pass. Addi>onal per cluster tests include bounding sphere vs frustum, and tes>ng the bounding sphere’s screen space bounding box against a Hi-Z depth pyramid. Be careful that you account for perspec>ve distor>on, as spheres become ellipsoids under projec>on. I won’t go into further detail on cluster culling, as it’s covered in great detail in the excellent GPU-Driven Rendering Pipelines presenta>on [4] from SIGGRAPH last year.

Draw Compaction
With cluster and triangle culling of draws on the GPU, it’s extremely important to remove zero sized draws from submission.

Compaction
At 133us - Efficiency drops as we hit a string of empty draws At 151us - 10us of idle time
The grey draws in this GPU capture are empty draw indirects. At ﬁrst, the command processor (CP) cost is hidden by in-ﬂight draws. Around 133us, the eﬃciency drops as we hit a string of empty draws. At 151us, we suﬀer around 10us of idle >me. However, the total impact is worse than 10us, since the GPU doesn’t instantly resume 100% eﬃciency, as it takes >me to ﬁll the CUs with waves. Clusters of culled draws can easily overwhelm the command processor, which is poten>ally 1.5ms in a 60hz frame (seen in actual shipped games with real content). While the savings from GPU culling s>ll exceeds this cost, it is very important that we compact zero size draws in order to get the biggest gain. Even with 0 primi>ves, fetching indirect argument isn’t free; there is a memory latency of ~300ns. The CP can hide a few of these in a row, but they add up. Addi>onally, the CP is consuming command buﬀer packets, and state changes aren’t free.

Compaction
Count = Min(MaxCommandCount, pCountBuffer)
The CPU will issue the worst case number of draws, so zero size draws will cause the GPU to process indirect args even if they have zero surviving primi>ves. The GPU needs control over the draw count and state changes. The ExecuteIndirect API in DirectX 12 has an op>onal count buﬀer and oﬀset, which the GPU will use to clamp the upper bound of draws that the command processor will unroll. Some IHVs currently patch this value with a compute shader, or run other sub-op>mal paths. However, the feature is new, and widespread use will encourage IHVs to improve the drivers in this area.

Compaction
u Parallel Reduction u Keep > 0 Count Args
We can do better!
A cross plahorm approach to draw compac>on is to do a parallel reduc>on with atomics in group shared memory. Each thread loads the indirect arguments for a draw and determines if the draw is worth keeping. A barrier allows all threads to complete and then the ﬁrst thread in a group allocates output space for the surviving draw args. Another barrier is performed so each thread gets the output loca>on, and then the surviving draw args are wriTen to the des>na>on buﬀer. In this example, batchData is the ExecuteIndirect count buﬀer, and the oﬀset is the loca>on of drawCountCompacted. With GCN intrinsics, and a thread group size of 64, we can do beTer!

Compaction
u Parallel prefix sum to the rescue!
0001233445567889
The issue with op>mizing the compac>on is that each thread needs to write in a con>guous range, so using the thread id as the index wouldn’t give us this, and we want to avoid global synchroniza>on like the previous compac>on algorithm. This is where parallel preﬁx sum comes to the rescue! On the boTom range, you can see the indices we want computed per thread in order for each ac>ve thread to write into the correct con>guous slot.

Compaction
u __XB_Ballot64
u Produce a 64 bit mask u Each bit is an evaluated predicate per wavefront thread
__XB_Ballot64(threadId & 1)
0101010101010101
The ﬁrst thing to men>on is a compiler intrinsic known as ballot. Ballot can be used to construct a 64 bit mask where each bit is an evaluated predicate per wavefront thread. For inac>ve threads, based on the execu>on mask, the bit will be 0 for these threads. In this example, you can see a predicate that sets 0 for even threads, and 1 for odd threads. NV has a 32 wide NvBallot instruc>on available since Fermi: hTps://www.opengl.org/registry/specs/NV/shader_thread_group.txt

Compaction

__XB_Ballot64(indexCount > 0)

1001110101011101

& Thread 5

Thread 5 Execution Mask

1111100000000000

=

Population Count “popcnt” = 3

1001100000000000

Taking ballot further, we have this example showing thread 5. Before thread 5, we have three other threads that are valid, so we want to calculate the value 3 for thread 5’s output slot. By using ballot to generate a bit mask of surviving draw calls, we can & this mask against a thread execu>on mask where all bits are 0 except for threads lower than the current thread. In this example, we can see the execu>on mask for thread 5, with only bits 0 to 4 set. Looking at the resultant bit range, one can see that a popula>on count of the 1s will produce our expected output slot.

Compaction
u V_MBCNT_LO_U32_B32 [5]
u Masked bit count of the lower 32 threads (0-31) 
u V_MBCNT_HI_U32_B32 [5]
u Masked bit count of the upper 32 threads (32-63) 
u For each thread, returns the # of active threads which come before it.
GCN has two instruc>ons that can be paired with ballot to produce the correct compac>on results. V_MBCNT_LO will produce a masked bit count of the lower 32 threads, and V_MBCNT_HI will produce a masked bit count of the upper 32 threads. Chaining these instruc>ons together will, for each thread, count the # of ac>ve threads which come before it, similar to the reference implementa>on listed here.

Compaction

__XB_MBCNT64(__XB_Ballot64(indexCount > 0))

1001110101011101

0001233445567889

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15

Combining ballot and masked bit count will compact our surviving draw call stream within a wavefront without the need for any synchroniza>on or group shared memory.

Compaction
u No more barriers!  u Atomic to sync
multiple wavefronts  u Read lane to replicate
global slot to all threads
The GCN op>mized compac>on shader looks like this. We no longer have any barriers. In order to compact across mul>ple wavefronts, we have a single atomic opera>on per wavefront that reserves the output space for all the surviving draw calls across each wavefront. Instead of using a barrier so that all threads get globalSlot calculated correctly, we can read the value of globalSlot from the lane that computed it.

Triangle Culling
I’m now going to go over the per triangle culling ﬁlters performed on clusters that survive the ini>al coarse culling.

Per-Triangle Culling
u Each thread in a wavefront processes 1 triangle u Cull masks are balloted and counted to determine compaction index u Maintain vertex reuse across a wavefront u Maintain vertex reuse across all wavefronts - ds_ordered_count [5][15]
u +0.1ms for ~3906 work items – use wavefront limits
As men>oned already, each thread in a wavefront processes 1 triangle. Various culling opera>ons are applied, and the surviving triangles across a wavefront need to be counted to determine the compac>on index, or, the loca>on in the resultant index buﬀer where the surviving indices will be wriTen. This step is important for maintaining vertex reuse across a wavefront. Each wavefront then writes out the block of surviving indices to its output loca>on. If ordering across all wavefronts is important, such as with translucent or procedural rendering, then the block of surviving indices can be wriTen out in wavefront crea>on order using ds_ordered_count. I found that using ds_ordered_count to maintain vertex reuse across an en>re mesh was usually not worth the cost, as work items of 256 triangles gives perfect vertex reuse. The factors contribu>ng to the added cost are due to the way ds_ordered_count works under the hood, the size of the vertex cache, and what happens to vertex reuse when you start to remove parts of the mesh. If using ds_ordered_count, you can op>mize it further through carefully tuned wavefront limits.

Per-Triangle Culling
For Each Triangle Unpack Index and Vertex Data (16 bit) Orientation and Zero Area Culling (2DH) Perspective Divide (xyz/w) Depth Culling – Hi-Z (NDC) Small Primitive Culling (NDC) Frustum Culling (NDC) Count Number of Surviving Indices Compact Index Stream (Preserving Ordering) Reserve Output Space for Surviving Indices Write out Surviving Indices (16 bit)

Scalar Branch (!culled)
Scalar Branch (!culled) Scalar Branch (!culled) __XB_BALLOT64 __XB_MBCNT64 __XB_GdsOrderedCount 
(Optional)

This is an overview of opera>ons that the cull shader is performing on 1 triangle per thread across each work item.
Triangle data is unpacked, the various culling ﬁlters are executed, count/compac>on/reserve is performed, and then the indices are wriTen out as 16 bit. Since compute cannot write out 16 bit types, I ﬁrst zero the output buﬀer, use & 1 as a predicate on the thread id to determine low or high masking, and use InterlockedOr against the output loca>on. This cleverly uses the L2 cache as a write combiner.
Another important op>miza>on to men>on, is that on consoles, you can use branch on a comparison with ballot to give the compiler a scalar branch uniformity hint in order to improve your code gen.

Per-Triangle Culling
u Without ballot u Compiler generates two tests for most if-statements u 1) One or more threads enter the if-statement u 2) Optimization where no threads enter the if-statement
u With ballot (or high level any/all/etc.), or if branch on scalar value (__XB_MakeUniform) u Compiler only generates case# 2 u Skips extra control flow logic to handle divergence
u Use ballot for force uniform branching and avoid divergence u No harm letting all threads execute the full sequence of culling tests
Without ballot, the compiler will generate two tests for most if statements - one is for the case where one or more threads enter the if-statement, and the other is an op>miza>on where the compiler will check to see if everyone *didn't* enter the if-statement, and if so it branches over the if-statement. Really it's just a single comparison test and the compiler essen>ally checks to see if all lanes had the same result, so the compiler is basically genera>ng a ballot for you. So you get code gen that looks like the top block. If you explicitly use ballot (or any/all/etc. which are high-level versions of ballot), or if you branch on a scalar value (i.e. __XB_MakeUniform), the compiler only generates the single "if (allNotEntering) goto end;" part and skips the extra control ﬂow logic to handle divergence. In the case of the culling work loop, I use ballot to force uniform branching and avoid divergence (including the slight codegen hit), because there's no harm in leLng all threads execute the full sequence of culling tests. If any thread needs to run that code, then all threads end up running it because of the SIMD being 64-wide. There is a case where you should use divergent branching - if any of the culling tests involve memory fetches or LDS ops, it is worth masking those out, such as with depth Hi-Z culling.

Orientation Culling
The ﬁrst, and most important ﬁlter, is orienta>on culling.

Triangle Orientation and Zero Area (2DH)
On average, 50% of a mesh will be culled from backface. So we need a test which is as cheap as possible. One of the cheapest tests is the one described in this paper [3], using the determinant of a 3x3 matrix with homogeneous coordinates [2]. This technique avoids clipping and projec>on, which includes ¼ rate reciprocal instruc>ons coming from the perspec>ve divide. Using GCN speciﬁc op>miza>ons, we can skip all the tests aOerwards if backfacing already removed all the triangles within a wavefront. The direc>on of the determinant test is based on whether you are culling front or back facing triangles. This par>cular test works under MSAA or EQAA condi>ons, as the zero area is not a small primi>ve test, but a degenerate triangle test (which any decent mesh pipeline should be removing oﬄine, anyways).

Here is an example of the backface determinant test applied to Solas from a par>cular view.

Locking the current view, and then moving behind him shows that all backfacing triangles have been removed, as expected.

Patch Orientation Culling
When culling tessellated patches, it is also important to men>on that the 2DH determinant test will not work correctly for back faces that displace into view. These faces would be culled pre-displacement, so you would lose a contribu>ng por>on of your silhoueTe. For tessellated patches, we instead do back face culling in view space with a dot product bias that is determined by the max displacement amount.

Small Primitive Culling
Another eﬀec>ve ﬁlter is small primi>ve, or, culling triangles that do not generate pixel coverage.

Rasterizer Efficiency

16 pixels / clock 100% Efficiency

12 pixels / clock 75% Efficiency
1 pixel / clock 6.25% Efficiency

Each GCN rasterizer can read one triangle per clock and produce up to 16 pixels per clock. Because of this, small triangles are extremely ineﬃcient to rasterize.
The leO image produces 4 quads, 16 pixels, at peak eﬃciency. The middle image produces 4 quads, but only 12 pixels are valid. It consumes 16 threads in the pixel shader though, due to helper lanes. Helper lanes s>ll take >me to pack and prepare, so they actually hurt your pixel rate. Eﬃciency in the middle image is lost due to par>ally ﬁlled quads, as the GPU shades in blocks of 2x2 pixel quads.
The right image has become bound by hiLng primi>ve setup limits.

Vi

Rasterizer Efficiency
Vj Vj
Vi
While not directly related to culling, this helpful pixel shader will iden>fy meshes that are too dense, which will be aﬀec>ng how many pixels are being delivered per clock. This is done by measuring the number of helper pixels. Or in other words, the number of covered pixels divided by the number of threads in a wavefront. MSAA can have valid pixel threads with out-of-range barycentric coordinates, so switching from linear center to linear centroid will make it more accurate in this case. I used this to get a rough idea of how prevalent small triangles were in our content, and whether or not a small primi>ve ﬁlter would be eﬀec>ve. As a bonus, this tool can now be used by ar>sts to get a sense for how dense their meshes are given their LOD seLngs, and decimate accordingly.

Small Primitive Culling (NDC)
u This triangle is not culled because it encloses a pixel center
any(round(min) == round(max))
I originally started with a very exhaus>ve ﬁxed point hardware precise small primi>ve ﬁlter, but later changed it to the approxima>on you see here, for non-MSAA targets. MSAA targets need to bias the test by enlarging based on sample count. If using custom programmable sample points, well, you’re on your own. For MSAA, we need to essen>ally determine the maximum distance (in subpixels) between the pixel center and the outermost subpixel sample and use this to inﬂuence the test. The general idea is that you take a screen space bounding box of a triangle, and snap min and max to the nearest pixel corner. If the min and max snap to either the same horizontal or ver>cal edge, the triangle does not enclose a pixel center, therefore not contribu>ng pixel coverage. In this example, this triangle is not culled because it encloses a pixel center.

Small Primitive Culling (NDC)
u This triangle is culled because it does not enclose a pixel center
any(round(min) == round(max))
In a simple case, this triangle is culled because the min and max snap to the same loca>on.

Small Primitive Culling (NDC)
u This triangle is culled because it does not enclose a pixel center
any(round(min) == round(max))
In a more complex case, this triangle is also culled. The min and max snap to diﬀerent ver>cal coordinates, but the same horizontal coordinate.

Small Primitive Culling (NDC)
u This triangle is not culled because the bounding box min and max snap to different coordinates
u This triangle should be culled, but accounting for this case is not worth the cost
any(round(min) == round(max))
This test is conserva>ve, so there is a case where triangles should be culled, but are not, as shown by this example. The bounding box min and max snap to diﬀerent ver>cal and horizontal coordinates, yet the triangle does not enclose any pixel centers. Accoun>ng for this case is not worth the cost, considering how cheap this test is.

Here is an example of the small primi>ve test applied to Solas, standing in the middle of the room, from a par>cular view.

Locking the view, and moving over to him shows quite a number of sub pixel triangles that have been removed with this ﬁlter.

The projector may not show the removed triangles very well, so hopefully this enlarged version does a beTer job. No>ce quite a number of removed triangles from the hands, head, and highly detailed pelt over his back. This extra concentra>on of triangles is typically due to importance of ﬁdelity during close up cinema>c shots during gameplay.

Frustum Culling
The next per triangle ﬁlter to cover is frustum culling

Frustum Culling (NDC)

0,0

Max.X < 0

Max

Max.Y < 0
1,0
Max
Min

Min

0,1

1,1

Min.Y > 1

Min.X > 1

Most engines have whole object frustum culling on the CPU, making per cluster or triangle GPU frustum culling only eﬀec>ve when these objects intersect the planes. AOer the earlier culling ﬁlters, we now have post-projec>on ver>ces, and a huge budget of available ALU, so we do trivial frustum culling of 4 planes in 4 cycles, which s>ll does provide some beneﬁt in fringe cases, especially for composite objects which are made up of many parts.
Near and far plane culling is usually not worth the ALU for most >tles. Similar to back face culling, it is important to men>on that tessellated patches also require some form of tolerance values, in order to prevent incorrect culling of patches which tessellate from outside to inside of the view.

Here is an example of the frustum culling ﬁlter. This is the current view with just frustum culling enabled. We have an object which survives CPU frustum culling, but there are s>ll quite a lot of triangles that could be removed.

AOer locking the view, moving backwards shows us how many triangles were removed using this ﬁlter.

Depth Culling
The last ﬁlter, and the one which is the most involved to implement, is depth culling.

Depth Tile Culling (NDC)
u Another available culling approach is to do manual depth testing u Perform an LDS optimized parallel reduction [9], storing out the conservative
depth value for each tile
16x16 Tiles
Another available triangle culling approach is to do manual depth tes>ng. However it is worth no>ng that directly reading depth for cluster or triangle culling is extremely scene dependent due to availability, and the quality of occluders at any given >me. The general technique is to take the depth buﬀer and perform an LDS op>mized parallel reduc>on [9], storing out the conserva>ve depth min or max value for each >le. In my ini>al tests, I ran a full z pre-pass that produced a 16x16 depth >le grid, which I then tested against a screen space bounding box of the triangle or cluster. If the box was fully contained within a single >le, I would do a fast depth test and reject it. This approach, while fast, would only remove a frac>on of triangles; any occluded triangles that straddled a >le border wouldn’t be rejected. Modifying the ﬁlter to cull larger triangles spanning mul>ple >les would be extremely expensive, and not worth the cost.

Depth Tile Culling (NDC)

u ~41us on XB1 @ 1080p  u Bypasses LDS storage  u Bandwidth bound
u Shared with our light tile culling

Here is a variant of parallel depth reduc>on which uses GCN lane swizzling to share data, bypassing LDS storage. With a 16 bit ESRAM depth buﬀer already decompressed, this computa>on runs in approximately 41 microseconds on the XB1 @ 1080p, and is completely bandwidth bound. We use the results from this reduc>on for other parts of our rendering including light >le culling.

Depth Pyramid Culling (NDC)
u Another approach to depth culling is a hierarchical Z pyramid [10][11][23] u Populate the Hi-Z pyramid after depth laydown u Construct a mip-mapped screen resolution texture u Culling can be done by comparing the depth of a bounding volume with the depth stored in
the Hi-Z pyramid 
int mipMapLevel = min(ceil(log2(max(longestEdge, 1.0f))), levels - 1);
Another approach to depth culling is a hierarchical Z pyramid [10][11], which starts at the resolu>on of the depth buﬀer, and goes all the way to a single pixel. The ﬁrst level of the pyramid is populated aOer depth laydown, similar to the depth >les method. AOer which, we populate the remaining mip levels in the pyramid through a custom downsample pass. Each texel in mip level N contains the min or max depth of all corresponding texels in mip level N-1. Culling can be done by comparing the depth of a bounding volume’s longest edge with the depth stored in the Hi-Z pyramid. Because the pyramid goes down to a single level, we can very easily get a single mip level to fetch, instead of using mul>ple fetches to handle overlapping quads. This is the approach I ended up using for depth based culling, except I also accelerated it with HTILE.

AMD GCN HTILE
u Depth acceleration meta data called HTILE [6][7] u Every group of 8x8 pixels has a 32bit meta data block u Can be decoded manually in a shader and used for 1 test -> 64 pixel rejection
u Avoids slow hardware decompression or resummarize u Avoids losing Hi-Z on later depth enabled render passes

DEPTH

HTILE

GCN has a depth accelera>on meta data called HTILE [7] which accelerates regular GPU depth opera>ons. Every 8x8 group of pixels has a corresponding 32 bit meta data block. While this meta data accelerates regular GPU depth opera>ons, it can be decoded manually in a shader and used for early rejec>on of 64 pixels with a single test, or for any other relevant purpose.
HTILE is usually imprecise, and the bounds must be conserva>ve. Addi>onally, the bounds can only grow un>l you “resummarize”, where every depth value must be read in order to recompute the bounds.
On consoles, HTILE is used to give us conserva>ve depth tes>ng without having to decompress the depth buﬀer for tes>ng in a shader, or disabling Hi-Z on subsequent depth enabled render passes. We have a decompression compute shader which binds the HTILE surface as an R32 UINT texture, manually decodes the >le informa>on, and produces a depth texture.
There are some gotchas with using HTILE, but manual HTILE decoding or encoding can be a big performance win in a variety of scenarios. Currently, HTILE is only directly accessible to console developers.

AMD GCN HTILE
When compu>ng the ﬁrst downsampled mip level of our Hi-Z pyramid, we can leverage the fact we’ve already read the input depth values. So we can also perform full and\or half res lineariza>on of the depth values, and we can also write out half resolu>on HTILE so other passes like par>cles can use Hi-Z culling against that mip level, without needing to resummarize the half resolu>on depth buﬀer. Since we need to build each HTILE meta data block from 64 pixels, we can’t just use our already reduced 4 to 1 min\max values. We need to parallel reduce all pixels in an 8x8 >le to produce the correct min and max values for HTILE. You could do a parallel reduc>on in LDS like all the cool kids do, or you could be even more awesome and do it with lane swizzling.

AMD GCN HTILE

DS_SWIZZLE_B32 [5] V_READLANE_B32 [5]

Because each HTILE entry represents an 8x8 pixel block, we can use a wave wide min and max opera>on across 64 depth values in a >le using lane swizzle.
The DS_SWIZZLE_B32 instruc>on swizzles input thread data based on an oﬀset mask and returns, without reading or wri>ng DS memory banks.
Lane swizzle only works on 32 lanes, not 64, so we need to do a ﬁnal combine which merges the ﬁrst 32 lanes with the last 32 lanes. This is done with the read lane instruc>on, allowing us to grab the reduced value from another lane.

AMD GCN HTILE
u Manually encode; skip the resummarize on half resolution depth! u HTILE encodes both near and far depth for each 8x8 pixel tile. u Stencil Enabled = 14 bit near value, and a 6 bit delta towards far plane u Stencil Disabled = Min\Max depth encoded in 2x 14 bit UNORM pairs
Rather than paying the cost of a depth read back during a resummarize, we can manually encode HTILE during the downsample opera>on. HTILE encodes both near and far depth for each 8x8 pixel >le. Near is used for trivial accept, whereas far is used for trivial reject; anything in between these planes must do hi-resolu>on tes>ng. If stencil is enabled, we have a 14 bit near value, and a 6 bit delta towards the far plane. If stencil is not enabled, min and max depth is encoded into two 14 bit pairs. The boTom 4 bits in both cases is zMask, which we set to zero for clear. Our Hi-Z pyramid doesn’t need stencil, so this encoding rou>ne is for the non Hi-Stencil format.

Software Z
u One problem with using depth for culling is availability
u Many engines do not have a full Z pre-pass u Restricts asynchronous compute scheduling u Wait for Z buffer laydown 
u You can load the Hi-Z pyramid with software Z!
u In Frostbite since Battlefield 3 [12] u Done on the CPU for the upcoming GPU frame u No latency 
u You can prime HTILE!
u Full Z pre-pass u Minimal cost
One problem with using depth for culling is availability. Many engines only have a par>al z pre-pass, or none at all. This restricts how early you can kick oﬀ asynchronous compute work. You need to wait for Z buﬀer laydown before performing the depth test for culling. Frostbite has had a soOware rasterizer for occluders since BaTleﬁeld 3 [12], which is generated on the CPU for the upcoming GPU frame; the results of which can be used to the load Hi-Z pyramid prior to any related rendering passes, with no latency. In addi>on to loading the Hi-Z pyramid, you can also use your soOware raster to conserva>vely prime your HTILE buﬀer as if you had a full pre-pass! Without a soOware rasterizer or a full Z pre-pass, you can use a trick like re-projec>ng your previous depth buﬀer and tes>ng with that.

This image shows Solas behind a pillar, and the results of the CPU rasterized occlusion buﬀer in the top leO. Using this buﬀer, a Hi-Z pyramid was constructed, and the triangles for Solas are being depth tested against the appropriate mip level in this texture.

This image visualizes the occluder geometry used to produce the soOware occlusion buﬀer for this scene.

Locking the view, and moving to the other side of the pillar, shows the surviving triangles for Solas, and what was rejected by Hi-Z culling.

Batching and Perf
Lastly, I’m going to go over how the batching is structured to make the overlap of culling and rendering eﬃcient.

Batching
u Fixed memory budget of N buffers * 128k triangles u 128k triangles = 384k indices = 768 KB u 3 MB of memory usage, for up to 524288 surviving triangles in flight

128k triangles (768KB) 128k triangles (768KB)

128k triangles (768KB) 128k triangles (768KB) Render
128k triangles (768KB) 128k triangles (768KB) Render

128k triangles (768KB) 128k triangles (768KB)

In order to eﬃciently run all the culling ﬁlters against a scene and render the results, the batching had to be carefully architected. The number of triangles in a scene can wildly vary between game teams or even diﬀerent views, and predictable memory usage is desirable.
We start with a ﬁxed memory budget of N buﬀers * 128k triangles, where N is high enough to get decent overlap between culling and render, and N should be at least 4. Doing a dispatch, wait, draw, loop would be bad, as that would cause the CP to stuTer. We want to go a couple of dispatches ahead of render to account for this eﬃciently.
Assuming 16bit unsigned short, 384k triangle indices is 786Kb of memory.
4 buﬀers is approximately 3MB, which allows for up to half a million triangles in ﬂight. By sizing the buﬀers this way, and with careful scheduling, the data stays resident in the L2 cache when the vertex wavefronts execute.

Batching
Mesh Section (20k tri) Mesh Section (34k tri) Mesh Section (4k tri) Mesh Section (20k tri) Mesh Section (70k tri)
…

Culling (434k triangles)

434k / 512k capacity

Output #0 Output #1 Output #2 Output #3 Render #0 Output #0 Output #1 Output #2 Output #3 Render #1

In this example, we have 4 buﬀers, which gives us a total surviving triangle capacity of 512k. We have to calculate output requirements before culling, in case all triangles survive. I thought of doing a rough heuris>c against 50% backface culled, but certain projec>ons could cause problems.
You can see that culling is processing 434k triangles, which ﬁts well within our 512k limit. Render #0 will occur, and then the next pass can reuse the output buﬀers. This leads into a more complex case…

Batching
Mesh Section (20k tri) Mesh Section (34k tri) Mesh Section (4k tri) Mesh Section (20k tri) Mesh Section (70k tri)
…

Culling (546k triangles)

546k / 512k capacity

Output #0 Output #1 Output #2 Output #3 Render #0,0 Output #0 Render #0,1 Output #0 Output #1 Output #2 Output #3 Render #1

In this example, culling is processing more triangles than we have capacity for. When we determine that we’ve exhausted our buﬀers, we can do a mid-dispatch ﬂush of the rendering. This will free up our output buﬀers for rendering the remaining triangles.
Using triangle lists is nice, because we can trivially cut up a mesh without concern, as long as we maintain ordering for op>mal vertex reuse, or translucent objects.

Batching
u Overlapping culling and render on the graphics pipe is great u But there is a high startup cost for dispatch #0 (no graphics to overlap) u If only there were something we could use….

Startup Cost Dispatch #0

Render #0 Dispatch #1

Render #1 Dispatch #2

Render #2 Dispatch #3

Render #3

Overlapping culling and render wavefronts on the graphics pipe is great, but there is a high startup cost for the ini>al dispatch, when there is no graphics work to overlap. If only there were something we could use…

Batching
u Asynchronous compute to the rescue! 
u We can launch the dispatch work alongside other GPU work in the frame
u Water simulation, physics, cloth, virtual texturing, etc. u This can slow down “Other GPU Stuff” a bit, but overall frame is faster! u Just be careful about what you schedule culling with 
u We use wait on lightweight label operations to ensure that dispatch and render are pipelined correctly

Other GPU Stuff Dispatch #0

Render #0 Dispatch #1

Render #1 Dispatch #2

Render #2 Dispatch #3

Render #3

Asynchronous compute to the rescue! We can launch the dispatch work alongside other GPU work in the frame, such as water simula>on, physics, cloth, virtual texturing, etc. This can slow down some of the graphics pipe work a bit, but overall frame >me is faster. Just be careful about “what” you schedule culling to run with.
We use inexpensive wait on label opera>ons to ensure that dispatch and render are pipelined correctly. On PC we aim for fewer batches at a larger size due to the inability of DirectX12 to issue eﬃcient mid command buﬀer fences.
In general, you want to schedule your async compute to happen at the same >me as low-intensity rendering work, like a depth prepass or shadows. Use fences to bracket the dispatches so they don’t start early or late on the GPU, and make sure to ﬂush the async compute command buﬀer so it doesn’t stall the GPU wai>ng on the auto-kickoﬀ.
AOer that, you can use compute shader limit APIs to restrict the total number of thread groups per CU allowed or disable some CUs from either compute or graphics.
You can also kick oﬀ async compute to do the work during the last stages of post processing on the previous frame.

Performance
443,429 triangles @ 1080p 171 unique PSOs
For the performance ﬁgures, this is the test scene used. There are quite a number of render passes, and with 171 unique PSOs, but we’ll look at a single gbuﬀer pass of 450k triangles, rendered at 1080p on both plahorms. For the Xbox One, the depth buﬀer and a few of the gbuﬀer color targets are in ESRAM, with everything else in DRAM.

Performance

Processed Culled
Rendered

* Scene Dependent

Filter Orientation
Depth* Small* Frustum*

Exclusively Culled

46%

204,006

42%

187,537

30%

128,705

8%

35,182

100% 78% 22%

443,429 348,025 95,404

Inclusively Culled

46%

204,006

20%

90,251

8%

37,606

4%

16,162

Aside from orienta>on culling, the other ﬁlters are very scene dependent. If you have a lot of dense meshes, the small primi>ve ﬁlter can be very eﬀec>ve, especially in the case of dense shadow maps. If you have aggressive view culling on the CPU, or in the coarse cluster culling pass, then the frustum culling may be less useful. However, once you have projected your ver>ces for the other ﬁlters, doing frustum culling is 4 cycles for 4 planes, so it doesn’t hurt to leave it.
nd
Next to orienta>on culling, the depth ﬁlter is the 2 most eﬀec>ve, but is completely dependent on the quality of your depth buﬀer prior to culling. If you have a full z pre-pass, or can load it from soOware occluders or re-projected previous frame depth, then it may do wonders.
You’ll no>ce that for this scene, we’ve managed to cull enough that we are only leO with 22% of the original triangle count. Now imagine feeding the resultant culled index buﬀer into related passes, where we don’t need to worry about the cost of culling.

Performance
Synchronous

No Tessellation Asynchronous

Platform Base Cull Draw Total Cull Draw Total

XB1 (DRAM) 5.47ms 0.24ms 4.54ms 4.78ms 0.26ms 4.56ms 4.56ms

PS4 (GDDR5) 4.56ms 0.13ms 3.76ms 3.89ms 0.15ms 3.80ms 3.80ms

PC (Fury X) 0.79ms 0.06ms 0.47ms 0.53ms 0.06ms 0.47ms 0.47ms

443,429 triangles @ 1080p 171 unique PSOs

No Cluster Culling

Here are the performance ﬁgures for this scene, on both consoles, and an AMD Fury X on PC. Even in DRAM, the XB1 culling is slowest, and barely any >me at all to process half a million triangles in a gbuﬀer pass. Synchronously, we’re saving 15-30% of our rendering cost, and asynchronously we’re saving a bit more. The draw and cull >mes get a liTle bit longer when running asynchronously, but you’ll no>ce that the overall cost goes down. This is due to some resource conten>on between compute and graphics.
A shadow or depth pass would improve performance even further than this, likely but an addi>onal 10-15%, but I wanted to show the eﬀec>veness of per triangle culling even in a color pass with varying PSO changes.

Performance

Tessellation Factor 1-7 (Adaptive Phong)

Synchronous

Asynchronous

Platform Base Cull Draw Total Cull Draw Total

XB1 (DRAM) 19.3ms 0.24ms 11.1ms 11.3ms 0.26ms 11.2ms 11.2ms

PS4 (GDDR5) 12.8ms 0.13ms 8.08ms 8.21ms 0.15ms 8.10ms 8.10ms

PC (Fury X) 3.01ms 0.06ms 0.64ms 0.70ms 0.06ms 0.64ms 0.64ms

443,429 triangles @ 1080p 171 unique PSOs

No Cluster Culling

And here are the performance ﬁgures when we add a complex tessella>on expansion factor to all the triangles. Speciﬁcally, a screen space adap>ve phong tessella>on with a factor no larger than 7. Here you’ll see a massive increase in ini>al rendering cost, due to numerous hardware boTlenecks.
Because of this, our culling cost stays the same, as we are doing culling prior to tessella>on, but the performance improvement to the ﬁnal draw >me is much higher, as the cost of rendering a surviving triangle is much more extreme.
In this scene, synchronously culling saves between 40-80% of the rendering >me, and asynchronously it saves a bit more.

Future Work
u Reuse results between multiple passes
u Once for all shadow cascades u Depth, gbuffer, emissive, forward, reflection u Cube maps – load once, cull each side 
u Xbox One supports switching PSOs with ExecuteIndirect
u Single submitted batch! u Further reduce bottlenecks  u Move more and more CPU rendering logic to GPU  u Improve asynchronous scheduling
It can be argued that tradi>onal triangle processing in compute may not be the most eﬀec>ve use of the silicon, though aside from performance improvements, especially for shadow maps, this system serves as a plahorm for chaining other passes using the ﬁltered index buﬀer for source triangles, instead of the unﬁltered original index buﬀer. Addi>onally, the results from the culling can be resubmiTed into subsequent passes from the same view, giving a performance ampliﬁca>on by skipping culling and reusing results. The Xbox One supports switching PSOs per mul> draw packet with ExecuteIndirect! This means we can submit a single batch, regardless of PSO diﬀerences, and further reduce boTlenecks. I can’t stress how awesome this feature is, and we will deﬁnitely be using it going forward.

Future Work
u Instancing optimizations
u Each instance (re)loads vertex data  u Synchronous dispatch
u Near 100% L2$ hit u ALU bound on render - 24 VGPRs, measured occupancy of 8 u 1.5 bytes bandwidth usage per triangle  u Asynchronous dispatch u Low L2$ residency - other render work between culling and render u VMEM bound on render u 20 bytes bandwidth usage per triangle
For future improvements: Each instanced draw is unrolled into mul>ple draws, since each instanced draw needs its own culled index buﬀer range. Instancing is primarily a CPU win, so the unrolling isn’t an issue for that under DX12, except for the unnecessary memory pressure of each instance reloading the same vertex data. However, this system is geLng incredible L2$ hits for the instanced data, when running synchronously. With un-instanced data, I’ve measured about 20 bytes of bandwidth usage per triangle, but with instancing due to the batch chunk size and near perfect L2$ residency, I’m measuring 1.5 bytes of bandwidth usage per triangle, which is excellent. So nothing needs to be done in the synchronous case, but the asynchronous case can be improved a lot.

Future Work
u Maximize bandwidth and throughput u Load data into LDS chunks, bandwidth amplification u Partition data into per-chunk index buffers u Evaluate all instances 
u More tuning of wavefront limits and CU masking
An improvement to instancing would be to load the vertex data once into chunks of LDS for bandwidth ampliﬁca>on, as each instance would perform culling against LDS loaded data. We also want to inves>gate more at careful tuning of wavefront limits, and also CU masking.

Hardware Tessellation
Another interes>ng use case for compute mesh processing is to op>mize hardware tessella>on GPU boTlenecks. There are a number of cases where hardware tessella>on can be extremely beneﬁcial, especially when you are looking at op>mizing content crea>on, procedural algorithms, or oﬄoading CPU level of detail to the GPU. It isn’t for everyone, as even internally there are some >tles that cannot aﬀord the overhead, but I’m going to brieﬂy men>on some strategies I did to further improve performance when hardware tessella>on is used, such as on Dragon Age: Inquisi>on, and Star Wars: BaTlefront.

Hardware Tessellation

Input Assembler

Vertex (Local) Shader Hull Shader Tessellator

• Tessellation Factors • Silhouette Orientation • Back Face Culling • Frustum Culling • Coarse Culling (Hi-Z)

Domain (Vertex) Shader

Rasterizer

Pixel Shader

Output Merger

When using tessella>on, the goal is to produce vertex waves at peak rate per SE. If not, then you want the reason to be “pixel waves are not draining fast enough”, i.e. the tessella>on itself is not geLng in your way.
In a tradi>onal hardware tessella>on pipeline, the hull shader would do the heavy liOing of calcula>ng adap>ve screen space tessella>on factors, as well as various patch level culling techniques. The calculated factors would range between 0 and whatever your max tessella>on factor is set to.
Let me explain the two main reasons why hull shaders are so bad and why we want to move the work over to compute. Hull shaders tend to have very few ac>ve threads out of the 64 per wave. One issue is because the GPU can only ﬁt so much control point data into LDS.
The other issue is because the shader compiler implements the patch constant func>on by, in the case of 3 vertex triangle patches, turning oﬀ 2 out of the 3 ac>ve threads, and only running code on the remaining thread.
Between these two problems, you are geLng very low parallelism in what tends to be a very complex shader.
In general, the recommenda>on for small tessella>on factors is to load as much data as late as possible so it happens aOer expansion, i.e. in the domain shader.

Hardware Tessellation

Input Assembler

Vertex (Local) Shader

Hull (Pass-through) Shader

Load Final Factors

Tessellator

Domain (Vertex) Shader

Rasterizer

Pixel Shader

Output Merger

Mesh Data
Compute Shader
• Tessellation Factors • Silhouette Orientation • Back Face Culling • Frustum Culling • Coarse Culling (Hi-Z)

A ﬁrst step op>miza>on is to oﬄoad the work that the hull shader is doing, by moving these costly calcula>ons to a compute dispatch earlier in the frame. The results would be stored into a factors buﬀer that the hull shader could then index with SV_Primi>veId.
This op>miza>on makes the hull shader stay ac>ve for the bare minimum amount of >me, which is nice, but s>ll suﬀers from high expansion boTlenecks and other ineﬃciencies. A factor of 0 would tell the hardware to cull the patch, and anything else would do a tessellated draw, including a factor of 1.
When I ﬁrst started trying out tessella>on on GCN, I expected some overhead, but I was shocked to ﬁnd such a disparity between the cost of rendering a regular draw vs. a tessellated draw with a factor of 1. Low tessella>on factors would perform reasonably well, but high tessella>on factors performed very poorly.
Digging into it more, it turns out that vertex reuse is disabled at the vertex shader stage, and is instead enabled at the domain shader stage when the tessella>on factor is greater than 1. This equates to about 3x more ver>ces!
Addi>onally, these factor 1 draws suﬀer from the same parallelism constraints that I just discussed.

Hardware Tessellation
Mesh Data Compute Shader
Patches with factor 0 (culled) are not processed further, and do not get inserted to any work queue.

Structured Work Queue #1 (Patches with factor [1…1]
Tessellation Factors
Structured Work Queue #2 (Patches with factor [2…7]
Tessellation Factors
Structured Work Queue #3 (Patches with factor [8…N]
Tessellation Factors

The improved op>miza>on is to have a compute dispatch that, based on the compute tessella>on factors, buckets the patches into one of three structured work queues. Culled patches with a factor of 0 are not processed further, and do not get inserted to any work queue.

Hardware Tessellation

Structured Work Queue #1 (Patches with factor [1…1]
Tessellation Factors

Non-Tessellated Draw

No Expansion Factor
Avoid Tessellator!

Structured Work Queue #2 (Patches with factor [2…7]
Tessellation Factors

Tessellated Draw

Low Expansion Factor
GCN Friendly ☺

Structured Work Queue #3 (Patches with factor [8…N]
Tessellation Factors

Compute Shader Patch SubD 1 -> 4 Tessellation Factor 1/4

High Expansion Factor
GCN Unfriendly ☹

Patches with a factor of 1 get placed into a queue that will be rendered without tessella>on.
Patches with a factor of 2…7 get placed into a queue to be rendered with tessella>on.
Patches with higher factors get placed into a queue that will undergo coarse reﬁnement prior to tessella>on [14].
The general goal here is to produce small patches, so that we can parallelize more of the mesh across more CUs. All the ver>ces heading into the domain stage need to be processed on the same CU, due to tessella>on patch constants being stored in LDS. So the larger a patch, the less parallelism is achieved.
The compute shader will do a coarse subdivision of the patch into 4 smaller patches, and push them into the tessella>on work queue with ¼ of the original tessella>on factor.
One thing you need to handle is accoun>ng for T-junc>ons between varying patch levels. Using an algorithm like PN-AEN will give you triangle patches which include edge adjacency informa>on. This is helpful for solving this issue.

Summary
u Small and inefficient draws are a problem u Compute and graphics are friends  u Use all the available GPU resources  u Asynchronous compute is extremely powerful  u Lots of cool GCN instructions available  u Check out AMD GPUOpen GeometryFX [20]
In summary, small and ineﬃcient draws are a problem. DirectX 12 gives us an API to submits tons of draws at great performance from the CPU, but the GPU can s>ll choke on these. Compute and rasteriza>on are friends; treat your draws as data, and have both compute and graphics help each other out. Use idle GPU resources to remove ﬁxed func>on boTlenecks. Asynchronous compute is extremely powerful - be sure to schedule compute wavefronts alongside the rest of your frame, but don’t forget that you can overlap compute and graphics work on the same pipe, many developers do not realize this. Remember that there are lots of cool GCN intrinsics available to op>mize with. Grab a coﬀee, sit on a comfortable couch with your laptop, and read through the en>re GCN instruc>on set documenta>on. You’ll ﬁnd all sorts of crazy things you can exploit. Also be on the lookout for AMD GPUOpen. Many of the intrinsics I covered today will be exposed soon on PC for you to u>lize! Lastly, if you are interested in implemen>ng something similar to this tech, be sure to check out GPUOpen GeometryFX; which is much easier than reverse engineering Frostbite to get at our custom solu>on ☺

MAKE RASTERIZATION  GREAT AGAIN!
Lets work together and make rasteriza>on great again!

Acknowledgements

u Matthäus Chajdas (@NIV_Anteru) u Ivan Nevraev (@Nevraev) u Alex Nankervis u Sébastien Lagarde (@SebLagarde) u Andrew Goossen u James Stanard (@JamesStanard) u Martin Fuller (@MartinJIFuller) u David Cook u Tobias “GPU Psychiatrist” Berghoff (@TobiasBerghoff) u Christina Coffin (@ChristinaCoffin) u Alex “I Hate Polygons” Evans (@mmalex) u Rob Krajcarski u Jaymin “SHUFB 4 LIFE” Kessler (@okonomiyonda) u Tomasz Stachowiak (@h3r2tic) u Andrew Lauritzen (@AndrewLauritzen) u Nicolas Thibieroz (@NThibieroz) u Johan Andersson (@repi) u Alex Fry (@TheFryster)

u Jasper Bekkers (@JasperBekkers) u Graham Sellers (@grahamsellers) u Cort Stratton (@postgoodism) u David Simpson u Jason Scanlin u Mike Arnold u Mark Cerny (@cerny) u Pete Lewis u Keith Yerex u Andrew Butcher (@andrewbutcher) u Matt Peters u Sebastian Aaltonen (@SebAaltonen) u Anton Michels u Louis Bavoil (@LouisBavoil) u Yury Uralsky u Sebastien Hillaire (@SebHillaire) u Daniel Collin (@daniel_collin)

I want to thank Chris>na Coﬃn and Mark Cerny for their mentoring of this presenta>on, Johan Andersson for allowing me to work on this research, Ivan Nevraev for ﬁelding numerous driver and compiler requests, and I also want to thank the numerous people that oﬀered guidance, ideas, improvements, support, and friendly conversa>on over a beer. Your help was much appreciated!

References

u

[1] “The AMD GCN Architecture – A Crash Course” – Layla Mah

u

[2] “Clipping Using Homogenous Coordinates” – Jim Blinn, Martin Newell

u

[3] "Triangle Scan Conversion using 2D Homogeneous Coordinates“ - Marc Olano, Trey Greer

u

[4] “GPU-Driven Rendering Pipelines” – Ulrich Haar, Sebastian Aaltonen

u

[5] “Southern Islands Series Instruction Set Architecture” – AMD

u

[6] “Radeon Southern Islands Acceleration” – AMD

u

[7] “Radeon Evergreen / Northern Islands Acceleration” - AMD

u

[8] “GCN Architecture Whitepaper” - AMD

u

[9] “Optimizing Parallel Reduction In CUDA” – Mark Harris

u

[10] “Hierarchical-Z Map Based Occlusion Culling” – Daniel Rákos

u

[11] “Hierarchical Z-Buffer Occlusion Culling” – Nick Darnell

u

[12] “Culling the Battlefield: Data Oriented Design in Practice” – Daniel Collin

u

[13] “The Rendering Pipeline – Challenges & Next Steps” – Johan Andersson

u

[14] “GCN Performance Tweets” – AMD

u

[15] “Learning from Failure: … Abandoned Renderers For Dreams PS4 …” – Alex Evans

u

[16] “Patch Based Occlusion Culling For Hardware Tessellation” - Matthias Nießner, Charles Loop

u

[17] “Tessellation In Call Of Duty: Ghosts” – Wade Brainerd

u

[18] “MiniEngine Framework” – Alex Nankervis, James Stanard

u

[19] “Optimal Bounding Cones of Vectors in Three Dimensions” – Gill Barequet, Gershon Elber

u

[20] “GPUOpen GeometryFX” – AMD

u

[21] “Sample Distribution Shadow Maps” – Andrew Lauritzen

u

[22] “2D Polyhedral Bounds of a Clipped, Perspective-Projected 3D Sphere” – Mara and McGuire

u

[23] “Practical, Dynamic Visibility for Games” - Stephen Hill

Thank You!
Questions?
graham@frostbite.com
Twitter - @gwihlidal

“If you’ve been struggling with a tough ol’ programming problem all day, maybe go for a walk. Talk to a tree. Trust me, it helps.“
- Bob Ross, Game Dev

I’d also like to thank GDC for having me present today, MicrosoO and AMD for allowing me to show a lot of “secret sauce”, and everyone here that aTended my talk. As a reminder, please ﬁll out the electronic evalua>on that was mailed to you. And with that, I’d like to open this up to any ques>ons you may have.

Instancing Optimizations
u Can do a fast bitonic sort of the instancing buffer for optimal front-to-back order u Utilize DS_SWIZZLE_B32
u Swizzles input thread data based on offset mask u Data sharing within 32 consecutive threads
u Only 32 bit, so can efficiently sort 32 elements u You could do clustered sorting
u Sort each cluster’s instances (within a thread) u Sort the 32 clusters

