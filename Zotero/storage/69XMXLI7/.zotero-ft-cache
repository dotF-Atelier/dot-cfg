GPU Zen 2

GPU Zen 2
Advanced Rendering Techniques Edited by Wolfgang Engel
Black Cat Publishing Inc. Encinitas, CA

Editorial, Sales, and Customer Service Oﬃce
Black Cat Publishing Inc. 144 West D Street Suite 204 Encinitas, CA 92009 http://www.black-cat.pub/
Copyright © 2019 by Black Cat Publishing Inc.
All rights reserved. No part of the material protected by this copyright notice may be reproduced or utilized in any form, electronic or mechanical, including photocopying, recording, or by any information storage and retrieval system, without written permission from the copyright owner.
ISBN 13: 978-1-79758-314-3

Printed in the United States of America 12 11 10 09 08

10 9 8 7 6 5 4 3 2 1

Contents

Preface

xi

I Rendering

1

Patrick Cozzi, editor

1 Adaptive GPU Tessellation with Compute Shaders

3

Jad Khoury, Jonathan Dupuy, and Christophe Riccio

1.1 Introduction .................................................................................................. 3 1.2 Implicit Triangle Subdivision ....................................................................... 4 1.3 Adaptive Subdivision on the GPU ................................................................ 9 1.4 Discussion .................................................................................................. 13 1.5 Acknowledgments ...................................................................................... 14 Bibliography....................................................................................................... 15

2 Applying Vectorized Visibility on All Frequency Direct

Illumination

17

Ho Chun Leung, Tze Yui Ho, Zhenni Wang, Chi Sing Leung, and

Eric Wing Ming Wong

2.1 Introduction ................................................................................................ 17 2.2 The Precomputed Radiance Transfer.......................................................... 18 2.3 Rewriting the Radiance Equation ............................................................... 20 2.4 The Vectorized Visibility ........................................................................... 22 2.5 Lighting Evaluation .................................................................................... 23 2.6 Shader Implementation for the Generalized SAT Lookup ......................... 28 2.7 Dynamic Tessellation ................................................................................. 30 2.8 Results ........................................................................................................ 34

v

vi

Contents

2.9 Conclusion ..................................................................................................37 2.10 Acknowledgments.....................................................................................38 Bibliography .......................................................................................................38

3 Nonperiodic Tiling of Noise-based Procedural Textures

41

Aleksandr Kirillov

3.1 Introduction.................................................................................................41 3.2 Wang Tiles ..................................................................................................42 3.3 Nonperiodic Tiling of Procedural Noise Functions.....................................43 3.4 Tiled Noise Filtering ...................................................................................50 3.5 Tiling Improvements ...................................................................................52 3.6 Results.........................................................................................................54 3.7 Performance ................................................................................................54 3.8 Limitations..................................................................................................58 3.9 Conclusion ..................................................................................................58 3.10 Future Work ..............................................................................................59 Bibliography .......................................................................................................60

4 Rendering Surgery Simulation with Vulkan

63

Nicholas Milef, Di Qi, and Suvranu De

4.1 Introduction.................................................................................................63 4.2 Overview.....................................................................................................63 4.3 Render Pass Architecture ............................................................................64 4.4 Handling Deformable Meshes.....................................................................69 4.5 Memory Management System ....................................................................71 4.6 Performance and results ..............................................................................73 4.7 Case Study: CCT.........................................................................................75 4.8 Conclusion and Future Work.......................................................................76 4.9 Source Code................................................................................................77 4.10 Acknowledgments.....................................................................................77 Bibliography .......................................................................................................77

5 Skinned Decals

79

Hawar Doghramachi

5.1 Introduction.................................................................................................79 5.2 Overview.....................................................................................................79 5.3 Implementation ...........................................................................................80

Contents

vii

5.4 Pros and Cons............................................................................................. 86 5.5 Results ........................................................................................................ 87 5.6 Conclusion.................................................................................................. 88 Bibliography....................................................................................................... 88

II Environmental Eﬀects

89

Wolfgang Engel, editor

1 Real-Time Fluid Simulation in Shadow of the Tomb Raider

91

Peter Sikachev, Martin Palko, and Alexandre Chekroun

1.1 Introduction ................................................................................................ 91 1.2 Related Work .............................................................................................. 91 1.3 Simulation .................................................................................................. 92 1.4 Engine Integration .................................................................................... 104 1.5 Optimization............................................................................................. 108 1.6 Future Work.............................................................................................. 110 Acknowledgments ............................................................................................ 110 Bibliography..................................................................................................... 110

2 Real-time Snow Deformation in Horizon Zero Dawn: The

Frozen Wilds

113

Kevin Örtegren

2.1 Introduction .............................................................................................. 113 2.2 Related work............................................................................................. 114 2.3 Implementation ........................................................................................ 114 2.4 Results ...................................................................................................... 120 2.5 Conclusion and Discussion....................................................................... 122 Bibliography..................................................................................................... 123

III Shadows

125

Mauricio Vives, editor

1 Soft Shadow Approximation for Dappled Light Sources

127

Mariano Merchante

1.1 Introduction .............................................................................................. 127 1.2 Detecting Pinholes.................................................................................... 129

viii

Contents

1.3 Shadow Rendering ....................................................................................133 1.4 Temporal Filtering.....................................................................................135 1.5 Results.......................................................................................................137 1.6 Conclusion and Future Work.....................................................................139 Bibliography .....................................................................................................140

2 Parallax-Corrected Cached Shadow Maps

143

Pavlo Turchyn

2.1 Introduction...............................................................................................143 2.2 Parallax Correction Algorithm..................................................................144 2.3 Applications of Parallax Correction..........................................................149 2.4 Results.......................................................................................................150 Bibliography .....................................................................................................152

IV 3D Engine Design

155

Wessam Bahnassi, editor

1 Real-Time Layered Materials Compositing Using Spatial

Clustering Encoding

157

Sergey Makeev

1.1 Introduction...............................................................................................157 1.2 Overview of Current Techniques ..............................................................158 1.3 Introduced Terms ......................................................................................159 1.4 Algorithm Overview .................................................................................159 1.5 Algorithm Implementation........................................................................164 1.6 Results.......................................................................................................172 1.7 Conclusion and Future Work.....................................................................172 Acknowledgments.............................................................................................175 Bibliography .....................................................................................................175

2 Procedural Stochastic Textures by Tiling and Blending

177

Thomas Deliot and Eric Heitz

2.1 Introduction...............................................................................................177 2.2 Tiling and Blending ..................................................................................178 2.3 Precomputing the Histogram Transformations..........................................184 2.4 Improvement: Using a Decorrelated Color Space .....................................188 2.5 Improvement: Preﬁltering the Look-up Table ...........................................190

Contents

ix

2.6 Improvement: Using Compressed Texture Formats.................................... 195 2.7 Results ...................................................................................................... 196 2.8 Conclusion................................................................................................ 197 Acknowledgments ............................................................................................ 197 Bibliography..................................................................................................... 200

3 A Ray Casting Technique for Baked Texture Generation

201

Alain Galvan and Jeﬀ Russell

3.1 Baking in Practice .................................................................................... 202 3.2 GPU Considerations ................................................................................. 211 3.3 Future Work.............................................................................................. 213 Bibliography..................................................................................................... 214

4 Writing an Eﬃcient Vulkan Renderer

215

Arseny Kapoulkine

4.1 Memory Management .............................................................................. 216 4.2 Descriptor Sets ......................................................................................... 219 4.3 Command Buﬀer Recording and Submission........................................... 229 4.4 Pipeline Barriers....................................................................................... 233 4.5 Render Passes ........................................................................................... 238 4.6 Pipeline Objects........................................................................................ 240 4.7 Conclusion................................................................................................ 245 Acknowledgments ............................................................................................ 247

5 glTF—Runtime 3D Asset Delivery

249

Marco Hutter

5.1 The Goals of glTF .................................................................................... 249 5.2 Design Choices......................................................................................... 250 5.3 Feature Summary ..................................................................................... 251 5.4 Ecosystem................................................................................................. 256 5.5 Tools and Workﬂows ................................................................................ 257 5.6 Extensions ................................................................................................ 261 5.7 Application support.................................................................................. 263 5.8 Conclusion................................................................................................ 264

V Real-time Ray Tracing

265

Anton Kaplanyan, editor

x

Contents

1 Real-Time Ray-Traced One-Bounce Caustics

267

Holger Gruen

1.1 Introduction...............................................................................................267 1.2 Previous Work...........................................................................................269 1.3 Algorithm Overview .................................................................................270 1.4 Implementation Details.............................................................................273 1.5 Results.......................................................................................................275 1.6 Future work...............................................................................................275 1.7 Demo ........................................................................................................276 Bibliography .....................................................................................................278

2 Adaptive Anti-Aliasing using Conservative Rasterization and

GPU Ray Tracing

279

Rahul Sathe, Holger Gruen, Adam Marrs, Josef Spjut,

Morgan McGuire, and Yury Uralsky

2.1 Introduction...............................................................................................279 2.2 Overview...................................................................................................280 2.3 Pixel Classiﬁcation using Conservative Rasterization...............................280 2.4 Improved Coverage and Shading Computation .........................................284 2.5 Image Quality and Performance................................................................288 2.6 Future work...............................................................................................291 2.7 Demo ........................................................................................................291 Bibliography .....................................................................................................292

Preface
This book—like its long line of predecessors—is created with the intend to helping readers to better achieve their goals.
For generations, books were used to preserve valuable information. They are an important source of knowledge in our modern world. With the rise of social media, information is obscured and transformed into whatever the agenda of the poster is. It became acceptable to bother other people with information that is sometimes tasteless, mindless and/or nonsensical in all areas of life, including graphics programming. Political parties and companies drive large scale misinformation activities (sometimes called marketing or information warfare) with noise levels that are hard to bear.
This book is meant to provide an oasis of peace and intellectual reﬂection. All of us who worked on it, tried to make sure this collection of articles is practically useful, stimulating for your mind and a joy to read.
The awesome screenshot on the cover is provided by Jeroen Roding with permission by Guerilla Games. Thank you!
I would like to thank Eric Lengyel for editing the articles and creating the beautiful page layout. I would also like to thank Anton Kaplanyan, Mauricio Vives, Patrick Cozzi and Wessam Bahnassi for being the section editors.
I also want to thank at this point everyone for supporting this book series and its predecessors since 2001. These books started friendships, careers, companies and much more over the years. They certainly changed my life in awesome ways!
Love and Peace,
—Wolfgang Engel
xi

I

Rendering

Real-time rendering is an exciting ﬁeld in part because how rapidly it evolves and advances but also because of the graphics community’s eagerness and willingness to share their new ideas, opening the door for others to learn and share in the fun!
In this section we introduce ﬁve new rendering techniques that will be relevant to game developers, hobbyist and anyone else interested in the world of graphics.
The article “Adaptive GPU Tessellation with Compute Shaders” by Jad Khoury, Jonathan Dupuy, and Christophe Riccio suggests to make rasterization more eﬃcient for moderately distant polygons by procedurally reﬁning coarse meshes as they get closer to the camera with the help of compute shaders. They achieve this by manipulating an implicit (triangle-based) subdivision scheme for each polygon of the scene in a dedicated compute shader that reads from and writes to a compact, double-buﬀered array.
The article “Applying Vectorized Visibility on All frequency Direct Illumination” by Ho Chun Leung, Tze Yui Ho, Zhenni Wang, Chi Sing Leung, Eric Wing Ming Wong describes a new PRT approach with visibility functions represented in vector graphics form. This results in a diﬀerent set of strengths and weaknesses compared to other PRT approaches. This new approach can preserve the ﬁdelity of high frequency shadows and accurately account for a huge number of light sources, even with coarsely tessellated 3D models. It can also handle the specular component from mirror to blurry reﬂections.
The article “Non-periodic Tiling of Noise-based Procedural Textures” by Aleksandr Kirillov shows a method to combine noise-based procedural texture synthesis with a non-periodic tiling algorithm. It describes modiﬁcations to several popular procedural noise functions that directly produce texture maps containing the smallest possible complete Wang tile set. It can be used as a preprocessing step or during application runtime.
The article “Rendering Surgery Simulation with Vulkan” by Nicholas Milef, Di Qi, and Suvranu De shows a rendering system design around surgery simulation including how higher-level design decisions propagate to lower-level usage of Vulkan.
The last article in the section “Skinned Decals” by Hawar Doghramachi describes a way on how to dynamically apply decals to a character for example to show the impact position of a projectile. This technique is overcoming the drawback of deferred decals in that scenario, where in case the target area is inﬂuenced by several bones, the decals are “swimming” on top of the target mesh.
—Patrick Cozzi
1

1
I
Adaptive GPU Tessellation with Compute Shaders
Jad Khoury, Jonathan Dupuy, and Christophe Riccio
1.1 Introduction
GPU rasterizers are most eﬃcient when primitives project into more than a few pixels. Below this limit, the Z-buﬀer starts aliasing, and shading rate decreases dramatically [Riccio 2012]; this makes the rendering of geometrically-complex scenes challenging, as any moderately distant polygon will project to subpixel size. In order to minimize such subpixel projections, a simple solution consists in procedurally reﬁning coarse meshes as they get closer to the camera. In this chapter, we are interested in deriving such a procedural reﬁnement technique for arbitrary polygon meshes.
Traditionally, mesh reﬁnement has been computed on the CPU via recursive algorithms such as quadtrees [Duchaineau et al. 1997, Strugar 2009] or subdivision surfaces [Stam 1998, Cashman 2012]. Unfortunately, CPU-based reﬁnement is now fundamentally bottlenecked by the massive CPU-GPU streaming of geometric data it requires for high resolution rendering. In order to avoid these data transfers, extensive work has been dedicated to implement and/or emulate these recursive algorithms directly on the GPU by leveraging tessellation shaders (see, e.g., [Niessner et al. 2012, Cashman 2012, Mistal 2013]). While tessellation shaders provide a ﬂexible, hardwareaccelerated mechanism for mesh reﬁnement, they remain limited in two respects. First, they only allow up to log 2 64  6 levels of subdivision. Second, their performance drops along with subdivision depth [AMD 2013].
In the following sections, we introduce a GPU-based reﬁnement scheme that is free from the limitations incurred by tessellation shaders. Speciﬁcally, our scheme allows arbitrary subdivision levels at constant memory costs. We achieve this by manipulating an implicit (triangle-based) subdivision scheme for each polygon of the scene in a dedicated compute shader that reads from and writes to a compact, double-buﬀered
3

4

1. Adaptive GPU Tessellation with Compute Shaders

array. First, we show how we manage our implicit subdivision scheme in Section 1.2. Then, we provide implementation details for rendering programs we wrote that leverage our subdivision scheme in Section 1.3.

1.2 Implicit Triangle Subdivision

1.2.1 Subdivision Rule

Polygon reﬁnement algorithms build upon a subdivision rule. The subdivision rule describes how an input polygon splits into subpolygons. Here, we rely on a binary triangle subdivision rule, which is illustrated in Figure 1.1(a). The rule splits a triangle into two similar subtriangles 0 and 1, whose barycentric-space transformation matrices are respectively

1 2 1 2 1 2

M 0  1 2 1 2 1 2,

(1.1)

 0

0 1

and

 

12

1 2

1

2

 

M1  1 2 1 2 1 2.

(1.2)

 0

0 1

Listing 1.1 shows the GLSL code we use to procedurally compute either M 0 or M1
based on a binary value. It is clear that at subdivision level N  0, the rule produces 2 N triangles; Figure 1.1(b) shows the reﬁnement produced at subdivision level N  4, which consists of 2 4  16 triangles.

Figure 1.1. The (a) subdivision rule we apply on a triangle (b) uniformly and (c) adaptively. The subdivision levels for the red, blue, and green nodes are respectively 2, 3, and 4.

1.2 Implicit Triangle Subdivision

5

mat3 bitToXform(in uint bit) {
float s = float(bit) - 0.5; vec3 c1 = vec3( s, -0.5, 0); vec3 c2 = vec3(-0.5, -s, 0); vec3 c3 = vec3(+0.5, +0.5, 1);
return mat3(c1, c2, c3); }
Listing 1.1. Computing the subdivision matrix M 0 or M1 from a binary value.

1.2.2 Implicit Representation

By construction, our subdivision rule produces unique subtriangles at each step. Therefore, any subtriangle can be represented implicitly via concatenations of binary words, which we call a key. In this key representation, each binary word corresponds to the partition (either 0 or 1) chosen at a speciﬁc subdivision level; Figure 1.1(b, c) shows the keys associated to each triangle node in the context of (b) uniform and (c) adaptive subdivision. We retrieve the subdivision matrix associated to each key through successive matrix multiplications in a sequence determined by the binary concatenations. For example, letting M 0100 denote the transformation matrix associated to the key 0100, we have

M 0100  M 0  M1  M 0  M 0

(1.3)

In our implementation, we store each key produced by our subdivision rule as a 32-bit unsigned integer. Below is the bit representation of a 32-bit word, encoding the key 0100. Bits irrelevant to the code are denoted by the ‘_’ character.

MSB

LSB

____ ____ ____ ____ ____ ____ ___1 0100

Note that we always prepend the key’s binary sequence with a binary value of 1 so we can track the subdivision level associated to the key easily. Listing 1.2 provides the GLSL code we use to extract the transformation matrix associated to an arbitrary key.
Since we use 32-bit integers, we can store up to a 32 1  31 levels of subdivision, which includes the root node. Naturally, more levels require longer words. Because longer integers are currently unavailable on many GPUs, we emulate them using integer vectors, where each component represents a 32-bit wide portion of the entire key. For more details, see our implementation, where we provide a 63-level subdivision algorithm using the GLSL uvec2 datatype.

6

1. Adaptive GPU Tessellation with Compute Shaders

mat3 keyToXform(in uint key) {
mat3 xf = mat3(1);
while (key > 1u) {
xf = bitToXform(key & 1u) * xf; key = key >> 1u; }
return xf; }
Listing 1.2. Key to transformation matrix decoding routine.

1.2.3 Iterative Construction
Subdivision is recursive by nature. Since GPU execution units lack stacks, implementing GPU recursion is diﬃcult. In order to circumvent this diﬃculty, we store the triangles produced by our subdivision as keys inside a buﬀer that we update iteratively in a ping-pong fashion; we refer to this double-buﬀer as the subdivision buﬀer. Because our keys consists of integers, our subdivision buﬀer is very compact. At each iteration, we process the keys independently in a compute shader, which is set to write in the second buﬀer. We allow three possible outcomes for each key: it can be subdivided to the next level, downgraded to the previous subdivision level, or conserved as is. Such operations are very straightforward to implement thanks to our key representation. The following bit representations match the parent of the key given in our previous example along with its two children:

MSB

LSB

parent: ____ ____ ____ ____ ____ ____ ____ 1010

key: ____ ____ ____ ____ ____ ____ ___1 0100

child1: ____ ____ ____ ____ ____ ____ __10 1000

child2: ____ ____ ____ ____ ____ ____ __10 1001

Note that compared to the key representation, the other keys are either 1-bit expansions or contractions. The GLSL code to compute these representations is shown in Listing 1.3; it simply consists of bit shifts and logical operations, and is thus very cheap.
Listing 1.4 provides the pseudocode we typically use for updating the subdivision buﬀer in a GLSL compute shader. In practice, if a key needs to be split, it emits two new words, and the original key is deleted. Conversely, when two sibling keys must merge, they are replaced by their parent's key. In order to avoid generating two copies of the same key in memory, we only emit the key once from the 0-child, identiﬁed

1.2 Implicit Triangle Subdivision

7

using the test provided in Listing 1.5. We also provide some unit tests we perform on the keys to avoid producing invalid keys in Listing 1.6. For the keys that do not require any modiﬁcation, they are simply re-emitted, unchanged.

uint parentKey(in uint key) {
return (key >> 1u); }
void childrenKeys(in uint key, out uint children[2]) {
children[0] = (key << 1u) | 0u; children[1] = (key << 1u) | 1u; }
Listing 1.3. Implicit subdivision procedures in GLSL.

buffer keyBufferOut { uvec2 u_SubdBufferOut[]; }; uniform atomic_uint u_SubdBufferCounter;
// write a key to the subdivision buffer void writeKey(uint key) {
uint idx = atomicCounterIncrement(u_SubdBufferCounter); u_SubdBufferOut[idx] = key; }
// general routine to update the subdivision buffer void updateSubdBuffer(uint key, int targetLod) {
// extract subdivision level associated to the key int keyLod = findMSB(key);
// update the key accordingly if (/* subdivide ? */ keyLod < targetLod && !isLeafKey(key)) {
uint children[2]; childrenKeys(key, children);
writeKey(children[0]); writeKey(children[1]); } else if (/* keep ? */ keyLod == targetLod) { writeKey(key);

8

1. Adaptive GPU Tessellation with Compute Shaders

} else /* merge ? */ {
if (/* is root ? */ isRootKey(key)) {
writeKey(key); } else if (/* is zero child ? */ isChildZeroKey(key)) {
writeKey(parentKey(key)); } } }
Listing 1.4. Updating the subdivision buﬀer on the GPU.

bool isChildZeroKey(in uint key) { return (key & 1u == 0u); } Listing 1.5. Determining if the key represents the 0-child of its parent.

bool isRootKey(in uint key) { return (key == 1u); } bool isLeafKey(in uint key) { return findMSB(key) == 31; }
Listing 1.6. Determining whether a key is a root key or a leaf key.
It should be clear that our approach maps very well to the GPU. This allows us to compute adaptive subdivisions such as the one shown in Figure 1.1(c). Note that an iteration only permits a single reﬁnement or coarsening operation per key. Thus when more are needed, multiple buﬀer iterations should be performed. In our rendering implementations, we perform a single buﬀer iteration at the beginning of each frame.
1.2.4 Conversion to Explicit Geometry
For the sake of completeness, we provide here some additional details on how we convert our implicit subdivision keys into actual geometry. We achieve this easily with GPU instancing. Speciﬁcally, we instantiate a triangle for each subdivision key located in our subdivision buﬀer. For each instance, we determine the location of the triangle vertices using the routines of Listing 1.7. Note that these routines focus on computing the coordinates of the vertices of the subdivided triangles; extending them to handle other attributes such as normals or texture coordinates is straightforward.

1.3 Adaptive Subdivision on the GPU

9

// barycentric interpolation vec3 berp(in vec3 v[3], in vec2 u) {
return v[0] + u.x * (v[1] - v[0]) + u.y * (v[2] - v[0]); }
// subdivision routine (vertex position only) void subd(in uint key, in vec3 v_in[3], out vec3 v_out[3]) {
mat3 xf = keyToXform(key); vec2 u1 = (xf * vec3(0, 0, 1)).xy; vec2 u2 = (xf * vec3(1, 0, 1)).xy; vec2 u3 = (xf * vec3(0, 1, 1)).xy;
v_out[0] = berp(v_in, u1); v_out[1] = berp(v_in, u2); v_out[2] = berp(v_in, u3); }
Listing 1.7. Compute the vertices v_out of the subtriangle associated to a subdivision key generated from a triangle deﬁned by vertices v_in.

1.3 Adaptive Subdivision on the GPU
1.3.1 Overview
In this section, we describe a tessellation technique for polygonal geometry that leverages our implicit subdivision scheme. Our technique computes an adaptive subdivision for each polygon in the scene, so as to control their extent in screen-space and hence minimize subpixel projections; we describe how we compute such subdivisions using a distance-based LOD criterion in Section 1.3.2. Since adaptive subdivisions usually lead to T-junction polygons, we also discuss how we avoid them entirely; we discuss the issue of T-junctions in Section 1.3.3.
In practice, our technique requires three GPU kernels with OpenGL 4.5; Figure 1.2 diagrams the OpenGL pipeline of our implementation. The ﬁrst kernel (LodKernel) updates the subdivision buﬀer in a compute shader using the algorithms described in the previous section. In addition, we perform view-frustum culling for each key and write the visible ones to a buﬀer (CulledSubdBuﬀer) using an atomic counter. Next, we launch a second compute kernel (IndirectBatcherKernel) that prepares an indirect compute dispatch call for the next subdivision update (i.e., the next invocation of LodKernel), as well as an indirect draw call for the third and ﬁnal kernel. The ﬁnal kernel (RenderKernel) executes the indirect drawing commands to render the

10

1. Adaptive GPU Tessellation with Compute Shaders

Figure 1.2. OpenGL pipeline of our compute-based tessellation shader. The green, red, and gray boxes respectively denote GPU memory buﬀers, GPU code execution, and CPU code execution.

ﬁnal geometry to the framebuﬀer (FrameBuﬀer). It instances a grid of triangles (InstancedGeometryBuﬀers) for each key located in the frustum-culled subdivision buﬀer (CulledSubdBuﬀer).

1.3.2 LOD Function

In order to guarantee that the transformed vertices produce rasterizer-friendly polygons, we rely on a distance-based criterion to determine how to update the subdivision buﬀer. Indeed, under perspective projection, the image plane size s at distance z from the camera scales according to the relation

s



z





2z

tan



θ 2

,

where θ  0, π  is the horizontal ﬁeld of view. Based on this observation, we derive
the following routine to determine the ideal subdivision level k that each key should target:

float distanceToLod(float z) {
float tmp = s(z) * targetPixelSize / screenResolution; return -log2(clamp(tmp, 0.0, 1.0)); }

Here, the parameter z denotes the distance from the camera to the subtriangle associated to the key being processed. Listing 1.8 provides the GLSL pseudocode we execute in LodKernel.

1.3 Adaptive Subdivision on the GPU

11

buffer VertexBuffer { vec3 u_VertexBuffer[]; }; buffer IndexBuffer { uint u_IndexBuffer[]; }; buffer SubdBufferIn { uvec2 u_SubdBufferIn[]; };
void main() {
// get threadID (each key is associated to a thread) int threadID = gl_GlobalInvocationID.x;
// get coarse triangle associated to the key uint primID = u_SubdBufferIn[threadID].y; vec3 v_in[3] = vec3[3](
u_VertexBuffer[u_IndexBuffer[primID * 3 ]], u_VertexBuffer[u_IndexBuffer[primID * 3 + 1]], u_VertexBuffer[u_IndexBuffer[primID * 3 + 2]], );
// compute distance-based LOD uint key = u_SubdBufferIn[threadID].x; vec3 v[3]; subd(key, v_in, v); float z = distance((v[1] + v[2]) / 2.0, camPos); int targetLod = int(distanceToLod(z));
// write to u_SubdBufferOut updateSubdBuffer(key, targetLod); }
Listing 1.8. Adaptive subdivision using a distance-based criterion.

1.3.3 T-Junction Removal
As for any other adaptive polygon-reﬁnement scheme, our technique can produce T-junction triangles whenever two neighboring keys diﬀer in subdivision level. For instance, Figure 1.1(c) shows a T-junction between the neighboring triangles associated to the keys 00, 0101 and 0100. T-junctions are problematic for rendering because they lead to visible cracks whenever the vertices are displaced by a smoothing function or a displacement map. Fortunately, our subdivision scheme has the property that it does not produce T-junctions as long as two neighboring keys diﬀer by no more than one subdivision level; this is noticeable for the green and blue keys of Figure 1.1(c). In order to guarantee such key conﬁgurations, we apply our distance-based criteria to the centroid of the hypotenuse of each subtriangle; see Listing 1.8. We observed that this approach guarantees crack-free renderings for any target edge length lower than 16 pixels (we noticed some T-junctions above this value when the instanced grid is highly

12

1. Adaptive GPU Tessellation with Compute Shaders

tessellated). Therefore, we chose to rely on such a system as it avoids the need for a sophisticated T-junction removal system; Listing 1.9 shows the code we use in the vertex shader of our RenderKernel.

buffer VertexBuffer { vec3 u_VertexBuffer[]; }; buffer IndexBuffer { uint u_IndexBuffer[]; }; in vec2 i_InstancedVertex; in uvec2 i_PerInstanceKey;
void main() { // get coarse triangle associated to the key uint primID = i_PerInstanceKey.y; vec3 v_in[3] = vec3[3]( u_VertexBuffer[u_IndexBuffer[primID * 3 ]], u_VertexBuffer[u_IndexBuffer[primID * 3 + 1]], u_VertexBuffer[u_IndexBuffer[primID * 3 + 2]], );
// compute vertex location uint key = i_PerInstanceKey.x; vec3 v[3]; subd(key, v_in, v); vec3 finalVertex = berp(v, i_InstancedVertex);
// displace, deform, project, etc. }
Listing 1.9. Adaptive subdivision using a distance-based criterion.

1.3.4 Results
To demonstrate the eﬀectiveness of our method, we wrote a renderer for displacementmapped terrains, and another one for meshes; our source code is available on github at https://github.com/jadkhoury/TessellationDemo, and a terrain rendering result is shown in Figure 1.3. In Table 1.1, we give the CPU and GPU timings of a zoomin/zoom-out sequence in the terrain at 1080p. The camera's orientation was ﬁxed, looking downwards, so that the terrain would occupy the whole framebuﬀer, thus maintaining constant rasterization activity. We conﬁgured the renderer to target an average triangle edge length of 10 pixels; Figure 1.3 shows the wireframe of such a target. The testing platform is an Intel i7-8700k CPU, running at 3.70 GHz, and an Nvidia GTX 1080 GPU with 8 GiB of memory. Note that the CPU activity only consists of OpenGL uniform variables and driver management. On current implementations, such tasks run asynchronously to the GPU.

1.4 Discussion

13

Figure 1.3. Crack-free, multiresolution terrain rendered entirely on the GPU using computebased subdivision and displacement mapping. The alternating colors show the diﬀerent subdivision levels.

Kernel LOD Batch Render

CPU (ms) 0.038 0.028 0.035

GPU (ms) 0.042 0.003 0.184

CPU stdev 0.160 0.011 0.018

GPU stdev 0.031 0.001 0.013

Table 1.1. CPU and GPU timings and their respective standard deviation over a zoom-in sequence of 5000 frames.

As demonstrated by the reported numbers, the performance of our implementation is both fast and stable. Naturally, the average GPU rendering time depends on how the terrain is shaded. In our experiment, we use a constant color so that the reported performances correspond exactly to the overhead caused by vertex processing of our subdivision technique.
1.4 Discussion
We introduced a novel compute-based subdivision algorithm that runs entirely on the GPU thanks to an implicit representation. In future work, we would like to explore the

14

1. Adaptive GPU Tessellation with Compute Shaders

feasibility of this representation for more complex subdivision schemes such as Catmull-Clark. In the meantime, we provide next a few additional considerations that we think can be relevant in the context of our work.
How much memory should be allocated for the buﬀers containing the subdivision keys? This depends on the target polygon density in screen space. The buﬀers should be able to store at least 3max_level 1 nodes, and do not need to exceed a capacity of 4 max_level nodes. The lower bound corresponds to a perfectly restricted subdivision, where each neighboring triangle diﬀer by one level of subdivision at most. The higher bound gives the number of cells at the ﬁnest level in case of uniform subdivision.
Is our subdivision technique prone to Floating-point precision issues? There are no issues regarding the implicit subdivision itself, as each key is represented with bit sequences only. However, problems may occur when computing the transformation matrices in Listing 1.1. Our 31-level subdivision implementation does not have this issue, but higher levels will, eventually. A simple solution to delay the problem on OpenGL 4+ hardware is to use double precision, which should provide suﬃcient comfort for most applications.
How about combining this technique with tessellation shaders to overcome the subdivision limits of the hardware? We have actually implemented such an approach. Our open-source implementation is available on github at https://github.com/ jdupuy/opengl-framework (see the demo-isubd-terrain demo). With both approached at hand, we leave it up to the developer to decide which approach is best given his software and hardware constraints.
There are two ways to control polygon density. Either use the implicit subdivision, or reﬁne the instanced triangle grid. Which approach is best? This will naturally depend on the platform. Our code provides tools to modify the tessellation of the instanced triangle grid, so that its impact can be thoroughly measured; Figure 1.4 plots the performance evolution that we measured on our platform.
Can our implicit subdivision scheme smooth input meshes? Our implicit subdivision scheme oﬀers the same functionality as tessellation shaders. Therefore, any smoothing technique that runs with tessellation shaders run with our subdivision shaders. For instance, the mesh renderer we provide implements PN-triangles [Vlachos et al. 2001] and Phong Tessellation [Boubekeur and Alexa 2008] to smooth the surface of the coarse meshes we reﬁne; Figure 1.5 shows our mesh renderer applying either bilinear interpolation or Phong Tessellation to a coarse triangle mesh.
1.5 Acknowledgments
This chapter is the result of Jad Khoury’s master thesis, which was supervised by Jonathan Dupuy. All authors conducted this work at Unity Technologies.

Bibliography

15

Figure 1.4. Performance evolution with respect to the level of subdivision of the instanced triangle grid on an NVidia GTX 1080.
Figure 1.5. Our subdivision technique applied on (a) a triangle mesh using (b) bilinear interpolation and (c) Phong tessellation [Boubekeur and Alexa 2008].
Bibliography
AMD 2013. GCN Performance Tweets. List of all GCN performance tweets that were released during the ﬁrst few months of 2013. URL: http://developer.amd.com/wordpress/media/2013/ 05/GCNPerformanceTweets.pdf). BOUBEKEUR, T., AND ALEXA, M. 2008. Phong Tessellation. ACM Transactions on Graphics (Proc. SIGGRAPH Asia 2008) 27:5. CASHMAN, T. 2012. Beyond Catmull Clark? A Survey of Advances in Subdivision Surface Methods. Comput. Graph. Forum 31:1, 42–61. URL: https://doi.org/10.1111/j.1467-8659. 2011.02083.x).

16

1. Adaptive GPU Tessellation with Compute Shaders

DUCHAINEAU, M., WOLINSKY, M., SIGETI, D., MILLER, M., ALDRICH, C., AND MINEEV-WEINSTEIN, M. 1997. ROAMing terrain: real-time optimally adapting meshes. In Proceedings of the 8th Conference on Visualization ‘97, pp. 81–88. IEEE Computer Society Press.
MISTAL, B. 2013. Gpu terrain subdivision and tessellation. In GPU Pro 4, 3–20.
NIESSNER, M., LOOP, C., MEYER, M., AND DEROSE, T. 2012. Feature-adaptive GPU Rendering of Catmull-Clark Subdivision Surfaces. ACM Trans. Graph. 31:1, 6:1–6:11.
RICCIO, C. 2012. Southern Islands in deep dive. SIGGRAPH Tech Talk. URL: https://www. g-truc.net/doc/Siggraph2012%20Tech%20talk.pptx.
Stam, J. 1998. Exact Evaluation of Catmull-Clark Subdivision Surfaces at Arbitrary Parameter Values. In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ‘98, pp. 395–404. URL: http://doi.acm.org/10.1145/280814.280945.
STRUGAR, F. 2009. Continuous distance-dependent level of detail for rendering heightmaps. Journal of graphics, GPU, and game tools 14:4, 57–74.
VLACHOS, A., PETERS, J., BOYD, C., AND MITCHELL, J. 2001. Curved PN Triangles. In Proceedings of the 2001 Symposium on Interactive 3D Graphics, I3D ‘01, pp. 159–166. URL: http://doi.acm.org/10.1145/364338.364387.

2
I
Applying Vectorized Visibility on All Frequency Direct Illumination
Ho Chun Leung, Tze Yui Ho, Zhenni Wang, Chi Sing Leung, and Eric Wing Ming Wong
2.1 Introduction
The Precomputed Radiance Transfer (PRT) [Sloan et al. 2002] is a general framework for illuminating surfaces using some precomputed transfer functions. It plays an important role in the more elaborated rendering applications (e.g., [Elcott 2016]) nowadays. The preparation for PRT usually involves a precomputation step, which is a computationally expensive step for evaluating the light bounce and transfer in a scene; the output is a set of transfer functions, and these transfer functions are usually prepared in a per vertex manner. Such transfer functions can be prepared by using computational simulations (e.g., ray tracing) or measuring from the real world with some speciﬁcally designed equipment [Matusik et al. 2003]. Either simulated or measured, the fundamental objective of PRT is still the same, i.e., providing instant access to the transfer functions for the rendering applications.
The formulation of PRT is so general that it can practically capture all possible visual eﬀects due to illuminations, including shadows, interreﬂections, lighting functions, etc. While PRT can render many visual features, the real challenges are to make it also eﬃcient; and there are two of them. First, the PRT approach needs an eﬃcient means to evaluate the radiance equation (i.e., a numerical integral) with potentially several hundred thousand samples. The evaluation has to be eﬃcient because we might need to do the evaluation for a few million times per frame (e.g., an evaluation for each onscreen pixel; screen resolution 1280962). Second, they need an algorithm to compress the transfer functions to a manageable data size. When local eﬀects (e.g., view dependent lighting, shadows and interreﬂections) are included, the data size of the
17

18

2. Applying Vectorized Visibility on All Frequency Direct Illumination

precomputed transfer functions can be in gigabytes order. Even if the memory consumption and the bandwidth of the distribution medium are not a concern, it will not be worth the eﬀort to spend all that memory for illumination.
Thankfully, the human visual system is very forgiving to illumination being a bit diﬀerent to reality. Even if we simplify the lighting evaluation to an extreme measure, the rendered results can still be visually pleasing. For instance, if we restrict the view positions to a single point at indeﬁnite far and ignore the local eﬀects, PRT will reduce to MatCap [Brauer 2010, Moreno 2018]; if we ignore just the local eﬀects, PRT will reduce to the environment mapping with mipmapping for the glossy reﬂections [Scheuermann and Isidoro 2006]; if we restrict the view positions to a single point and ﬁx the 3D model postures (i.e., essentially a static image), PRT will reduce to Image Based Lighting [Russell 2015]. Other than the simpliﬁed approaches, there exist many PRT approaches [Ng et al. 2003, Ng et al. 2004, Sloan et al. 2003, Tsai and Shih 2006, Kautz et al. 2002, Liu et al. 2004, Ben-Artzi et al. 2006, Lam et al. 2010, Wang et al. 2009, Wang et al. 2013], which attack the problem head-on by modeling the transfer functions as a whole instead of dropping the eﬀects whenever they are inconvenient.
This article presents the implementation and the rendering algorithms for the vectorized visibility [Ho et al. 2018], i.e., a variant of the PRT approaches. It diﬀerentiates itself from the other PRT approaches by having its visibility functions represented in vector graphics forms. This fundamental diﬀerence is so profound that the corresponding rendering algorithm evolves through a very diﬀerent path as compared to other PRT approaches; this results in a diﬀerent set of strengths and weaknesses.
Our PRT approach can preserve the ﬁdelity of high frequency shadows and accurately account for huge number of light sources even with coarsely tessellated 3D models; and it can also handle the specular component from mirror to blurry reﬂection1. Both per-vertex and per-fragment direct illumination are supported. It can also make use of dynamic tessellation to provide a better scalability, which is faster than the perfragment with better quality than the per-vertex. The specular component in the real world does have shadows2; and our algorithm can capture the visual impression of the specular component shadows3.

2.2 The Precomputed Radiance Transfer

Before we can appreciate the diﬀerences, we need a deeper understanding of the PRT approaches. We begin with the radiance equation. The radiance equation is given by

 I ω 0, n,  

fr ω 0 , n, ω  ω dω,

(2.1)

Ω

1 The video demonstration of our BRDF editing feature https://youtu.be/tvNnQfL5UT4. 2 The video of the specular component shadows in the real world https://youtu.be/EezUtywshyI. 3 The video of the specular component shadows from our demo program https://youtu.be/
dlLLZTyY7xs.

2.2 The Precomputed Radiance Transfer

19

where ω 0 is the viewing direction, ω is the incident direction, n is the surface normal, fr is the Bidirectional Reﬂectance Distribution Function (BRDF) (or, the transfer function),  is the environment map (or the illuminations), and Ω is the spherical domain. (Note that, we will refer to fr as the transfer function from this point onward.) Conceptually, it is just describing that we can have the appearance of a surface point by adding
up the inﬂuence of all involving light sources. In simple terms, it is just a complicated
way to describe the multiple light source application, and the integration sign simply
means having as many light sources as possible.
An obvious intention of writing the radiance equation in its integral form is to
apply frequency analysis. That is transforming the transfer function fr and the illuminations  to their frequency representations, and performing the integration in the frequency domain. In the frequency domain, the convolution of fr and  becomes a dot product of their frequency coeﬃcient vectors C fr and C , i.e.,

I ω 0 , n,   C fr  C .

(2.2)

Figure 2.1 shows an intuitive illustrative ﬁgure to demonstrate the radiance equation evaluation using frequency representations. Using the lower frequency components only, for instances 16 coeﬃcients per representation, a numerical integral potentially requiring hundred thousand samples can then be approximated by a simple dot product of two 16-component vectors; and it makes the radiance evaluation eﬃcient enough for real time applications as presented by Sloan et al. [2002].
Sloan et al. [2002] use spherical harmonics as his basis for the frequency representation, which is, in vague terms, the Fourier series for the spherical domain. Like Fourier series, spherical harmonics is a global basis, which is ineﬃcient for handling high frequency signals. As a result, Sloan et al. [2002] can only render blurry eﬀects, e.g., blurry shadows and the diﬀuse lighting function.
Ng et al. [2003] extends Sloan’s work by replacing the global basis with a local basis4 (i.e., Haar wavelets in the cubemap space). To make things more focused, the eﬀects involved are simpliﬁed from global illumination (i.e., view independent lighting, visibility, and interreﬂections) to direct illumination (i.e., view independent lighting and visibility). In this case, the radiance equation becomes

 I ω 0, n,  

S ω f ω 0 , n, ω  ωω n  dω,

(2.3)

Ω

where  is the environment map, S is the visibility function separated from fr, f is fr without the visibility info, and ω n is the cosine of the incident angle. Using the

4 A basis is a prototype of functions. A global basis means each of its functions is non-zero in general throughout the domain. On the contrary, a local basis (also referred as compactly supported basis) means each of its functions is non-zero only within the neighborhood of a particular point.

20

2. Applying Vectorized Visibility on All Frequency Direct Illumination

Figure 2.1. An intuitive illustrative ﬁgure to demonstrate the radiance equation evaluation using frequency representations. The eﬃciency of this approach come from aggressively dropping less important components until the number of components is manageable.

advantage of the local basis (i.e., more eﬃcient in handling high frequency signals), Ng et al. [2003] attains the all-frequency shadow rendering quality.

2.3 Rewriting the Radiance Equation

While the local basis does have a superior performance w.r.t. that of the global basis, they use the same strategy to make the radiance equation evaluation eﬃcient, i.e., aggressively dropping less important components until the number of components is manageable. For instance, suppose the illuminations are come from a 6256256 cubemap, it will give us 400k number of light sources. Transforming it to spherical wavelet representations [Ng et al. 2003], we have 400k components; even if an impractical amount of important components (say 1k components) is being used, we will be discarding 99.75% of the information.
Having a signiﬁcant amount of information loss is bound to cause a signiﬁcant damage to the radiance equation evaluation, and therefore the rendered results. In this case, the subtle shadow translation in the darker regions will be discarded. A discussion about this issue could be found in [Ng et al. 2004]. These translations, though subtle, do have a signiﬁcant impact to the visual impression; and the image quality loss due to the aggressive strategy will be immediately identiﬁable if the rendered results are compared to the ground truth.
If the eﬃciency is the only concern, aggressively dropping less important components in frequency domain is not the only option. We do have a numerical tool that can provide accurate numerical integrals regardless of the number of samples, i.e., the summed area table (SAT) [Crow 1984]. To apply SATs to the radiance equation evaluation, the radiance equation needs to be adjusted accordingly. We begin the adjustment from Equation (2.3), i.e., the direction illumination with visibility equation. Removing the cosine term ω n , we have

 I ω 0, n,  

S ω f ω 0, n, ω  ω dω.

(2.4)

Ω

2.3 Rewriting the Radiance Equation

21

Replacing the integration domain Ω with the visibility function S ω, we have

 I ω 0, n,  

f ω 0 , n, ω  ω dω.

(2.5)

S

Green’s theorem, one of the major theorems in multivariable calculus, gives a relationship between the line integral of a two-dimensional vector ﬁeld over a closed path (i.e., a contour integral) and the double integral over the region it encloses. Coincidentally, the visibility function S happens to be some regions enclosed by the contour of its visible regions. This unusual coincidence allows us to apply Green’s theorem to Equation (2.5).
Applying Green’s theorem, we can rewrite the radiance equation from a double integral to a contour integral, i.e.,

 I ω 0 , n,   SAT  f ω 0 , n, ω  ωp  dp ,

(2.6)

A

where A is the contour of the visibility function S; SAT g denotes the pre-integrated
spherical function of a given spherical function g, i.e.,

pθ

pφ

  SAT gp 

g φ, θ sin θ dφ dθ,

(2.7)

0

0

where p is a unit vector,  pφ , pθ  is the spherical coordinates of p, and φ, θ is the
parameterization of Ω in spherical coordinates. Figure 2.2 shows an intuitive illustrative ﬁgure to demonstrate the radiance equation evaluation using the vectorized visibility representation.

Figure 2.2. An intuitive illustrative ﬁgure to demonstrate the radiance equation evaluation using the vectorized visibility representation. Using the visibility function as the domain and applying Green’s theorem, we can convert the radiance equation to a contour integral. The vectorized visibility happens to be the ideal candidate for the domain of the contour integral.

22

2. Applying Vectorized Visibility on All Frequency Direct Illumination

As a contour integral, Equation (2.6) only needs to process a one dimensional data, i.e., the contour of the visibility function. Having the contour represented in vector graphics forms, we decouple the visibility function from the illumination resolution. Hence, even with 400k light sources, Equation (2.6) can still provide an accurate integral without dropping any information at the same computational cost. This makes Equation (2.6) both eﬃcient and accurate for the radiance equation evaluation.
The relative pros and cons between doing it from the frequency analysis perspective vs. Equation (2.6) might be not immediate obvious. Metaphorically, their relative pros and cons are analogous to that of Finite Element Method (FEM) vs. Finite Boundary Method (FBM) respectively. The downside of FBM is that it takes more eﬀorts to formulate the simulation (in a lot of cases, such formulations may not even be feasible at all), whilst FEM works in general. However, if FBM is applicable, it can provide several orders higher accuracy at a tiny fraction of computational cost. Two applications of Equation (2.6), i.e., diﬀuse lighting and specular lighting, will be discussed in Section 2.5.

2.4 The Vectorized Visibility
Our representation, i.e., the vectorized visibility [Ho et al. 2015], is designed to model all visibility functions in a small neighborhood, instead of just for a single point. It
represents them by using a sequence of position vectors a i  (see Figure 3(b)). This
representation has an intuitive geometrical meaning, i.e., some closed paths on the 3D
model (see Figure 2.3(a)). Given a vectorized visibility a i , we can synthesize the
visibility contour at any point in the 3D space with a simple operation (see Figure 2.3(d)), i.e., translation then normalization. To be precise, the synthesized visibility
contour u i  at an arbitrary point p is given by u i   normalize a i  p .
The process to obtain the vectorized visibility at a given sample origin o is rendering the 3D model to a visibility cubemap centered at o (see Figure 2.4(a)), sweeping along the occluded region boundary, and recording the corresponding position vectors to some sequences (see Figure 2.4(b)). For a more eﬃcient manipulation, all the individual sequences are concatenated together with some zero area closed paths to form a single sequence (see Figure 2.4(c)). The resulting sequence is the vectorized visibility
a i . Two paths to and from the same pair of endpoints will cancel out each other
during the integration. Therefore, we can add an arbitrary number of them without altering the integration.
We prepare one vectorized visibility for each vertex, with each vertex as its sample origin. The resulting vectorized visibility of all vertices will then be stored in a 3D texture of resolution Wv H v D, where Wv H v  No. of vertices, and D is the number of position vectors (see Figure 2.4(d)). The process is simple and deterministic, which can be implemented easily with a bit of C++ and GLSL, and a generation program along with the source code is readily available in the demo package.

2.5 Lighting Evaluation

23

(a) the sample origin (b) the vectorized visibility (c) the synthesis locations
(d) the synthesized visibility contours Figure 2.3. More about the nature of the vectorized visibility. (a) The vectorized visibility sampled at the yellow dot plotted on the 3D model. (b) The vectorized visibility in Figure (a). (c) Indicating the locations for the visibility contour synthesis in Figure (d). (d) The visibility contours synthesized at the locations identiﬁed in Figure (c) using the vectorized visibility shown in Figure (b). A single vectorized visibility representation can already synthesize a family of visibility contours.

(a)

(b)

(c)

(d)

Figure 2.4. The data structure for the vectorized visibility. (a) The visibility cubemap. (b) The sequences of position vectors obtained from the boundaries in Figure (a). (c) Multiple sequences can be connected to form a single sequence. (d) The position vector sequences of all vertices would be stored in a 3D texture.

2.5 Lighting Evaluation

For the diﬀuse component, the radiance equation for direct illumination is given by

 Id 

S ω max n ω, 0  ω dω.

(2.8)

Ω

24

2. Applying Vectorized Visibility on All Frequency Direct Illumination

Applying Green’s theorem as described in Equation (2.6), we have

 I d  n  SAT ω ωp  dp.

(2.9)

A

We use the synthesized visibility contour u i , which is the unit vector sequence pre-
sented in Section 2.4, as the representation of A. Each consecutive pairs of unit vectors deﬁnes a line segment, and the contour integral is evaluated by summing up the partial results of all line segments. Then, the radiance becomes

 I d  n  vsat SAT ω ω , u i , u i1 , i

(2.10)

where vsat is the generalized SAT lookup operator for line segments to be presented in Section 2.6.
For the specular component, we use the Phong lighting function, and its radiance equation is given by

 I s 

S ω Phong r, ω, s  ω dω,

Ω

(2.11)

where Phong r, ω, s  max r ω, 0s, r is the reﬂected view direction, and s is the
glossiness. However, due to the s power of r ω, it is not practical to directly factorize Equation (2.11) like the diﬀuse component in Equation (2.9).
Therefore, we approximate I s with

Is



Is Is

Is



I Cap ICap

Is ,

(2.12)

IwChapearendIsICap areΩ

Phong r, ω, s  ω dω is
the corresponding radiance

the radiance value without visibility, and values of I s and Is by substituting an alge-

braically friendly function for the Phong lighting function. As shown in Figure 2.5,

Equation (2.12) expresses I s as a fraction of the radiance value without visibility Is .

The intention is to isolate the inﬂuence of visibility from the lighting function as a

ratio. Such that we can evaluate the ratio with an algebraically friendly function, while preserving the feature of the lighting function with Is . Our algebraically friendly function is

Cap



r,

ω,

ξ





max

r

ω



cos

ξ

,

0


,

1cos ξ

(2.13)

where ξ is the radius of a circular window W r, ξ  centered at r. Better than that, both
the ratio I Cap ICap and Is are computationally friendly. Is can be obtained easily from the cubemap mipmap of . However, calculating the ratio is a bit tricky. Expanding the ratio, we have

2.5 Lighting Evaluation

25

Figure 2.5. Visualizing each of the components separately. The specular component is esti-
mated by the product of a ratio estimator and the ﬁltered  without visibility.

  I Cap 

Cap r, ω, ξ   ω dω

A



max r ω cos ξ, 0  ω dω

A

.

  ICap

Cap r, ω, ξ   ω dω

max r ω  cos ξ, 0  ω dω

Ω

Ω

(2.14)

Examining Equation (2.14) carefully, we see that r cannot be factorized out of the integral due to the presence of the max function. The max function can be considered as a form of visibility function, which is a circular window in this case. By using this fact, the max function place can be taken by changing the domain to the intersection of the original domain and the circular window; then, we have

 r ω cos ξ   ω dω

I Cap 

AW r ,ξ 

.

 ICap

r ω cos ξ   ω dω

ΩW r ,ξ 

(2.15)

Without the explicit presence of the max function, r can be factorized, and we have

  r 

ω ω dω cos ξ 

 ω dω

I Cap 

AW r,ξ 

AW r,ξ 

.

ICap

  r 

ω ω dω cos ξ 

 ω dω

Wr,ξ 

Wr,ξ 

(2.16)

The integrals in Equation (2.16) can all be evaluated eﬃciently using a similar formu-
lation to Equation (2.10) given the contour of A W r, ξ . The contour of A W r, ξ 
can be obtained by clamping all the unit vectors in A to the circular window; each
clamped unit vector is given by

vclamp a i   r

cos ξ

 a i rb  rb a i rt  rt a i rb 2 a i  rt 2
ai,

sin ξ ,

if a i  r  cos ξ; otherwise,

(2.17)

26

2. Applying Vectorized Visibility on All Frequency Direct Illumination

Figure 2.6. Intersecting a visibility contour with a circular window. (a) shows a visibility contour (black and red) and a circular window (blue). (b) shows the visibility contour of their intersection. As indicated by the red arrows, the clamped unit vectors will form either the missing paths of intersected regions or some zero-area round trips.
where rb , r, rt  are orthonormal vectors. The clamped unit vectors will form either the
missing paths of intersected regions or some zero-area round trips as shown in Figure 2.6(b).
The GLSL code for the evaluation process of the diﬀuse and specular components is provided below.

void vsatlookup(

sampler2D ysat, // the pre-integrated environment map

sampler3D vcptmap, // the vectorized visibility 3D texture

vec2 ci,

// the texture coordinates for vmap

corresponding to a given vertex

vec3 cv,

// the vertex position in global coordinate

system

mat3 mtx,

// local to global coordinate system

rotation matrix

out vec4 ID,

// the diffuse component radiance value

float phongbound, // the glossiness

vec3 ax,

// these three vectors form a coordinates

vec3 ay,

// system with ay pointing the reflected

vec3 az,

// view direction

out vec4 II)

// the specular component radiance ratio

{

int DD = textureSize(vcptmap, 0).z; // the number of position

// vector in the visibility

float dd = 1.0 / float(DD);

vec3 td = vec3(ci, 0.5 * dd);

vec3 ll;

// a temp variable for unit vectors

2.5 Lighting Evaluation

27

vec2 d0, d1; vec2 a0, a1;

// the spherical coordinates of a line segment for the diffuse component
// the spherical coordinates of a line segment for the spec. component

vec2 bound = vec2(cos(phongbound), sin(phongbound)); ID = vec4(0.0); II = vec4(0.0);

// synthesize visibility ll = normalize((mtx * texture(vcptmap, td).xyz) - cv); d0 = Float32Angle(ll); // unit vector to spherical coords

// clamping to the circular window a0 = Float32Angle(specboundc(bound, ax, ay, az, ll));

for (int i = 1; i < DD; i++) {
td.z += dd;

// doing integration line segment // by line segment

// synthesize visibility ll = normalize((mtx * texture(vcptmap, td).xyz) - cv); d1 = Float32Angle(ll); // unit vector to spherical coords

// clamping to the circular window a1 = Float32Angle(specboundc(bound, ax, ay, az, ll));

// generalize SAT lookup for diffuse component ID += satlookup(ysat, tppair(vec4(d0, d1)));

// generalize SAT lookup for specular component II += satlookup(ysat, tppair(vec4(a0, a1)));

d0 = d1; a0 = a1; }

// south pole correction for diffuse component if (ID.w < -0.01)
ID += texture(ysat, vec2(DEND, 1.0));

// south pole correction for specular component if (II.w < -1.0)
II += texture(ysat, vec2(DEND, 1.0)); }

28

2. Applying Vectorized Visibility on All Frequency Direct Illumination

2.6 Shader Implementation for the Generalized SAT Lookup

The ordinary SAT lookup has a well-known limitation, which is the domain of the integral ought to be an axis aligned rectangle. The straightforward way to extend it to support region deﬁned with line segments is approximating the line segments with horizontal lines. However, the horizontal line SAT lookup is not good enough for our representation because our representation is deﬁned with non-axis aligned line segments. Therefore, we further extend the SAT to the generalized SAT lookup.
Suppose we want to ﬁnd the integral swept by a non-axis aligned line segment as shown in Figure 2.7(a), and the functions in concern are the three component function of the diﬀuse component in Equation (2.9). Then, the true integral G will be a value in
between E and F, where E  SAT ω ωφ2 , θ1 SAT ω ωφ1, θ1  and F  SAT ω ωφ2 , θ2 SAT ω ωφ1, θ2 . Thus, we can express the true
integral G as an interpolation of E and F, i.e.,

G  vsat SAT ω ω , a 1, a 2   1 β E  βF ,

(2.18)

where a 1  φ1, θ1 , a 2  φ2 , θ2 , β is an interpolation factor in between 0,1, and vsat is the generalized SAT lookup operator for the line segment a 1, a 2 .
To compute β , we exploit the three component function of ω ω. Although the
three component function is designed for the radiance evaluation in the ﬁrst place, we
can also consider them as some weighted directions, which are weighted by the illumi-
nations . Considering it this way, we can calculate a centroid c  normalize E  F ,
which is the dominated light source direction of the yellow region in Figure 2.7(a).
If we assume that the dominated light source has all the energy and attains its
maximum at xc with a bell shape distribution sech 2  x  x c , the radiance evaluation

(a)

(b)

(c)

Figure 2.7. (a) Non-axis aligned line segment. (b) The bell shape function. (c) The rendering results comparison between the ordinary SAT vs. the generalized SAT. The rendered image using the generalized SAT lookup eliminates the jaggy shadow boundaries that caused by the wellknown limitation of the ordinary SAT.

2.6 Shader Implementation for the Generalized SAT Lookup

29

will degenerate to the 1D scenario as shown in Figure 2.7(b); xc is the perpendicular distance from c to the diagonal. Then, the interpolation factor β can be approximated by



 β 

0 

sech 2 sech 2

α α

x x

 

xc xc

 dx  dx



1 2



1 2

tanh

 α x c

,



(2.19)

where α is an empirical constant depending on the environment map resolution, and the suggested value for a 6256256 cubemap is 5.8. Equation (2.19) is formulated using a similar principle to derive the specular component radiance equation, i.e., the radiance ratio with and without visibility. The only diﬀerence is that it is done in a 1D scenario. The GLSL code for the generalized SAT lookup is provided below.

vec4 satlookup(sampler2D satmap, vec4 cc) {
vec2 A = cc.xy - cc.zw; vec4 E = texture(satmap, cc.xy) - texture(satmap, cc.zy); vec4 F = texture(satmap, cc.xw) - texture(satmap, cc.zw);
float dl = 0.5; if (abs(A.x) > 0.0001 && abs(A.y) > 0.0001) {
vec2 B = Float32Angle(normalize((E.xyz - F.xyz) * sign(E.w - F.w)));
B.x = (B.x + 0.5 / DSIDE) / 1.5; B.x += step(B.x, min(cc.x, cc.z)) / 1.5; B -= cc.zw; dl += tanh(5.87 * clamp((B.y / A.y - B.x / A.x), -1, 1))
* 0.5; }
return mix(E, F, dl); }

To demonstrate that the generalized SAT lookup can appropriately account for the integral of non-axis aligned line segments, we compare it to the horizontal line SAT lookup. If the SAT lookup can only provide the integrals of axis aligned line segments, the rendered image will have some jaggy shadow boundaries as shown in Figure 2.7(c). As shown in the ﬁgure, the rendered image using the generalized SAT lookup does not have the jaggy shadow boundaries.

30

2. Applying Vectorized Visibility on All Frequency Direct Illumination

2.7 Dynamic Tessellation
As mentioned in Section 2.3, applying Green’s theorem converts the radiance equation to the contour integral Equation (2.6). Representing the visibility functions in vector graphics form decouples them from the illumination resolution. These make Equation (2.6) an accurate and computationally friendly variant of the radiance equation. Evaluating the radiance equation with indeﬁnite accuracy using Equation (2.6) is intriguing; unfortunately, as promising as it might sound, it does not necessarily imply a better image quality. As a matter of fact, if the amount of sampled transfer functions does not catch up with the screen resolution, it will just be exaggerating the vertex-to-pixel interpolation artifact indeﬁnitely (see Figure 2.8(b)). In terms of Digital Signal Processing, this is the aliasing caused by not enough sampled transfer functions.
The visibility function for every point on the surface has diﬀerent content in general. Requiring a faithful all frequency shadows quality (see Figure 2.8(a)), we will have to sample the 3D model densely for the transfer functions, where the sampling needs to be dense enough to roughly correspond every onscreen pixel to a distinct sampled transfer function. We can roughly workout the required data size given the screen resolution. For instance, given the screen resolution 1280962 and the visibility function resolution 6256256 (1 bit storage), we will have 56 gigabytes for the visibility functions.
Most PRT approaches will have the visibility function premultiplied to the lighting functions, and compress the resulting data by removing redundancy across multiple dimensions (e.g., across the vertex dimension). There exist quite a number of compression algorithms for this task, e.g., cluster PCA [Sloan et al. 2003, Wang et al. 2009, Wang et al. 2013] and cluster tensors [Tsai and Shih 2006]. However, as demonstrated in [Ho et al. 2015], if the transfer functions are compressed down to a few megabytes, there can be signiﬁcant rendering artifacts.
General purpose compression algorithms come with a blossom variety of emphases. More elaborated PRT approaches will also have antialiasing integrated to their compression algorithms (e.g., [Ng et al. 2004] and [Wang et al. 2009]). These algorithms bound the signal frequency to safeguard them from exceeding the representation capability of the given amount of samples. In less formal words, that is blurring everything with higher order ﬁlters. Again, antialiasing is not the only option to handle aliasing. Aliasing, as the negative eﬀect caused by not enough samples, can also be solved by increasing the amount of samples. It is precisely what we does for the ground truth image, i.e., increasing the amount of sampled transfer functions such that each pixel has its own transfer function.
To realize this, we utilize the vectorized visibility. As mentioned in Section 2.4, the vectorized visibility is designed to model all visibility functions in a small neighborhood. Using this property, we can synthesize an individual visibility contour for each pixel. To render the direct illumination of a surface position, we synthesize the visibility contour using the vectorized visibility of its neighboring vertices, and then feed the synthesized visibility contours to Equation (2.6) for evaluations (i.e., the

2.7 Dynamic Tessellation

31

(a) Ground truth

(b) Per-vertex

(c) Per-fragment

(d) Dynamic tessellation

Figure 2.8. The rendering results for the teapot model: (a) ground truth, (b) per- vertex, (c) perfragment, and (d) dynamic tessellation. Figure (a) is the reference image generated using 6256256 number of directional lighting sources, where the visibility is handled by using shadow volume algorithm. Figures (b)–(d) are the rendering results of our algorithm using different rendering modes. The per-vertex result was intended to be the ground truth; however, it failed because of its severe aliasing.

radiance equation in contour integral form). If we intend to do per-fragment direct illumination rendering, the surface positions will all be the rasterized vertex positions of the onscreen pixels. As shown in Figure 2.8(c), the per-fragment rendering result accurately resemble the ground truth Figure 2.8(a) (therefore, aliasing free naturally).
Besides, if we intend to do per-vertex direct illumination rendering (see Figure 2.8(b)), the surface positions will all be the vertices. In other words, the whole rendering setup boils down to deﬁning the neighboring vertices for the surface positions in concern. This ﬂexibility gives us more freedom for the design of rendering modes.
Doing the rendering 1280962 full screen per-fragment with the radiance equation evaluated at 400k illumination resolution, our algorithm achieves a few frames per

32

2. Applying Vectorized Visibility on All Frequency Direct Illumination

second with a GTX 660 display card. Given the image quality requirement and the rendering capabilities of our approach (see Section 2.1), this rendering speed is practically impossible for most PRT approaches to achieve. However, the uncompromising image quality is simultaneously the major drawback of our algorithm, as the image quality is not always the primary concern. In a lot of cases, users might just prefer sacriﬁcing some image quality over a real time performance, which could be addressed gracefully by the antialiasing feature in most PRT approaches, while neither the pervertex nor the per-fragment of our algorithm oﬀer such a ﬂexible tradeoﬀ. To cover this loose end, we improvise a little bit using the ﬂexible rendering setup of our algorithm.
As mentioned in the introduction, we can use dynamic tessellation to provide a ﬂexible tradeoﬀ between speeds versus quality, which is faster than the per-fragment with better quality than the per-vertex and has a better scalability. If we examine the per-fragment rendered result in Figure 2.8(c), we can see that a good portion of regions are almost identical to the per-vertex result in Figure 2.8(b). This is equivalently saying that a lot of pixels are using some extremely expensive computation just to render some blurry signals. In other words, the utilization is very low.
To improve the utilization, an intuitive idea would be adjusting the sampling rate of individual triangles on demand according to the signal frequency. In terms of dynamic tessellation, it will decide the tessellation level per triangle. For the frequency estimation, we estimate the frequency per vertex with
 freq  A, n,     abs n  A SAT ω  ω  ω dω, (2.20)
where  is a blurry version of . Then, for a triangle, we have three frequency values
 freq0 , freq1, freq2  corresponding to its vertices. The frequency value per triangle
freqΔ is taken to be the maximum of the three frequency values. To convert freqΔ to the tessellation level per triangle LvΔ, we use the following equation

Lv Δ  floor tessscale freqΔ ,

(2.21)

where tessscale is an arbitrary constant for the tradeoﬀ. Adjusting tessscale to a higher value will result in a ﬁner grained tessellation. The parameter tessscale is coupled with the frame rates and adjusted automatically using a PID controller. In this case, suppose the computational power could be quantiﬁed, then each unit of computation power would bear more value in terms of rendering quality.
We allocate the GPU memory for the vertices, the inner tessellated vertices of edges, and the inner tessellated vertices of triangles as three textures, namely vmap, emap, and fmap (see Figure 2.9), where the size of emap and fmap are chosen to be large enough for the maximum tessellation level. They are managed this way for two practical reasons. First, any common edge of two neighboring triangles must have a single tessellation level in order to render a smooth image across neighboring triangles with diﬀerent tessellation levels. Second, as our radiance evaluation is computationally expensive, we want to ensure that the vertices and the common edges are evaluated only once.

2.7 Dynamic Tessellation

33

Figure 2.9. An illustrative ﬁgure to demonstrate the ﬂow of our dynamic tessellation. Three textures, namely vmap, emap, and fmap store the radiance values of the vertices, the inner tessellated vertices of edges, and the inner tessellated vertices of triangles. Managing it this way allows us to render the image seamlessly across the triangles with diﬀerent tessellation levels.

Each row in emap and fmap contains the radiance of the tessellated vertices of a triangle. Given a tessellation level Lv, we can calculate the number of tessellated vertices of the inner edges and the inner triangles in close form, i.e.,

N edge  Lv   2 Lv 1,

(2.22)

2 Lv 12 Lv 2

N face  Lv  

2

.

(2.23)

Each edge and each face are associated with their own tessellation level. Given an edge
with tessellation level Lv edge, we will only evaluate the radiance for N edge Lvedge  num-
ber of pixels in the emap. Similarly, given a triangle with tessellation level Lvface,
N face  Lvface  number of pixels will be evaluated in the fmap. The following geometric
shaders are the shaders for updating pixels of the inner edges and the inner triangles
depending on their tessellation levels.

void main() // emap {
float level_t0, level_t1, level;
vec2 wh = vec2(textureSize(ttmap, 0));
// level_t0 and level_t1 are the two tessellation // levels of the current edge. level_t0 = texture(ttmap, vec2(mod(btriangle_index[0].x + 0.5,
512), floor((btriangle_index[0].x + 0.5) / 512) + 0.5) / wh).x; level_t1 = texture(ttmap, vec2(mod(btriangle_index[0].y + 0.5,
512), floor((btriangle_index[0].y + 0.5) / 512) + 0.5) / wh).x;

34

2. Applying Vectorized Visibility on All Frequency Direct Illumination

// the edge tessellation level is taken to be the minimum // of two level_t0 and level_t1 level = min(level_t0, level_t1);
float N = pow(2, level); // Equation (2.22)
// emit a line primitive covering N-1 number of pixels. EndPrimitive(); }
void main() // fmap {
// the face tessellation level float level = texture(ttmap,
vec2(mod(btriangle_index[0] + 0.5, 512), floor((btriangle_index[0] + 0.5) / 512) + 0.5) / vec2(textureSize(ttmap, 0))).x;
float N = pow(2, level); N = (N - 1) * (N - 2) / 2; // Equation (2.23)
// emit a line primitive covering N number of pixels. EndPrimitive(); }
During the render-time, we prepare the per-vertex frequency map and then the perface tessellation level map. Given the tessellation levels, we evaluate the radiance values for vmap, emap, and fmap. Then, we render vmap, emap, and fmap to a screen buﬀer through straightforward rasterization (notice that, special attention has to be paid to render the common edges). The screen buﬀer will then contain the radiance values for the diﬀuse component and specular component individually. Finally, the post-processing eﬀects, e.g., texture mapping, material color, and bump mapping, etc., will be applied. The most beautiful part of this process is that it is all completed within the GPU.
2.8 Results
Now, we examine the performance aspect of our rendering algorithm. We begin with the rendering information and the model information. The rendering was carried out with a GTX 660 display card. The screen resolution was 1280962. The illuminations consisted of 400k light sources (i.e., St. Peter HDR cubemap; resolution 6256256).
As mentioned in Section 2.1, our algorithm does not require 3D models to be densely tessellated. As shown in Table 2.1, the girl model and the teapot model only have 6655 and 2038 vertices, which are coarsely tessellated, and they have 10333 and

2.8 Results

35

3706 faces. Their precomputed data sizes are 11.5 MB and 3.14 MB, and the allocated memory sizes to buﬀer the required tessellated data are 10.9 MB and 4.1 MB. Given the GPU graphics memory capacity nowadays (usually in GBs), both the precomputed data and the buﬀer for the tessellated data can reside at the GPU graphics memory comfortably. The preparation times for their vectorized visibility are 3.27 minutes and 0.93 minutes, where the preparation time is measured from loading the model ﬁle to having all the required data saved to the hard disk.
The dynamic tessellation here serves the role to distribute the computational resources adaptively to the region in need. tessscale controls the overall spending of the computational resources. Figure 2.10 shows the distribution of samples with the awareness to the content frequency given more and more computational resources. As shown in Figure 2.10, the density of samples starts increasing from some selective regions ﬁrst, then the less selective regions.
In particular, just by evaluating a faction of sample points to that of the per-fragment rendering (see Figure 2.10(d)), the dynamic tessellation (see Figure 2.11(d)) attains almost identical result to the per-fragment rendering (see Figure 2.11(c)), or even the brute force rendering (see Figure 2.11(a)).
The PID controller allows the users to specify their expected frame rate. The PID will adjust tessscale and try to fulﬁll the user expectation. The realized frame rate is limited to somewhere in between the per-vertex and the per-fragment frame rates. In particular, for the girl model, the expected frame rate we speciﬁed is 20 fps, the perfragment frame rate is 2 fps, and the per-vertex frame rate is 83 fps (see Table 2.1). The dynamic tessellation improves the rendering speed to almost ten times the perfragment frame rate, while achieving almost identical rendering results.
In addition, our algorithm can also capture the visual impression of the specular component shadows as shown in Figure 2.8 and Figure 2.11. However, the visual impression of the specular component shadows is not very appreciable in static images. Please visit the video to visualize the behavior of the specular component shadows in action5.

(a) tessscale 1

(b) tessscale 3

(c) tessscale 7 (d) tessscale 10.1

Figure 2.10. The dynamic tessellation with diﬀerent tessscale for the girl model. Increasing the tessscale will increase the density of samples for the more demanding regions.

5 The video of the specular component shadows from our demo program https://youtu.be/ dlLLZTyY7xs.

36

2. Applying Vectorized Visibility on All Frequency Direct Illumination

(a) Ground truth

(b) Per-vertex

(c) Per-fragment

(d) Dynamic tessellation

Figure 2.11. The rendering results for the girl model: (a) ground truth; (b) per-fragment; (c) per-vertex; (d) dynamic tessellation tessscale 10.1. Just by evaluating a faction of sample points (see Figure 2.10(d)) w.r.t. that of the per-fragment rendering, the dynamic tessellation attains almost identical result to the per-fragment.

Our algorithm also supports the BRDF editing feature [Ben-Artzi et al. 2006], which allows us to adjust the specular component glossiness continuously. The video shows the specular component glossiness transition from mirror to very rough surface6.
The intention of our specular component approximation, Equation (2.12), is to isolate the inﬂuence of visibility from the lighting function as a ratio. For the lighting function, ﬁltering the illuminations without visibility can be handled with relatively high accuracy even with the primitive form of cubemap mipmap. For the ratio estimator, because of its geometrical formulation, the shadows will topologically make sense on its own. Therefore, although the rendered results do not agree to the ground truth
6 The video demonstration of our BRDF editing feature https://youtu.be/tvNnQfL5UT4.

2.9 Conclusion

37

Number of vertices Number of faces Data size Per-vertex fps Per-fragment fps
Dynamic tessellation fps
Total memory size to buﬀer the tessellated data Preparation time for the visibility

The girl model 6655 10333
11.5 MB 83 fps 2.1 fps
20 fps (explicitly maintained with PID)
10.9 MB
3.27 minutes

The teapot model 2038 3706
3.14 MB 192 fps 2.1 fps 20 fps (explicitly maintained with PID)
4.1 MB
0.93 minutes

Table 2.1. The rendering information. The rendering is carried out with a GTX 660 display card. The screen resolution is 1280962. The illuminations consist of 400k light sources (i.e., St. Peter HDR cubemap; resolution 6256256). The information provided in the table shows that our algorithm does not require 3D models to be densely tessellated, the precomputed data size is very small, and the dynamic tessellation ten times the frame rate while maintaining the image quality.

precisely, there will be no identiﬁable visual clues within a rendered image, e.g., light bleeding, to tell whether the ground truth images or our rendered results are closer to our world (see Figure 2.8 and Figure 2.11). However, the approximation damage is indeed bigger than that would have been suggested by the static image comparisons. The video7 shows the visual lost due to our approximation, and we can see that our specular shadows are less responsive to the illuminations w.r.t. the ground truth.
2.9 Conclusion
In this article, we presented the implementation and the rendering algorithm relying on vectorized visibility. By exploiting its vector graphics properties and the GPU parallel architecture, our rendering algorithms supports a number of functionalities which appear to be impractical to be supported simultaneously, e.g., the per-fragment direct illumination with all frequency shadow quality, using coarsely tessellated 3D models, the BRDF editing, etc.
By integrating the dynamic tessellation feature, the scalability of our algorithm is improved drastically, which is faster than the per-fragment with better quality than the per-vertex. Having the frame rates coupled with the tessellation level, we practically
7 The video to demonstrate the visual lost due to our specular approximation https://youtu.be/ cgiKwniyktA.

38

2. Applying Vectorized Visibility on All Frequency Direct Illumination

make the frame rate an intrinsic property among the devices with diﬀerent computational capability.
While interreﬂection is not yet supported by the presented algorithm, it is feasible to extend our algorithm to support it by approximating the transfer functions in the higher dimension space, which is one of our ongoing research projects.
2.10 Acknowledgments
The work is supported by a research grant (CityU 11259516) from the Hong Kong Special Administrative Region.
Bibliography
BEN-ARTZI, A., OVERBECK, R., AND RAMAMOORTHI, R. 2006. Real-time BRDF editing in complex lighting. In ACM Trans. Graph., 25:3, pp. 945–954.
BRAUER, D. 2010. MatCap. URL: http://wiki.unity3d.com/index.php/MatCap.
CROW, F. 1984. Summed-area tables for texture mapping. In ACM SIGGRAPH computer graphics, 18:3.
ELCOTT, S., ET AL. 2016. Rendering techniques of ﬁnal fantasy XV. ACM SIGGRAPH 2016 Talks.
HO, T., XIAO, Y., FENG, R., LEUNG, C., AND WONG, T. 2015. All-Frequency Direct Illumination with Vectorized Visibility. In IEEE Trans. Vis. Comput. Graph., 21:8, pp. 945–958.
KAUTZ, J., SLOAN, P., AND SNYDER, J. 2002. Fast, arbitrary BRDF shading for low-frequency lighting using spherical harmonics. In Proc. 13th Eurograph. Workshop Rendering, pp. 291– 297.
LAM, P., HO, T., LEUNG, C., AND WONG, T. 2010. All-frequency lighting with multiscale spherical radial basis functions. In IEEE Trans. Vis. Comput. Graph., 16, pp. 43–56.
LIU, X., SLOAN, P., SHUM, H., AND SNYDER, J. 2004. All-frequency precomputed radiance transfer for glossy objects. In Proc. Eurograph. Symp. Rendering, pp. 337–344.
MATUSIK, W., PFISTER, H., BRAND, M., AND MCMILLAN, L. 2003. A data-driven reﬂectance model. In ACM SIGGRAPH 2003 Papers (SIGGRAPH ‘03). ACM, pp. 759–769. DOI: https://doi.org/10.1145/1201775.882343.
MORENO, J. 2018. MatCap Shaders. http://jeanmoreno.com/unity/matcap.
NG, R., RAMAMOORTHI, R., AND HANRAHAN, P. 2003. All-frequency shadows using non-linear wavelet lighting approximation. In ACM Transactions on Graphics, 22:3, pp. 376–381.
NG, R., RAMAMOORTHI, R., AND HANRAHAN, P. 2004. Triple product wavelet integrals for allfrequency relighting. In ACM Transactions on Graphics, 23:3.
RUSSELL, J. 2015. HDR Image-Based Lighting on the Web. In WebGL Insights. CRC Press, pp. 253–260.

Bibliography

39

SCHEUERMANN, T. AND ISIDORO, J. 2006. Cubemap ﬁltering with cubemapgen. Game Developers Conference 2006.
SLOAN, P., KAUTZ, J., AND SNYDER, J. 2002. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. ACM Trans. Graph. 21, pp. 527– 536.
SLOAN, P., HALL, J., HART, J., AND SNYDER, J. 2003. Clustered principal components for precomputed radiance transfer. ACM Trans. Graph., 22, pp. 382–391.
TSAI, Y. AND SHIH, Z. 2006. All-frequency precomputed radiance transfer using spherical radial basis functions and clustered tensor approximation. In ACM Transactions on Graphics, 25:3.
WANG, J., REN, P., GONG, M., SNYDER, J., AND GUO, B. 2009. All-frequency rendering of dynamic, spatially-varying reﬂectance. In ACM Trans. Graph., 28, pp. 1–10.
WANG, R., PAN, M., CHEN, W., REN, Z., ZHOU, K., HUA, W., AND BAO, H. 2013. Analytic double product integrals for all-frequency relighting. In IEEE Trans. Vis. Comput. Graph., 19:7, pp. 1133–1142.

3
I
Nonperiodic Tiling of Noisebased Procedural Textures
Aleksandr Kirillov
3.1 Introduction
Procedural noise functions have been one of the key tools for adding visual ﬁdelity in computer graphics for decades. They serve as a foundation for landscape geometry synthesis, creation of textures containing surface properties such as color and normals, simulation of atmospheric eﬀects and many other tasks.
With the ever-growing game environment scale and the amount of detail expected from modern games, developers are more and more frequently faced with content production challenges. Reduction of the time required for authoring and iterating on content is always among the hottest topics in the industry.
Procedural content creation is an increasingly popular solution to this problem. Procedural methods allow developers to automate and simplify tasks, from object placement to texture creation. Many large studios as well as independent developers that cannot aﬀord to produce assets manually already employ them. It is highly likely that these methods are going to become an industry standard, and an integral part of all modern content pipelines.
Most of the time games cannot aﬀord to evaluate procedural noise functions at runtime and instead store the precomputed results in textures. The majority of them only use basic tiling options provided by the hardware. Noise functions that are designed with eﬃcient evaluation on the GPU in mind are periodic with a relatively small period. Both cases lead to either repetition or loss of detail. A common approach to overcome this is to increase the texture resolution, to add decal textures or employ multitexturing. This results in an increase in memory consumption and memory bandwidth requirements.
In this chapter, we present a method to combine noise-based procedural texture synthesis with a nonperiodic tiling algorithm. We describe modiﬁcations to several popular procedural noise functions that directly produce texture maps containing the
41

42

3. Nonperiodic Tiling of Noise-based Procedural Textures

smallest possible complete Wang tile set. Our approach can be used as a preprocessing step or during application runtime.
Additionally, we present several improvements of the algorithm [Wei 2004] that implements Wang tiling on the GPU. We show how our modiﬁcations enable nonperiodic tiling for a large range of noise-based procedurally generated textures (see an example in Figure 3.1). Finally, we analyze the eﬀect these modiﬁcations have on the performance, and discuss their limitations.

Figure 3.1. An image generated with our algorithm.
3.2 Wang Tiles
Wang tiling uses a set of rectangular tiles of the same size with color-coded edges. A valid surface tiling is obtained by any composition of tiles without rotation or reﬂection where all the adjacent tiles share an edge color (see Figure 3.2). The initial assumption was that if a ﬁnite set of tiles could tile a plane in a valid way, a periodic tiling would exist as well. Later research has shown that a ﬁnite set exists that would tile a plane only nonperiodically. A series of successor works reduced this set, the latest [Jeandel and Rao 2015] proving that the minimal set required for nonperiodicity consists of 11 tiles with 4 edge colors.
Wang tiles were ﬁrst used for synthesizing large non-repetitive textures by Stam [1997]. He described construction of a very limited set of patterns, mostly focusing on water rendering. Neyret and Cani [1999] proposed using triangular patches with colorcoded edges instead of rectangles, described an algorithm to map those patches to surfaces, and provided techniques to generate them procedurally based on Perlin and Worley noises. Wei and Levoy [2000], as well as Efros and Freeman [2001], proposed methods to avoid repetition by using a small texture tile to create a larger non-repetitive texture that looks similar to this tile. While the result is guaranteed to be seamlessly tileable, both methods can introduce seams inside the texture and require additional computation and storage. Cohen et al. [2003] adapted these methods to generate sets of Wang tiles and introduced a stochastic process of laying down individual tiles to produce nonperiodic tilings. Wei [2004] enabled hardware ﬁltering for Wang tiles by laying them out in a single texture that itself forms a valid tiling, and proposed a hashbased algorithm to map the tiles on a plane.

3.3 Nonperiodic Tiling of Procedural Noise Functions

43

(a)

(b)

Figure 3.2. A schematic representation of Wang tiling. (a) A complete Wang tile set with two edge colors. (b) A valid Wang tiling of a surface using the tile set from (a).

Cohen [2003] observed that Wang tiles do not take into account tile corners, which may result in a discontinuity in the resulting image. Corner tiles [Lagae and Dutré 2006] address this problem by restricting the diagonal tile neighbors in addition to the horizontal and vertical ones.
We observe, however, an insuﬃciency in modern Wang tile set synthesis methods. Many procedural noise functions are limited to producing only periodic images. Noise functions that allow the construction of nonperiodic noise at runtime are usually computationally expensive, which makes them unusable by many real-time applications. We lift these limitations by combining Wang tiles and procedural noise functions.
In the following section we present modiﬁcations to several noise synthesis algorithms (see Figure 3.3) which directly output a minimal complete set of Wang tiles (16 square tiles with 2 edge colors). The tiles are arranged in a single texture map as proposed by Wei [2004]. Our methods guarantee seamless tiling, can be extended to support more edge colors or higher dimensions, and can be applied to the noise synthesis either during precomputation or at runtime. Full implementations of the original and the modiﬁed noise functions are available in the accompanying source code.
3.3 Nonperiodic Tiling of Procedural Noise Functions
3.3.1 Perlin Noise
Perlin noise [Perlin 1985, Perlin 2002] (Figure 3.3(a)) is probably the most well-known procedural noise function. It is fast and simple, and is widely used to simulate natural phenomena, such as clouds, ﬁre and smoke.
Perlin noise assigns a pseudo-random gradient vector to each point of the integer lattice. The value of the function at a point on the 2D plane is then determined by smooth interpolation between the four closest gradient vectors projected onto the vectors between the current point and the lattice point each vector is assigned to (see Listing 3.1). The pseudo-randomness is the result of successive hashing of the lattice point coordinates with a permutation table.

44

3. Nonperiodic Tiling of Noise-based Procedural Textures

(a)

(b)

(c)

(d)

(e)

Figure 3.3. Procedural noise function examples. (a) Perlin noise. (b) Better gradient noise. (c) Anisotropic and (d) isotropic Gabor noises. (e) Worley noise.

// The code in this chapter is tuned for better readability, // not for optimal performance. The code is provided in Cg.
float PerlinNoise2D(float2 xy) { uint2 in00 = floor(xy); uint2 in11 = in00 + uint2(1u, 1u); uint4 bounds = uint4(in00, in11);
// Interpolate is either a cubic (Perlin noise) or // a quintic (Improved Perlin noise) polynomial float2 weights = Interpolate(xy - in00); uint2 out00 = in00; uint2 out01 = uint2(in00.x, in11.y); uint2 out10 = uint2(in11.x, in00.y); uint2 out11 = in11;
// See the accompanying sample code for definitions // of gradients[] and hash() float g00 = dot(gradients[hash(out00)], xy - bounds.xy); float g01 = dot(gradients[hash(out01)], xy - bounds.xw); float g10 = dot(gradients[hash(out10)], xy - bounds.zy); float g11 = dot(gradients[hash(out11)], xy - bounds.zw); float v0 = lerp(g00, g01, weights.y); float v1 = lerp(g10, g11, weights.y); return lerp(v0, v1, weights.x); }
Listing 3.1. Evaluation of 2D Perlin noise.
Many noise functions are derived from Perlin noise. Improved Perlin noise [Perlin 2002] reduced visual artifacts in the derivatives and improved overall noise appearance. Modiﬁed noise [Olano 2005] replaced the permutation table with a hash function

3.3 Nonperiodic Tiling of Procedural Noise Functions

45

based on a pseudo-random number generator (PRNG), making the function evaluation very eﬃcient on the GPU. Better gradient noise [Kensler et al. 2008] (Figure 3.3(b)) used a permutation table per dimension, and diﬀerent hashing to better decorrelate the values. It also widened the ﬁlter kernel to improve band-limitation.
All the functions derived from Perlin noise have a common property: they are periodic, with a relatively small period. The period of these noise functions is determined either by the period of the hash function or by the size of the permutation table.
Toroidal boundary handling is a common way of making the lattice-based noise functions seamlessly tileable. To achieve wrapping around N, the lattice coordinates are taken with modulo N:
uint2 out00 = in00 % uint2(N, N); uint2 out01 = uint2(in00.x, in11.y) % uint2(N, N); uint2 out10 = uint2(in11.x, in00.y) % uint2(N, N); uint2 out11 = in11 % uint2(N, N);
We use a similar approach to construct a Wang tile set. We subdivide all the points of the lattice into two non-intersecting groups: boundary points, which form the boundaries of the Wang tiles within the texture, and inner points. We do a further subdivision of the boundary points: corner points; C; vertical border points (one subgroup per Wang tile edge color); V0 and V1; and horizontal border points; H 0 and H1 (see Figure 3.4(a)). The modiﬁed function needs to guarantee that the values at the tile borders that have the same edge color are the same, or, in other words, that the lattice points that are at the same position local to the tile and are in the same group share a gradient vector.

(a)

(b)

(c)

Figure 3.4. A schematic representation of our method for producing a Wang tile set from Perlin noise. (a) Lattice point subdivision. Each group is assigned a diﬀerent color: grey (C), light blue (V0), dark orange (V1), light orange (H 0), dark blue (H1), white (inner points). (b) The resulting positions of points in each group after coordinate mapping. The inner points are not aﬀected. (c) The resulting noise texture.

46

3. Nonperiodic Tiling of Noise-based Procedural Textures

uint2 TransformCoord(uint2 coord) { // (kPointsPerRow + 1)^2 is the size of the lattice covering // the resulting texture (65x65 in this case). // Each tile gets (kPointsPerRow / 4 + 1) lattice points const uint2 kPointsPerRow = 64; const uint2 kPointsPerTile = kPointsPerRow / 4;
uint2 localCoord = coord % kPointsPerRow; uint2 inTileCoord = localCoord % kPointsPerTile; uint2 tileCoord = localCoord / kPointsPerTile;
bool isXBorder = inTileCoord.x == 0; bool isYBorder = inTileCoord.y == 0;
// Tile edge colors in the final layout: // tile coord (binary): 00 01 10 11 // left: 0 0 1 1 // bottom: 0 0 1 1 uint left = (tileCoord.x / 2) & 1; uint bottom = (tileCoord.y / 2) & 1;
uint2 offset = uint2(bottom, left) * kPointsPerTile; uint2 borderCoord = inTileCoord + offset; uint borderX = isYBorder ? borderCoord.x : localCoord.x; uint borderY = isXBorder ? borderCoord.y : localCoord.y; uint x = isXBorder ? 0 : borderX; uint y = isYBorder ? 0 : borderY;
return uint2(x, y); }
Listing 3.2. A coordinate mapping function used with Perlin noise to produce Wang tiles.

The easiest way to achieve this is to modify the lattice point coordinates. We ﬁrst
check which group the lattice point belongs to, and then map the points that fall into
the same group to the same region of the lattice (see Listing 3.2).
For simplicity, we chose the following mapping: all corner points end up at 0, 0; horizontal borders either at  xt , 0 or at  xt  K, 0, depending on the edge color; vertical borders at 0, yt  or 0, yt  K , where K equals kPointsPerTile, and  xt , yt 
is the coordinate of the point local to the tile (see Figure 3.4(b)). As long as the regions
that these groups are mapped to do not overlap, this choice does not noticeably aﬀect
the resulting noise.
The last step is to call the TransformCoord function to modify the lattice point
coordinates:

3.3 Nonperiodic Tiling of Procedural Noise Functions

47

uint2 out00 = TransformCoord(in00); uint2 out01 = TransformCoord(uint2(in00.x, in11.y)); uint2 out10 = TransformCoord(uint2(in11.x, in00.y)); uint2 out11 = TransformCoord(in11);
The resulting noise is shown in Figure 3.4(c). The TransformCoord function can be used together with many other lattice-based procedural noise functions. We successfully applied it to modiﬁed noise and Perlin noise with the xor hashing function introduced by Kensler et al. [2008].
In order to avoid the corner problem mentioned in Section 3.2, our modiﬁcations are deﬁned in such a way that all the tile corners are the same, and thus are guaranteed to not produce any discontinuities.
3.3.2 Better Gradient Noise
Better gradient noise [Kensler et al. 2008] uses a 44 ﬁlter instead of a 22 ﬁlter employed by Perlin noise. In order to ensure that the colors at the tile borders are the same, we increase the border thickness to cover two additional rows of lattice points (see Figures 3.5(a) and 3.5(b) and Listing 3.3).
With Perlin noise we had to care only about the left and the bottom borders of the tile, because the other two borders were handled by the adjacent tiles. Here, we also take into account lattice points that lie within the tile, and have to check which group these points belong to as well.
In order to simplify the mapping, we deﬁne the boundary point subdivision into subgroups in a way diﬀerent from the one we used for Perlin noise. The corner points and the points that belong to the borders of one color form one group, C0, while the

(a)

(b)

(c)

Figure 3.5. A schematic representation of our method for producing a Wang tile set from better gradient noise. (a) Lattice point subdivision. The groups have the same colors as in Figure 3.4(a). (b) The resulting positions of points in each group after coordinate mapping. (c) The resulting noise.

48

3. Nonperiodic Tiling of Noise-based Procedural Textures

uint2 TransformCoordWideBorder(uint2 coord) {
// (kPointsPerRow + 1)^2 is the size of the lattice covering // the resulting texture (65x65 in this case). // Each tile gets (kPointsPerRow / 4 + 1) lattice points
const uint2 kPointsPerRow = 64; const uint2 kPointsPerTile = kPointsPerRow / 4; uint2 localCoord = coord % kPointsPerRow; uint2 inTileCoord = localCoord % kPointsPerTile; uint2 tileCoord = localCoord / kPointsPerTile; const uint2 kBorder00 = uint2(2, 2); const uint2 kBorder11 = kPointsPerTile - kBorder00;
bool isX0Border = inTileCoord.x <= kBorder00.x; bool isX1Border = inTileCoord.x >= kBorder11.x; bool isXBorder = isX0Border || isX1Border; bool isY0Border = inTileCoord.y <= kBorder00.y; bool isY1Border = inTileCoord.y >= kBorder11.y; bool isYBorder = isY0Border || isY1Border; bool isBorder = isXBorder || isYBorder; bool isCorner = isYBorder && isXBorder;
// Tile edge colors in the final layout: // tile coord (binary): 00 01 10 11 // left: 0 0 1 1 // bottom: 0 0 1 1 // right: 0 1 1 0 // top: 1 0 0 1
uint left = (tileCoord.x / 2) & 1; uint bottom = (tileCoord.y / 2) & 1; uint right = left != (tileCoord.x & 1); uint top = bottom == (tileCoord.y & 1);
bool zeroX = isCorner || !isXBorder; bool zeroY = isCorner || !isYBorder; uint tileOffsetX = zeroX ? 0 : (isX0Border ? left : right); uint tileOffsetY = zeroY ? 0 : (isY1Border ? top : bottom); uint2 tileOffset = uint2(tileOffsetX, tileOffsetY);
uint2 offset = tileOffset * kPointsPerTile + inTileCoord; return isBorder ? offset : localCoord; }
Listing 3.3. A coordinate mapping function with a wider border.

3.3 Nonperiodic Tiling of Procedural Noise Functions

49

points that belong to the borders of the second color are in another group, C1:
C 0  C V0  H 0, C1  V1  H1. Points from the group C0 are mapped to the coordi-
nate local to the tile,  xt , yt . Points from the other group are mapped to positions with a ﬁxed oﬀset from the coordinate local to the tile,  xt  K, yt  K , where K is equal
to kPointsPerTile. The resulting noise is shown in Figure 3.5(c).

3.3.3 Gabor Noise and Worley Noise

Gabor noise [Lagae et al. 2009] (Figures 3.3(c) and 3.3(d)) is a procedural noise function that oﬀers intuitive and direct control over the power spectrum, and at the same time supports anisotropy. It is based on the sparse convolution noise [Lewis 1989].
Sparse convolution noise is constructed by convolving an arbitrary kernel k with a Poisson noise process γ ,

 N  x, y  γ u, v  k  x u, y v  du dv.

(3.1)

The Poisson process consists of impulses of uncorrelated intensity distributed at ran-
dom uncorrelated locations  x k , yk ,

 γ  x, y  ak δ  x  xk , y  yk .

(3.2)

k

The Gabor kernel g is a multiplication of a circular Gaussian and a directional cosine wave,

g  x, y   Keπa2x2 y2  cos 2πF0  x cos ω0  y sin ω0 .

(3.3)

The parameters K and a control the magnitude and the radius of the Gaussian. F0 and ω0 control the frequency and the orientation of the cosine wave.
Gabor noise is constructed as a sparse convolution noise that uses the Gabor kernel as its kernel. Because the power spectrum of a sparse convolution noise is the power spectrum of the kernel, scaled by a constant as shown by Lewis [1989], the parameters of the Gabor kernel provide direct control over the power spectrum of the resulting noise.
In order to increase the computational eﬃciency, Gabor noise is evaluated on a grid. The properties of the Gabor kernels are generated per cell on the ﬂy using a PRNG. The size of a grid cell equals the radius of the Gabor kernels. This allows us to limit the evaluation of the noise function to the cell containing the point being evaluated, and its immediate neighbors.
Lagae et al. [2009] provide ways to produce both periodic and nonperiodic noise. In order to obtain noise with a period of N, the grid cells are enumerated in row-major order with cell coordinates taken with modulo N. The seed for the PRNG is then calculated as a sum of the cell index and a global oﬀset parameter. Nonperiodic noise uses Morton order for cell enumeration.

50

3. Nonperiodic Tiling of Noise-based Procedural Textures

Worley [1996] introduced a new texture basis function to complement procedural noise functions (Figure 3.3(e)). This function can be used to produce various textures like cobblestones, water and cellular structures. The algorithm randomly distributes feature points in space. Function Fn is deﬁned as the distance between the input and the n-th closest feature point. The resulting value is determined by a function of F1, , Fn.
Although the cellular texture basis function is not a procedural noise function in the same sense as other function discussed in this chapter, it is very relevant to texture synthesis. Lagae et al. [2010] observed that its implementation is similar to that of sparse convolution noise, and, inherently, Gabor noise. Both subdivide the space into a uniform grid, and a random number of positions are generated for each cell. The positions become the locations of feature points for Worley noise and the centers of Gabor kernels for Gabor noise.
A uniform grid is very tightly linked to the integer lattice: each grid cell can be identiﬁed by a lattice point in the corner of the cell. This means that for our purposes, the terms lattice point and grid cell are interchangeable. Because of that, we can directly use the TransformCoordWideBorder from Listing 3.3 for both Gabor noise and Worley noise. kPointsPerRow and kPointsPerTile represent the number of cells per row and the number of cells per tile, respectively. The only modiﬁcation we need to do is to set the border width to one cell to cover a 33 area:
const uint2 kBorder00 = uint2(1, 1);
The resulting noises are shown in Figures 3.6(a) and 3.6(b).
3.4 Tiled Noise Filtering
There are three methods that are commonly used to map a noise to a surface. Explicit parameterization is most commonly used with regular textures. It allows ﬁne-tuning, but requires additional memory and can introduce distortions and seams. Solid noise is a 3D noise function that is sampled at each surface point. The objects textured using solid noise appear carved out of a block of matter. The memory cost remains low if the noise is not precomputed. Surface noise [Lagae et al. 2010] is deﬁned directly on the surface, giving the appearance of the noise features following the curvature. This is diﬃcult to achieve in general, but sparse convolution noises (such as Gabor noise) enable this approach.
Our methods work with explicit surface parameterization and with solid noise, regardless of whether the noise is precomputed or evaluated at application runtime.
Surface noise obtained from sparse convolution noises is limited to runtime evaluation only. It is constructed from Gabor noise by projecting a three-dimensional Poisson distribution on the plane tangent to the surface, and evaluating two-dimensional Gabor noise in that plane (see Lagae et al. [2009]). There is no grid deﬁned in the

3.4 Tiled Noise Filtering

51

(a)

(b)

(c)

Figure 3.6. Noise function Wang tile texture maps. (a) Anisotropic Gabor noise. (b) Worley noise. (c) Worley noise from (b) with separated tiles. Note that the corresponding edges of the tiles marked with the same color have matching borders.

tangent plane for the ﬁnal noise evaluation, which renders our methods not applicable to surface noise.
When the noise is mapped to surfaces, special care must be taken to avoid aliasing on distant objects and on surfaces seen at an angle. While the precomputed noise can beneﬁt from the existing texture ﬁltering methods, noise functions that are evaluated at application runtime usually rely on the properties of the noise to reduce aliasing.
Anisotropic ﬁltering is a commonly used technique that increases the image quality on surfaces viewed at an oblique angle. It takes several samples in an elliptic area, producing a more accurate approximation of the pixel projection to the screen. When the ﬁlter area crosses the tile borders, it can produce incorrect values. All the discussed noises except the ones that use the TransformCoord function (Listing 3.2) exhibit such artifacts only when the anisotropic ﬁlter is wider than the width of the tile border. Perlin noise and other noise functions that require just one row of lattice points for the tile border always have this error present. The error is nearly invisible due to interpolation between the lattice points, so some games can completely neglect this. Others may choose to increase the width of the border region to remove it completely.

52

3. Nonperiodic Tiling of Noise-based Procedural Textures

Using mipmaps is one of the most common ways of ﬁltering textures on distant surfaces. When dealing with textures containing tiles, one would need to take into account the way the hardware performs the sampling while downscaling the images in order to minimize the discontinuities at the tile boundaries between two adjacent levels of detail.
Our experiments show that anisotropic ﬁltering hides the discontinuities introduced by general-purpose downscaling algorithms used to construct the mipmaps, even in high-contrast images. In our opinion, using a combination of anisotropic ﬁltering and mipmaps produces the best visual results (see Figure 3.7).
The proposed methods do not modify the underlying noise function. It follows that ﬁltering the procedural noise functions that use them can be done with the help of the techniques that are applicable to the original noise functions. Read Lagae et al. [2010] for details on these techniques.

(a)

(b)

(c)

Figure 3.7. Eﬀect of texture ﬁltering on the tile boundaries. (a) Point ﬁltering, no mipmaps. (b) Trilinear ﬁltering only. Note the visible tile borders. (c) Anisotropic ﬁltering and trilinear ﬁltering. Tile borders can still be distinguished, but are much less pronounced.

3.5 Tiling Improvements
3.5.1 Tile Packing
Wei [2004] introduced a scheme to pack the Wang tiles into a single texture. The solution is generic for K edge colors, minimizes the storage requirements by ensuring that each tile is used only once, avoids ﬁltering artifacts by arranging the tiles in the ﬁnal texture using a valid Wang tiling and is at the same time eﬃcient to compute. A

3.5 Tiling Improvements

53

piecewise function TileIndex1D is applied separately to the vertical and horizontal edges of the tile, and gives the vertical and horizontal position of the tile respectively:

TileIndex1D e1, e2   e02ee1,e2111211ee22222,112,,,

e1  e2  0; e1  e2  0; e2  e1  0; e1  e2  0; e1  e2  0.

(3.4)

Most applications, however, limit the usage of Wang tiles to just two edge colors, thus reducing the texture size. We propose a diﬀerent packing function that can be used speciﬁcally for two edge colors, ensures the same tile layout and is more eﬃcient to
compute than TileIndex1D e1, e2 :

TileIndex1D e1, e2   2e1 e1  e2 
 2e1  e2  e1
 2 max e1, e2  e1 e2.

(3.5)

We provide three equivalent forms of the same function to account for possible performance diﬀerences in the target hardware.

3.5.2 Edge Color Hashing and Repetition
Wei [2004] proposed the use of toroidal boundary handling to compute the tile index from the input texture coordinate. The tile coordinates are then used as inputs to a hash function based on a small permutation table, which determines the edge colors of the resulting tile.
The permutation table size, however, either grows linearly together with the size of the output texture, or causes repetition of the tile pattern. While the amount of memory required for the permutation table is very modest by modern standards, it has to be kept in very fast memory to provide eﬃcient access. Our experiments show that usage of the same hash function as Olano [2005] proposed for lattice point coordinate hashing produces acceptable results and is very fast to evaluate. The hash value is calculated by oﬀsetting the input by a ﬁxed number K, and applying the PRNG to the result twice [Blum et al. 1986]. Because modern GPUs have full support for integer operations, it is no longer necessary to constrain the PRNG parameters to avoid precision loss in ﬂoating-point numbers.
Applications that require higher periods of tile repetition and can aﬀord a more expensive hash function would beneﬁt from using cryptographic hashes. They oﬀer statistical randomness of the result even if evaluated in parallel (as opposed to many classical PRNGs), and do not depend on the evaluation order. An example of such a

54

3. Nonperiodic Tiling of Noise-based Procedural Textures

hash function is the implementation of an MD5 hash1 for GPUs by Tzeng and Wei [2008].
Figure 3.8 shows the tile patterns produced by these tiling methods and the stochastic tiling algorithm proposed by Cohen et al. [2003].

(a)

(b)

(c)

(d)

(e)

Figure 3.8. Tile patterns produced by diﬀerent tiling algorithms. Each image contains 12864 color-coded Wang tiles. (a) Stochastic. (b) Permutation table, 32 entries. (c) Permutation table, 64 entries. (d) Blum Blum Shub hash. (e) MD5 hash.

3.6 Results
Perlin [1985] showed that his noise function can be used as a building block for many realistic-looking textures. We would like to show that similar results can be achieved with the noise functions modiﬁed as proposed in this chapter (see Figure 3.9).
Additionally, we would like to make an observation that once a function F is applied to a Wang tile texture, the result remains a Wang tile texture with the same layout if F depends on the pixel color, or if it depends on the pixel position and forms a Wang tile texture with the same layout. This class of functions includes, for example, linear combinations of Wang tile textures and periodic functions, with the period being a multiple of the Wang tile size. We show some textures generated with our application in Figure 3.9. We provide a Unity project with our example implementations of the shaders producing these textures in the accompanying code samples.
3.7 Performance
We conducted a series of CPU and GPU tests to analyze the impact of the proposed modiﬁcations on the performance. All tests were performed on a computer with an Intel Core i7-6820HK CPU (2.7 GHz) and a GeForce GTX 980M (8GB) GPU. The screen resolution was set to 19201080. The GPU driver settings were left in the default state, allowing the application to control most parameters. VSync was turned oﬀ.
The test code was written in C++ using Visual Studio 2015. The compiler and linker settings were left in the default state for a console application in release conﬁg-

1 There are alternatives to MD5 which provide a better balance between hash complexity and tile pattern repetition.

3.7 Performance

55

Figure 3.9. Some interesting Wang tile texture maps produced using the presented techniques (left) and the corresponding tilings (right). Top to bottom: marble, detailed fabric, brick wall, cobblestones, stained glass.
uration. The code was written without usage of SIMD CPU extensions or multithreading. The application used OpenGL 4.5 as a graphics API.
We measured the average and the median execution time for all the variants of the noise functions being tested. The time values provided are in milliseconds. The last two columns of the following tables present the percentage diﬀerence in sampling performance between the original noise functions and the noise functions using the proposed modiﬁcations.
3.7.1 CPU Performance
We conducted a CPU performance comparison between the original noise functions and the noise functions using the proposed modiﬁcations by computing a 20482048 texture. The texture contents were updated 65 times. The ﬁrst iteration served as a warm-up and was excluded from the calculation of average and median duration values.
Lattice-based procedural noise functions used a lattice with 257257 points. All tables required by the functions contained 256 entries. The Perlin noise implementation used quintic interpolation.

56

3. Nonperiodic Tiling of Noise-based Procedural Textures

Noise function
Perlin, def. Perlin, opt. Better grad., def. Better grad., opt. Gabor, 0 kernels Gabor, 8 kernels Gabor, 16 kernels Gabor, 24 kernels Gabor, 32 kernels Gabor, 40 kernels

Non-tileable, ms

Avg

Med

31.77 31.71

24.74 24.64

194.20 194.25

207.52 207.55

2,613 2,627

18,955 18,850

34,863 34,857

51,233 51,233

63,740 63,515

67,184 67,399

Wang tiles, ms

Avg

Med

60.97 61.01

24.89 24.85

964.24 965.63

207.73 208.27

3,672 3,664

20,306 20,308

36,222 36,243

52,783 52,395

64,018 64,027

67,262 67,384

Diﬀerence, % Avg Med 91.9 92.4 0.61 0.85 397 397 0.10 0.35 40.5 39.5 7.13 7.73 3.9 3.98 3.03 2.27 0.44 0.81 0.12 −0.02

Table 3.1. Precomputed Noise Function Performance on the CPU.

We chose to use an anisotropic version of Gabor noise for the performance comparison. All the parameters of the Gabor kernel were ﬁxed. We measured the impact of the proposed modiﬁcations on the Gabor noise by introducing a value to cap the number of kernels per cell (we set the noise parameters to have 32 kernels as the mean value) and comparing the performance with the cap set to 0, 8, 16, 24, 32 and 40 kernels.
For the CPU performance test we provided two algorithms that compute the values of the Perlin noise and of the better gradient noise. The default implementation samples the function as usual, both the non-tileable and the modiﬁed versions. An optimized implementation ﬁrst precomputes the lattice gradients into a large lookup table, and then uses them to eﬃciently calculate the values of the ﬁnal function.
The default implementations of both lattice-based noise functions that produce Wang tiles are several times slower than the original functions. Still, the amount of time it takes to sample the functions once per pixel in a 20482048 texture is relatively small, indicating that they are very cheap to evaluate. The optimized implementations require additional memory, but make the performance of both the original and the modiﬁed function equal. The optimized Perlin noise implementation is also faster than the default one by more than 20% (see Table 3.1).
The evaluation of the anisotropic Gabor noise with zero Gabor kernels per cell closely estimates the performance cost of using the modiﬁed noise function. It is around 40% higher than that of the original function. The evaluation of additional Gabor kernels quickly reduces the weight of the boundary cell remapping to values close to zero in the overall function sampling duration.
3.7.2 GPU Performance, Precomputed Textures
We conducted a comparison between sampling a regular texture and sampling a Wang tile texture map on the GPU by rendering meshes that consist of two triangles covering

3.7 Performance

57

the whole screen. The texture being sampled contained a full mip chain. We tested two texture resolutions, 10241024 and 20482048. We rendered either a single mesh covering the whole screen or 16 such meshes located at the same depth in order to simulate overdraw.
Sampling a Wang texture once per pixel carries very little overhead. In this case the performance drops insigniﬁcantly, by less than 2%. Sampling a texture 16 times per pixel makes the diﬀerence more noticeable, up to about 20% (see Table 3.2). Our tests showed that enabling trilinear and anisotropic ﬁltering did not aﬀect the performance.

Texture wh @ samples 1K1K @ 1 1K1K @ 16 2K2K @ 1 2K2K @ 16

No tiling, ms

Avg

Med

0.992 0.526

5.567 4.159

1.434 0.784

5.346 4.227

Wang tiling, ms

Avg

Med

0.984 0.525

6.655 4.970

1.461 0.799

6.666 5.020

Diﬀerence, % Avg Med −0.81 −0.19 19.54 19.50 1.88 1.91 24.69 18.76

Table 3.2. Precomputed Noise Function Performance on the GPU.

3.7.3 GPU Performance, Direct Noise Evaluation
Finally, we conducted a performance comparison between the original and the modiﬁed versions of the noise functions, when the evaluation happens directly on the GPU at application runtime. The cost of sampling a modiﬁed noise function can be up to 10% higher than the cost of sampling the original function (see Table 3.3).

Noise function @ samples Perlin @ 1 Perlin @ 16 Better grad. @ 1 Better grad. @ 16 Gabor, 16 kern. @ 1 Gabor, 16 kern. @ 16 Gabor, 32 kern. @ 1 Gabor, 32 kern. @ 16 Gabor, 64 kern. @ 1 Gabor, 64 kern. @ 16

No tiling, ms Avg Med 1.25 0.67 6.87 5.65 2.58 1.5 20.97 20.97 11.05 10.90 175.3 173.2 21.21 21.12 340.0 340.0 42.01 42.05 671.6 671.5

Wang tiling, ms Avg Med 1.29 0.69 6.83 6.29 2.87 1.69 22.35 22.35 12.23 12.14 191.7 191.6 23.26 23.29 370.4 370.3 45.00 44.88 719.0 719.1

Diﬀerence, % Avg Med 2.63 2.37 −0.54 11.35 11.49 12.48 6.60 6.58 10.68 11.35 9.36 10.62 9.67 10.29 8.92 8.92 7.12 6.75 7.06 7.09

Table 3.3. Noise Function Evaluation Performance on the GPU.

58

3. Nonperiodic Tiling of Noise-based Procedural Textures

Overall, the results of the performance comparison tests indicate that the modiﬁcations presented in this chapter can be adopted by many applications without having a major eﬀect on the frame rate. Both precomputation and direct runtime evaluation of the modiﬁed noise functions are only slightly slower than the original. Sampling a precomputed Wang tile texture is roughly equivalent in performance to evaluating Perlin noise with simple tiling in the shader, and is at least several times faster than evaluating noises that oﬀer higher quality.
3.8 Limitations
As mentioned in Section 3.4, Wang tile textures require special downscaling algorithms in order to minimize errors on tile borders between adjacent mip levels. The same applies to block texture compression methods. Anisotropic ﬁltering, however, visually hides the discontinuities arising from texture compression as well.
Using a Wang tile texture within a texture atlas presents challenges very similar to those of using texture atlases with wrapping modes other than clamping. A detailed discussion is available in [Nvidia 2004]. If the Wang tile textures are of the same size, texture arrays can be utilized on the hardware that supports them.
Sampling a Wang tile texture requires using a version of the texture sampling function with explicitly provided gradients. Older or low-power hardware used in some mobile devices may lack support for gradient computation. Additionally, some old GPUs in mobile devices undergo a performance penalty when the texture coordinates used to sample a texture are modiﬁed in the fragment shader. This can be mitigated in some cases by adjusting the mesh UV coordinates in a way that would enforce all tile corners to correspond to mesh vertices, and by moving the texture coordinate calculation to the vertex shader.
The presented algorithms do not solve certain inherent problems of Wang tiles. For example, when the resulting tiles have large-scale distinct features, the tiling pattern becomes quite obvious.
Hash-based evaluation of tile edge colors requires a complete set of Wang tiles to operate. The number of tiles in the set grows very quickly when adding an edge color or increasing the number of dimensions. A full 2D set contains N 4 tiles, where N is the number of colors. This signiﬁcantly increases the memory requirements for the precomputed textures. A tile index texture map can be employed to reduce the memory occupied by the tile set by including only those tiles that are actually used. This, however, introduces additional complexity to the tile packing step, and adds a texture read to the shader.
3.9 Conclusion
We presented a set of modiﬁcations to several popular procedural noise functions that directly produce texture maps containing the smallest complete set of Wang tiles. The

3.10 Future Work

59

Figure 3.10. A comparison between simple tiling of a10241024 texture (top) and Wang tiling of a 512512 Wang tile texture map (bottom). The latter is nonperiodic and requires 4 times less memory.
proposed modiﬁcations can be used both at application runtime and during the preprocessing steps and can be generalized to higher dimensions and Wang tile sets with more edge colors.
The modiﬁed noise functions retain most of the key characteristics of the original functions [Kirillov 2018]. We discussed the eﬀect of using the proposed modiﬁcations on the noise function ﬁltering and the mapping of noise functions to surfaces. Additionally, we presented several improvements of the tiling algorithm on the GPU for Wang tiles.
Our modiﬁcations enable nonperiodic tiling for a large range of noise-based procedurally generated textures and eﬀects. The presented techniques can be used to produce large, non-repetitive and detailed terrain geometry, atmospheric eﬀects and realistic-looking, natural and artiﬁcial textures. The option to combine the Wang tile texture maps while maintaining tile layout further increases the diversity of the possible results.
The performance tests indicate that the proposed techniques can be adopted even by high-performance interactive real-time applications like games. The presented methods oﬀer a potential to decrease the memory consumption of the precomputed noise-based textures, and enable precomputation for large noise-based textures that would not ﬁt into the memory budget of an application otherwise, without sacriﬁcing the ﬁnal image variance (see Figure 3.10). The Wang tile texture map sampling is also straightforward to implement. This guarantees smooth integration into both existing and new applications.
3.10 Future Work
There are several interesting directions for future work. One is to investigate the possibility of deﬁning similar modiﬁcations for other noise functions. Adding the functionality to handle corner tiles would give an opportunity to reduce the repetition in the ﬁnal image even further. Introduction of an algorithm that minimizes the discontinuities on the tile borders between the adjacent levels of detail when downscaling the Wang tile textures would be beneﬁcial for high-quality rendering.

60

3. Nonperiodic Tiling of Noise-based Procedural Textures

Bibliography
BLUM, L., BLUM, M., AND SHUB, M. 1986. A Simple Unpredictable Pseudo-Random Number Generator. In SIAM Journal on Computing, 15:2, pp. 364–383. URL: https://doi.org/10.1137/ 0215025.
COHEN, M., SHADE, J., HILLER, S., AND DEUSSEN, O. 2003. Wang Tiles for Image and Texture Generation. 22, pp. 287–294. URL: http://doi.acm.org/10.1145/882262.882265.
EFROS, A. AND FREEMAN, W. 2001. Image Quilting for Texture Synthesis and Transfer. In Proceedings of ACM SIGGRAPH ‘01, pp. 341–346. URL: http://doi.acm.org/10.1145/383259. 383296.
JEANDEL, E. AND RAO, M. 2015. An aperiodic set of 11 Wang tiles. URL: https://arxiv.org/pdf/ 1506.06492.pdf.
KENSLER, A., KNOLL, A., AND SHIRLEY, P. 2008. Better Gradient Noise. SCI Institute Technical Report No. UUSCI-2008-001. URL: https://www.cs.utah.edu/aek/research/noise.pdf.
KIRILLOV, A. 2018. Non-periodic Tiling of Procedural Noise Functions. Proc. In ACM Comput. Graph. Interact. Tech. 1, 2, Article 32. URL: https://doi.org/10.1145/3233306.
LAGAE, A. AND DUTRÉ, P. 2006. An Alternative for Wang Tiles: Colored Edges Versus Colored Corners. In ACM Trans. Graph. 25:4, pp. 1442–1459. URL: http://doi.acm.org/10.1145/ 1183287.1183296.
LAGAE, A., LEFEBVRE, S., DRETTAKIS, G., AND DUTRÉ, P. 2009. Procedural Noise Using Sparse Gabor Convolution. In Proceedings of ACM SIGGRAPH ‘09, pp. 54:1–54:10. URL: http://doi.acm.org/10.1145/1531326.1531360.
LAGAE, A., LEFEBVRE, S., COOK, R., DEROSE, T., DRETTAKIS, G., EBERT, D., LEWIS, J., PERLIN, K, AND ZWICKER, M. 2010. State of the Art in Procedural Noise Functions. In EG 2010 – State of the Art Reports. Eurographics Association.
LEWIS, J. 1989. Algorithms for Solid Noise Synthesis. In Proceedings of ACM SIGGRAPH ‘89, pp. 263–270. URL: http://doi.acm.org/10.1145/74334.74360.
Neyret, F. and Cani, M. 1999. Pattern-based Texturing Revisited. In Proceedings of ACM SIGGRAPH ‘99, pp. 235–242. URL: http://dx.doi.org/10.1145/311535.311561.
NVIDIA. 2004. Improve Batching Using Texture Atlases. NVSDK 7.0 Whitepaper. URL: http://download.nvidia.com/developer/NVTextureSuite/Atlas_Tools/ Texture_Atlas_Whitepaper.pdf.
OLANO, M. 2005. Modiﬁed Noise for Evaluation on Graphics Hardware. In Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware, HWWS ‘05, pp. 105–110. URL: http://doi.acm.org/10.1145/1071866.1071883.
PERLIN, K. 1985. An Image Synthesizer. In Proceedings of ACM SIGGRAPH ‘85, pp. 287– 296. URL: http://doi.acm.org/10.1145/325165.325247.
PERLIN, K. 2002. Improving Noise. In Proceedings of ACM SIGGRAPH ‘02, pp. 681–682. URL: http://doi.acm.org/10.1145/566654.566636.

Bibliography

61

STAM, J. 1997. Aperiodic texture mapping. Tech. rep., R046. European Research Consortium for Informatics and Mathematics (ERCIM). URL: http://www.dgp.toronto.edu/people/stam/ reality/Research/pdf/R046.pdf.
TZENG, S. AND WEI, L. 2008. Parallel White Noise Generation on a GPU via Cryptographic Hash. In Proceedings of the 2008 Symposium on Interactive 3D Graphics and Games, I3D ‘08, pp. 79–87. URL: http://doi.acm.org/10.1145/1342250.1342263.
WEI, L. AND LEVOY, M. 2000. Fast Texture Synthesis Using Tree-structured Vector Quantization. In Proceedings of ACM SIGGRAPH ‘00, pp. 479–488. URL: http://dx.doi.org/10.1145/ 344779.345009.
WEI, L. 2004. Tile-based Texture Mapping on Graphics Hardware. In Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware, HWWS ‘04, pp. 55–63. URL: http://doi.acm.org/10.1145/1058129.1058138.
WORLEY, S. 1996. A Cellular Texture Basis Function. In Proceedings of ACM SIGGRAPH ‘96, pp. 291–294. URL: http://doi.acm.org/10.1145/237170.237267.

4
I
Rendering Surgery Simulation with Vulkan
Nicholas Milef, Di Qi, and Suvranu De
4.1 Introduction
While surgical simulation requires much of the same rendering functionality as games, critical diﬀerences necessitate simulation-speciﬁc optimizations and engine design decisions that aren’t commonly needed or provided in rendering engines for games. Given our unique use cases, we take advantage of the explicitness of the Vulkan API (as compared to OpenGL) to develop a rendering engine for surgical simulation. In this article, we explain how we tailored our rendering engine design around surgery simulation including how higher level design decisions propagate to lower-level usage of Vulkan. To achieve this goal, our rendering architecture is designed to be ﬂexible, maintainable and eﬃcient. In surgical use cases, soft tissues are modeled by deformable meshes which are specially handled by our eﬃcient memory system. We show how performance scales with our memory system. Later, we present a case study using our renderer in a virtual cricothyroidotomy (CCT) 3D simulator.
4.2 Overview
Virtual surgery simulators present some unique computational and development challenges that are less common in other applications such as games. Rendering is particularly important because the appearance of the simulator must be convincingly realistic to properly train surgeons for real-life surgery scenarios.
General-purpose game engines often have limited soft-body physics and haptics support. Platforms such as the Software Framework for Multimodal Interactive Simulation (SoFMIS) [Halic et al. 2011], the Simulation Open Framework Architecture (SOFA) [SOFA 2018], OpenSurgSim [OpenSurgSim 2017], and the Interactive Medical Simulation Toolkit (iMSTK) [iMSTK 2018] seek to ﬁll this gap. Our rendering engine in particular is part of the larger framework of iMSTK. In addition, newer APIs
63

64

4. Rendering Surgery Simulation with Vulkan

such as Vulkan [The Khronos Group 2018] provide more capabilities to make better use of computing resources and allow for more predictable performance compared to older graphics APIs such as OpenGL.

4.2.1 Current Rendering Challenges for Surgery Simulation
One of the main challenges with surgery simulation is the rendering of diﬃcult materials such as skin, tissue, and organs. Unlike in games, diﬃcult rendering scenarios cannot be avoided by artwork or level design. It’s necessary to render intricate details such as marks on organs and skin while making the rendering look photorealistic so that users can become familiar with the real surgical procedure.
Another challenge we face is the handling of dynamic meshes to prepare for rendering. Since a major component of useful surgery simulation is the simulation of the physical deformation of tissue and organs and their interactions with the virtual environment, the renderer needs to be able to eﬃciently and correctly handle the updating of these geometries. Although soft bodies exist in video games, they often have limited interactions with other objects (e.g., cloth simulation), so the simulation and rendering can be often be done exclusively on the GPU. However, surgery simulation also relies on haptic devices, so forces must be sent to the haptic device drivers. GPU computation isn’t as desirable option because the computation results must be send to the haptic devices as soon as possible and many of the physics computations use serialized constraint solving, making these algorithms diﬃcult to parallelize for the GPU. In order to render the deformable meshes, we needed to develop an architecture to quickly update data.
Finally, although the Vulkan API is more explicit than OpenGL, many API usage choices depend on the target hardware. In other words, it’s not always clear what acceptable parameters are to guarantee satisfactory performance. Memory management is one of the areas where the optimal use cases are particularly ambiguous.
In this chapter, we ﬁrst present our approach towards achieving realistic rendering with our rendering architecture. Next, we present methods for handling the computation and transfer of dynamic meshes. Finally, we share our approach to memory management for our rendering system.

4.3 Render Pass Architecture
Rendering architectures construct their render passes in a way to minimize rendering artifacts, reach performance goals, and allow for advanced rendering algorithms. For a speciﬁc application, it is critical to balance these tradeoﬀs. Additionally, the increased complexity of these rendering engines necessitates more complex shader code, which can be diﬃcult for users and engine developers to maintain.

4.3 Render Pass Architecture

65

Figure 4.1. The render pass architecture.
4.3.1 Render Pass Stages
Our rendering architecture is divided into individual render passes (Figure 4.1). Surgical simulation requires the rendering of diverse materials which some lighting techniques, such as deferred rendering, have diﬃculty expressing eﬃciently since lighting is decoupled from the geometry rendering. In deferred rendering, the geometry is rendering to a geometry buﬀer (G-buﬀer) that separate lighting passes read to perform lighting calculations for each fragment on the screen. Classical deferred renderers can be more eﬃcient for rendering many lights, but surgery simulation generally only requires a small amount of lights (usually less than a dozen). While branching can be used to allow for some material variety [Garawany 2016], it ultimately still limits the number and types of materials.
In contrast, in forward rendering, each material is evaluated independently, allowing for completely arbitrary material evaluations in each fragment shader for a material. While we chose forward rendering for this reason, we still incorporated a thinner Gbuﬀer so that we could later use its data for post-processing such as screen-space subsurface scattering.
Each render pass is explained in detail in the following sections.
Shadow Mapping In some surgery simulation scenarios, many shadow-casting lights are necessary for helping users to judge depth perception of their instruments in the virtual environment. The ﬁrst pass of each frame includes rendering shadows for each directional light into shadow maps, which we place into a texture array. Rendering shadow maps into texture arrays is a common approach [Pettineo 2015] as it allows for the binding of many shadows maps during a single draw, which is necessary for forward rendering architectures that evaluate multiple lights in one draw call. We save on material permutations by

66

4. Rendering Surgery Simulation with Vulkan

reusing the same shadow pipelines for each shadow pass (since the render passes are compatible). We pass the index of the shadow current shadow map to the shadow shader using a push constant that accesses an array of light inverse matrices.
Depth Prepass and Ambient Occlusion
Forward rendering is ineﬃcient for scenes with signiﬁcant overdraw. In order to minimize this, we implemented a depth-prepass as the next pass to minimize the lighting computation. We follow the depth-prepass with a horizon-based ambient occlusion (HBAO) pass [Bavoil et al. 2008]. By calculating ambient occlusion (AO) before the lighting passes, the AO becomes available for use in the lighting pass. We calculate the HBAO at quarter resolution by ﬁrst downscaling using a min-depth operator, and then we upscale the AO using a Gaussian-based bilateral depth-aware ﬁlter, as opposed to just a 2-pass Gaussian ﬁlter, to prevent AO bleeding.
Physically-Based Rendering (PBR) Lighting
The lighting passes for both opaque geometry and decals follow. During this pass, lighting is calculated for both specular and diﬀuse and written into separate accumulation HDR 16-bit buﬀers (Table 4.1), similar to what was done in CryEngine [Sousa et al. 2012]. In the case of the opaque geometry pass, we populate another buﬀer with 4 8-bit color channels for world-space normals and a subsurface scattering constant. The light types include both classical computer graphics light sources such as directional and point lights, and global illumination approximation, which in our case, is imagebased lighting (IBL) with split-sum approximation [Karis 2013]. Because we already calculated the AO, the IBL can be used with both decals and the underlying opaque geometry. Baked AO textures for rigid objects are read to supplement the screen-space AO, and the maximum of these two AO values is taken to determine the total AO. For the lighting pass, we support a roughness/metalness workﬂow for PBR content.

Buﬀer Type Diﬀuse Accumulation Specular Accumulation Normal/SSS Depth

R G B

A

16-bit per channel color

16-bit per channel color

x y z SSS strength

32-bit depth

Table 4.1. G-buﬀer layout.

Deferred Decals
Deferred decals are a low-cost yet ﬂexible method for adding details to underlying geometry. The decal pass diﬀers from the opaque rendering in that it doesn’t write to the normal or subsurface scattering (SSS) buﬀer but rather reads from it. One of the beneﬁts of using the underlying SSS buﬀer is that it allows the decal to blend in with

4.3 Render Pass Architecture

67

Figure 4.2. (left) A mesh without a decal, (right) a mesh with a decal aﬀected by subsurface scattering.
the surface underneath during the SSS render pass, so any marks on the skin for instance will look more naturally integrated. Since the decals are deferred, they need a normal, and we chose to use the underlying normal and the underlying SSS constant (Figure 4.2). Many use cases of the decals such as bleeding or marking only make small changes to the surface normal, so the underlying surface normal can provide an acceptable approximation.
Post Processing Surgery simulations include organic geometry such as skin and organs, which requires subsurface scattering (SSS) to display accurately (Figure 4.3). We chose to implement a screen-space SSS as opposed to a texture-space implementation for several reasons. First, screen-space avoids overdraw which becomes a problem when inside the body. Although a depth prepass mitigates this, there’s still the need to perform extra texture lookups as compared to screen space methods. Secondly, the diﬀusion proﬁle is relatively similar since most of the materials have blood and/or fat under the surface. Third, it samples across diﬀerent draw calls. This becomes particularly important when a mesh is split into smaller meshes in order to avoid unnecessary physics computations. For example, if a section of an organ undergoes operation, then it must be deformable, but the surrounding organ can be rigid. With screen-space SSS, the SSS can sample from across both meshes, creating a seamless rendering.
After the lighting pass, separable screen-space SSS [Jimenez et al. 2015] is applied to only the diﬀuse buﬀer. We keep a pool of 3 HDR buﬀers (one for the specular render target, one for the diﬀuse, and one free) in order to ping-pong the diﬀuse buﬀer during the two passes. After the SSS, the specular buﬀer and diﬀuse buﬀer are composited into the free buﬀer. These buﬀers are reused for later passes.
Bloom is then calculated in two passes at quarter resolution and then composited with the previous result. A ﬁlmic tone mapping pass [Hable 2010] follows the bloom

68

4. Rendering Surgery Simulation with Vulkan

Figure 4.3. A rendering of a polyp with subsurface scattering.
pass to map down to a 32-bit sRGB buﬀer. In the early stages of implementing SSS, we found that using a non-ﬁlmic tone mapper such as Reinhard [Reinhard et al. 2002] hide the eﬀect of the SSS by desaturating the eﬀect of the diﬀusion proﬁle.
4.3.2 Uber-Shader Approach and Material System
One of the challenges with forward rendering is the possible combinatorial explosion of diﬀerent shader options.
In Vulkan, all draw state is compiled into a single VkPipeline object. We build materials for each object; each material contains a VkPipeline object and associated descriptor sets. A single object may have multiple materials (and thus VkPipeline objects) to account for multiple render passes. For example, a single object could have a material for shadow map passes and another one for the lighting pass.
One of the challenges with forward rendering is the need for software developers to maintain a large number of possible shader permutations. This becomes especially problematic in our framework because we allow arbitrary combinations of textures to be attached to a material as well as diﬀerent draw modes such as for debug rendering. We chose to use an Uber-shader approach to reduce maintenance; we use a large shader that contains all possible shader code combinations and block out irrelevant parts during the shader compilation process. This generally works well for our applications since our applications are made to be photorealistic, so the shaders generally try to model lighting physics rather closely. Separate shader code paths can be made available for diﬀerent lighting conditions such as debug rendering without introducing branching.

4.4 Handling Deformable Meshes

69

Shaders are compiled as a build step for compilation of the C++ code, so only SPIR-V binary ﬁles are read into the engine at runtime. This creates some problems with texture resource management, however, since each material can provide a diﬀerent number texture resources. We solve this problem by creating small placeholder textures, but the lookup of these textures are restricted by specialization constants. Specialization constants allow the driver’s shader compiler to optimize away shader code that gets set during pipeline creation. Other expensive operations, such as PBR lighting code, can be optimized away for situations that don’t require it. One drawback of using specialization is that the pipeline objects can become incompatible for similar materials. In this case, they cannot be shared across draw calls.

4.4 Handling Deformable Meshes
Deformable meshes, which we deﬁne as meshes that include per frame vertex and index updates, require data transfer to the GPU, mesh recomputation, and eﬃcient memory usage.
4.4.1 Eﬃcient Data Transfer to the GPU
Deformable meshes require frequent and large-scale updates to the whole mesh, and transferring this to the GPU can be complex due to needed synchronization and slow if the data is large enough. Unlike OpenGL, Vulkan gives explicit control over the location of data, but the correct locations (e.g., which meshes should be on the GPU) are often diﬃcult to assess for a given use case. Furthermore, diﬀerent hardware vendors even have diﬀerent recommended memory types that aren’t universally available.
We speciﬁcally chose a solution that could work on a wider range of hardware and, through experimentation, gave acceptable results. We tested on Nvidia’s GTX 1080 GPU. We chose to use a single queue, the same queue used for rendering, to do the transfer operations required to update the meshes to avoid the need for inter-queue synchronization. Although some graphics drivers expose transfer-only queues that could potentially allow for higher bandwidth, this is not universally available across all major vendors [Willems 2018].
With using a single queue, there are two eﬀective ways to update data for rendering: 1) using a CPU-accessible staging buﬀer with a mirrored GPU buﬀer and running transfer operations or 2) using a CPU-accessible buﬀer. In comparing these two approaches, we found that for large meshes (i.e., larger than 100k triangles), the diﬀerences in performance were negligible between the two methods. Even with multiple render passes for the same mesh data that could cause redundant PCI-E bus usage, the performance was similar. On the other hand, using system memory (CPU-accessible buﬀer) allows us to avoid managing memory barriers to ensure the data is uploaded to the GPU in time.

70

4. Rendering Surgery Simulation with Vulkan

Figure 4.4. An example of dynamic mesh updating using double-buﬀering.
Multi-Buﬀering Our renderer includes a pool of command buﬀers for geometry passes, and each frame, a command buﬀer is recycled and rewritten to. When we ﬁnish writing to a command buﬀer, we submit it to the driver, and start recording another command buﬀer. The command buﬀer will run asynchronously to our render loop. We needed to avoid readwrite hazards but didn’t want to stall the render loop, so we implemented multi-buﬀering for vertex and index buﬀers (Figure 4.4). We buﬀer the data the same amount as the number of back buﬀers presented in the swap chain; if the application renders with triple-buﬀering, then the mesh data also uses triple-buﬀering. This makes tracking the region to update simple as the remainder of the frame number can just be passed to the update functions.
Our multi-buﬀering implementation is similar to unsynchronized multi-buﬀering in OpenGL with persistent data mapping [Hrabcak and Masserann 2012], but we have more control over the memory management and synchronization.
4.4.2 Normal Calculations per Frame with Smoothing Groups
Because the deformable meshes can have both topology changes and individual vertex displacements, the normals and tangents must be able to be recomputed each frame. Additionally, many of the geometries in surgery simulation tend to have organic shapes, leading to vertex seams along the edges of the UV maps. These seams cause lighting discontinuities which make surfaces incorrectly appear to have hard edges.
To handle these problems, on ﬁle import, we create a mapping on vertices that belong to the same smoothing group. With recomputation of the normals, these mappings are preserved. The ﬁnal normal for each vertex in the group is calculated from each neighboring triangle’s normal and each vertex that belongs to the group. The tangents are calculated separately, however, because they are aligned to each vertex’s UV coordinate, which diﬀers for each vertex in the group.

4.5 Memory Management System

71

One problem with this approach is that the tangents can diverge from the normals since tangents depend on the UV coordinates which are likely unique to each vertex, whereas normals can be shared across vertices. This produced shading artifacts that were highlighted by our BRDFs, but a simple ﬁx was to orthogonalize the tangentbitangent-normal basis in the vertex shader through the Gram-Schmidt process.

4.5 Memory Management System
Unlike OpenGL, Vulkan gives users explicit control on where they can store data, but the correct locations are often diﬃcult to assess. In addition, memory backings for resources such as images and buﬀers are not automatically allocated. In contrast, behind the scenes, OpenGL drivers do additional work such as memory defragmentation and suballocation. OpenGL abstracts the physically locations of all resources (such as in system RAM or VRAM), although this hides diﬀerences between diﬀerent GPUs and drivers. While this requires less development from the application side, this can lead to inconsistent performance across platforms. In order to ﬁll this gap lying in the Vulkan API, we implemented our own memory allocator.
4.5.1 Custom Memory Allocator
The performance implications of diﬀerent allocation strategies are not always obvious and can diﬀer depending on the vendor. Furthermore, certain allocations strategies can be tailored to speciﬁc applications. For instance, in surgery simulation, it’s uncommon for new geometry and resources to be added during the simulation. It is generally known beforehand what resources are needed for a speciﬁc application. This allows us to avoid implementing performance-sensitive features such as memory defragmentation.
Our memory manager separates diﬀerent resources into diﬀerent memory allocations (Figure 4.5). For instance, uniform buﬀers occupy a diﬀerent memory allocation than textures. The advantage of this approach is that the certain resource types need to be in certain areas of memory for optimal performance. For instance, staging buﬀers need to be in host visible memory, whereas images (e.g., textures) need to stay on the GPU, so they reside in device-local memory.
Certain resources such as images, uniform buﬀers, and storage buﬀers have alignment requirements. This allows mesh data to be more compact, which is useful since mesh data might need to be transferred each frame for dynamic objects.
With the exception of uniform buﬀers, a single VkBuffer occupies the whole underlying memory allocation. Internally, our memory manager uses lightweight abstract buﬀer objects that point to regions within the VkBuffer object. With uniform buﬀers and VkImages, however, multiple uniform buﬀers and images can ﬁll a single allocation. Uniform buﬀers are treated this way since the minimum uniform buﬀer size guaranteed by the speciﬁcation is 64 KiB.

72

4. Rendering Surgery Simulation with Vulkan

Figure 4.5. An overview of the memory manager.
The initial allocation size we chose was 16 MiB, and this allocation is used until it runs out of space and then a new allocation is made. For images and buﬀers larger than the allocation size, the allocation is expanded to account for these, so very large resources can potentially have their own allocation.
Mesh Data Mesh data is handled diﬀerently from the other resource types. Deformable meshes reside in host visible memory, whereas other meshes (e.g., rigid objects) are in device local memory through staging. Other game engines such as Source 2 have followed a similar approach for static/dynamic resources [McDonald 2016].
Deformable meshes also take up more space than rigid meshes need as they are multi-buﬀered. When the mesh is initially allocated, this extra buﬀering space is also allocated in the same location. For host visible memory, this works out well since the transfer to the GPU is implicitly done, avoided overhead in calling transfer functions.
In some operations, such as mesh cutting operations, additional triangles or vertices can be added or removed to the mesh. Adding new vertices or triangles would require more buﬀer space. Because operations such as cutting are a high-frequency operation, we needed a way to expand geometry all of the time without allocating new memory and deallocating old memory, which could be costly and cause memory fragmentation. To solve this problem, we allow users to specify a load factor that sets a maximum size of the geometry relative to the original mesh size. We allocate additional space within each buﬀer subsection for each frame (Figure 4.6).

4.6 Performance and results

73

Figure 4.6. A buﬀer layout allowing for per-frame updates.
4.6 Performance and results
To analyze the performance of our memory management system within our renderer, we devised a benchmark that tests the role of diﬀerent default memory allocation sizes. A size of zero MB represents a naive approach of creating one memory allocation per vertex and index buﬀer. We found two areas that showed improvement based on our test scenario: application load time (Figure 4.7) and frame time (Figure 4.8). We tested on a Windows computer with an Intel i7 6850k CPU and an Nvidia GTX 1080.
There are optimal allocation sizes that meet both of these metrics. For example, larger allocations (such as 256 MiB) have slightly longer load times while they have comparable performance to smaller allocations (4 MiB). Diﬀerent default allocation sizes have been proposed by vendors such as 8MB for mesh data and 128 MB for textures [McDonald 2016] or 256 MiB [Sawicki 2018]. This depends on the hardware and application resources sizes to some extent, but there should be a fairly large range of acceptable default allocation sizes, as demonstrated by our data. Unless very large resources are used (e.g. 10s of MB per resource), there’s a point of diminishing returns for runtime performance with larger buﬀer sizes. However, larger allocation sizes slightly increase load time and will waste more memory in cases when they aren’t saturated.
For our tests, we compared rigid mesh data with deformable meshes. Rigid meshes require two buﬀers, a staging (CPU) and a device local (GPU) buﬀer. Meanwhile, the deformable meshes require larger buﬀers to account for multi-buﬀering, but they remain on the CPU, resulting in much lower the allocation times, particularly for more allocations. We experienced a larger decrease in load time performance when using the naive allocation strategy for rigid meshes as compared to deformable meshes, which indicates that making many GPU allocations can be much slower than CPU allocations. For the runtimes, the dynamic meshes require rewriting of the data and transfer to the GPU, which slows down performance compared to rigid meshes.

74

4. Rendering Surgery Simulation with Vulkan

Figure 4.7. 10,000 meshes, each made up of 100 lines.

Figure 4.8. 10,000 meshes, each made up of 100 lines.

4.7 Case Study: CCT

75

4.7 Case Study: CCT
Surgeons perform the cricothyroidotomy (CCT) procedure as an emergency procedure when patients have a restricted airway. The steps involved in the procedure are as follows:
 Palpating the neck region to identify the locations of the thyroid and cricoid cartilages which are the landmarks anatomies in this procedure.
 Making an incision along the midline of the neck through the skin and the fat tissue to uncover the cricothyroid membrane underneath.
 Making an incision along the membrane to open an entrance to the trachea.
 Inserting an endotracheal tube inside the trachea through the new incision.
There are two main problems with the CCT simulation from a rendering perspective: eﬃciently updating the large geometric models representing the fat and membrane tissues and rendering the surface of each cut.
4.7.1 Dynamic Mesh Update
The CCT procedure requires at least two incisions to allow for intubation. This is accomplished on the CPU side through a mesh cutting algorithm which causes the mesh to regenerate in order to incorporate topological changes. In addition, the mesh is deformed during each physics step, which causes the vertex positions to be displaced and the normals and tangents to be recalculated. All of this data must be reuploaded to the GPU each frame.
Initially, we had diﬃculty with transfer speeds for this use case as we used staging and GPU buﬀers to handle all transfers. We wrote to a host visible buﬀer and performed a transfer operation, and we operated on a single queue. We quickly found this actually substantially sped up performance, so we switched to using host-visible memory.
4.7.2 Surface Cut Rendering
The rendering of the cut mesh introduced a few challenges with our current framework. First, the outer surface, the skin for example, must be rendered with a diﬀerent material than the inner surface, which in our case is the fat tissue. On top of this, the UV coordinates are generated during runtime as the new surface mesh is recreated.
We wanted to reduce the number of surface meshes to be updated each frame, but our renderer doesn’t currently support multiple materials per mesh. Both the skin and the fat tissue use the same shading logic, but they diﬀer in texture sets. In order to circumvent our single material per mesh limitation, we project the sides of the mesh onto diﬀerent regions of a texture atlas.

76

4. Rendering Surgery Simulation with Vulkan

Figure 4.9. The CCT case study demonstrating cutting.
When performing the cut, we needed a visual cue (e.g., bleeding) for the progress of the cut. We opted to use a pool of blood decals to display the cutting path while it was being performed by the user. The decals automatically recycle after the pool hits a certain maximum number so they can be reused for multiple cuts.
4.8 Conclusion and Future Work
Rendering for surgery simulation is critical for creating a realistic immersive experience for training surgeons. The Vulkan API provided more ﬂexibility, but also some challenges with determining the optimal implementation strategies given our use cases.
We were able to create a rendering architecture that reduced the amount of shader maintenance needed in the future and would give us more accurate visualization. Our handling of deformable meshes allows us to eﬃciently render the output of various CPU-based algorithms. Finally, our memory management system allows us to scale our applications without worrying about introducing substantial overhead, and it can easily be extended support new types of resources.
In the future, we hope to expand these subsystems to further improve performance and rendering capabilities. One area we would like to explore is using asynchronous buﬀer transfers with multiple queues for mesh updates, as this could potentially decrease the transfer times by using more bandwidth. Another area that could see performance improvements would be the normal/tangents recalculations, possibly through compute shaders as this takes a considerable amount of time each frame depending on the topology and number of triangles of the mesh. Finally, we would like to expand our material system to allow for more complex and expressive materials.

4.9 Source Code

77

Figure 4.10. A rendering of an internal organ using our renderer.
4.9 Source Code
The source code is available as part of iMSTK: https://www.imstk.org.
4.10 Acknowledgments
Research reported in this publication was supported by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health (NIH) under Award Number 2R01EB005807, 5R01EB010037, 1R01EB009362, 1R01EB014305; National Heart, Lung, and Blood Institute (NHLBI) of NIH under Award Number 5R01HL119248; National Cancer Institute (NCI) of NIH under Award Number 1R01CA197491 and NIH under Award Number R44OD018334.
Bibliography
BAVOIL, L., SAINZ, M., AND DIMITROV, R. 2008. Image-space horizon-based ambient occlusion. In ACM SIGGRAPH 2008 talks, p. 22.

78

4. Rendering Surgery Simulation with Vulkan

GARAWANY, R. 2016. Deferred Lighting in Uncharted 4. In Advances in Real-Time Rendering in Games: Part I.
HABLE, J. 2010. Uncharted 2: HDR lighting. In Game Developers Conference 2010.
HALIC, T., VENKATA, S., SANKARANARAYANAN, G., LU, Z., AHN, W., AND DE, S. 2011. A software framework for multimodal interactive simulations (SoFMIS). In MMVR, pp. 213–217.
HRABCAK, L. AND MASSERANN, A. 2012. Asynchronous Buﬀer Transfer. CRC Press.
IMSTK. 2018. URL: http://www.imstk.org/.
JIMENEZ, J., ZSOLNAI, K., JARABO, A., FREUDE, C., AUZINGER, T., WU, X., PAHLEN, J., WIMMER, M., AND GUTIERREZ, D. 2015. Separable Subsurface Scattering. In Computer Graphics Forum, 34:6, pp. 188–197.
KARIS, B. 2013. Real shading in Unreal Engine 4. In Proc. Physically Based Shading Theory Practice, pp. 621–635.
MCDONALD, J. 2016. High Performance Vulkan: Lessons Learned from Source 2. In GPU Technology Conference 2016.
OPENSURGSIM. 2017. URL: https://www.sofa-framework.org/.
PETTINEO, M. 2015. Rendering the alternate history in The Order 1886. In Advances in RealTime Rendering in Games: Part II.
REINHARD, E., STARK, M., SHIRLEY, P., AND FERWERDA, J. 2002. Photographic tone reproduction for digital images. In ACM transactions on graphics, 21:3, pp. 267–276.
SAWICKI, A. 2018. Memory Management in Vulkan DX12. In Game Developers Conference 2018.
SOFA. 2018. URL: https://www.sofa-framework.org/.
SOUSA, T., KASYAN, N., AND SCHULZ, N. 2012. CryENGINE 3: Three Years of Work in Review. CRC Press.
THE KHRONOS GROUP. 2018. Vulkan 1.1.81 – A Speciﬁcation.
WILLEMS, S. 2018. GPUInfo. URL: https://www.gpuinfo.org/.

5
I

Skinned Decals
Hawar Doghramachi

5.1 Introduction
Decals that are dynamically added to a scene are a great way to increase interactivity by allowing the player to change the game environment. An eﬃcient way to do this for static environments is the use of deferred decals [Krassnigg 2010], [Persson 2011]. However, in general deferred decals fail to deliver convincing results on top of skinned meshes. A typical use-case is shooting at a character and creating wounds at the impact position of the projectiles. We will present a technique for adding dynamically decals on top of arbitrary, skinned meshes that is compatible to common rendering architectures and easy to integrate into existing rendering engines.
5.2 Overview
Dynamically added deferred decals work pretty well as long as the target area is rigid, i.e., only inﬂuenced by one bone. In this case it is possible to intersect a ray that represents the projection direction of the decal with the skinned mesh and record the triangle and barycentric coordinates of the intersection. With this information, each frame the position and normal of the intersection can be updated and a deferred decal applied accordingly. However, this method fails to deliver convincing results when the target area is inﬂuenced by several bones. Typical artifacts in such cases are decals that are “swimming” on top of the target mesh, not stretching according to the underlying surface, producing distracting overlaps and projecting on top of areas that were initially not in the inﬂuence area of the decals.
A method that doesn't suﬀer from the aforementioned problems was mentioned by [Bronx 2011]. The idea is to render the skinned mesh that receives a decal once when a new decal is created. Instead of rendering the mesh itself into a render target, it uses the mesh texture coordinates as vertex shader output position. In a pixel shader the decal texture coordinates are calculated, similarly to deferred decals, and the corresponding texels are fetched from the decal texture and output into the mesh texture.
79

80

5. Skinned Decals

This implies that each mesh instance has its own texture that serves as render target resource. Finally when the skinned mesh is rendered in the shading pass, the updated mesh texture is fetched at the mesh texture coordinates and the resulting decals are automatically applied. Unfortunately there are several drawbacks which make this technique impractical for game production:
 Usually modern games use several textures for shading purposes. In general you can expect to have a diﬀuse, normal, specular and roughness map and in case of parallax occlusion mapping and displacement mapping additionally a height map. In this way several texture maps would need to be duplicated per mesh instance which can have a large memory impact. Since compressing textures at runtime is most likely too expensive, textures would need to be stored uncompressed, in this way increasing the memory requirements and the memory bandwidth when reading from these textures.
 To avoid severe aliasing issues, mipmapping is required. However, generating mipmaps for all required mesh textures, each time a new decal is added, can have a signiﬁcant performance impact.
 Once a decal is added, it can't be selectively removed from the mesh textures.
The proposed technique is based on the same base idea as the aforementioned method, but overcomes several of its drawbacks. The idea is to output decal texture coordinates instead of decal texture values when a new decal is created and the corresponding skinned mesh is rendered. In this way neither does it matter how many texture maps are utilized (diﬀuse, normal, specular, etc.) nor do we have to bother with generating mipmaps for all of these texture maps. Each mesh instance requires only one additional texture, called hereafter decal lookup map, which holds the decal texture coordinates, a fading value and an index for each decal added on top of the skinned mesh. This decal lookup map is used during shading to fetch and apply the decal textures. In this way decals correctly stretch according to the underlying surface, don't overlap under motion and don't project on top of mesh areas that were initially not in the inﬂuence area of the decals.

5.3 Implementation
The presented system can be divided into three steps (Figure 5.1) that will be described in more detail in the following subsections. All explanations are assuming: columnmajor matrix layout, right-handed coordinate system, NDC depth-range from 0.0 to 1.0 and left top corner as texture / screen space origin. Furthermore the explanations assume the use of DirectX 12, but the system could be also implemented with e.g., OpenGL or Vulkan.

5.3 Implementation

81

Figure 5.1. Overview of the three steps involved in the proposed technique. While the third step is executed each frame, the ﬁrst two steps are only excuted when a new decal is added. The skinned model used in this image was authored by KatsBits [2014].
5.3.1 Decal setup
This step doesn't diﬀer from the setup that is required for deferred decals. In this step we have to construct a matrix that deﬁnes the position, orientation and size of a decal, called hereafter decal matrix. In the example of creating a decal at the impact position of a weapon projectile, we ﬁrst need to ﬁnd the closest intersection on the path of the projectile. For this we intersect the corresponding ray with the front-facing triangles of the target skinned mesh. This intersection test has to be performed either on CPU or GPU, depending on where skinning is done and where we have access to the skinned vertices. On CPU this is straight forward; after getting from the physics system the mesh of closest intersection, the corresponding skinned triangles are checked for intersection. In case of GPU skinning, the skinned triangles are tested in parallel on compute shader threads and the resulting decal matrix is constructed directly on the GPU and passed via a GPU buﬀer to the subsequent step. This could be a good use case for DirectX Ray Tracing where the intersection part could be accelerated.
From the position and normal of the closest intersection we create a view matrix and from the extents of the decal an orthographic projection matrix. These matrices

82

5. Skinned Decals

are combined with a texture matrix, that maps texture coordinates from 1,1 range into 0,1 range, to the ﬁnal decal matrix. Listing 5.1 shows pseudo code how to calcu-
late the decal matrix.
5.3.2 Update decal lookup map
Each time a new decal is added, the underlying skinned mesh is rendered into the decal lookup map using the mesh texture coordinates as output position. The decal texture coordinates are calculated in the same way as for deferred decals by multiplying the decal matrix with the world space position of the mesh vertices. By setting the output depth to the z component of the decal texture coordinates, and using custom clip planes for the x and y components, pixels outside of the decal will be clipped away. Listing 5.2 shows the HLSL code for the corresponding vertex shader.
Since each texel of the decal lookup map can only store information for one decal, overlapping decals are not supported. Therefore it has to be ensured that each decal texel in the decal lookup map doesn't get overridden. For this the pixel shader of this step is outputting its result via an unordered access view (UAV) into the decal lookup map that makes it possible to examine the target texel before overwriting it. When the

// decalTangent is a vector perpendicular to decal projection // direction and specifies the rotation of a decal on target surface.

Vector3 viewPosition = hitNormal * decalDepth * 0.5 + hitPosition; Vector3 bitangent = Normalize(CrossProduct(decalTangent, -hitNormal)); Vector3 tangent = Normalize(CrossProduct(-hitNormal, bitangent));

Matrix4x4 viewMatrix = Inverse(Matrix4x4((tangent,

0,

bitangent, 0,

hitNormal, 0,

viewPosition, 1));

Matrix4x4 projMatrix(2/decalWith, 0,

0,

0,

0,

2/decalHeight, 0,

0,

0,

0,

-1/decalDepth, 0,

0,

0,

0,

1);

Matrix4x4 textureMatrix(0.5, 0, 0, 0, 0, -0.5, 0, 0, 0, 0, 0.5, 0, 0.5, 0.5, 0.5, 1);

Matrix4x4 decalMatrix = textureMatrix * projMatrix * viewMatrix;

Listing 5.1. Pseudo code for calculating decal matrix.

5.3 Implementation

83

VS_Output main(VS_Input input) {
VS_Output output;
// transform position from world to decal space float3 decalTC = mul(constBuffer.decalMatrix,
float4(input.position, 1.0f)).xyz; output.decalTC = decalTC.xy;
// Write rasterized fragments to output texture using the input // mesh texture coordinates and use z component of decal UV to // clip decal in z direction. output.position = float4(float2(input.texCoords.x, 1.0f
- input.texCoords.y) * 2.0f - 1.0f, decalTC.z, 1.0f);
// Clip decal in x and y directions (clipDistances declared // with SV_ClipDistance semantic). output.clipDistances = float4(decalTC.x, 1.0f - decalTC.x,
decalTC.y, 1.0f - decalTC.y);
output.normal = input.normal; return output; }
Listing 5.2. Vertex shader for updating decal lookup map.
target texel already contains information for another valid decal, the newly added decal texel will be discarded. To check that the target texel contains already information for a valid decal and not for a decal that is considered to cause an overlap with existing decals, a small GPU buﬀer is used, called hereafter decal validity buﬀer. Each entry of this buﬀer corresponds to one decal on the target mesh and is initialized to 0, i.e., all decals are valid at the beginning. In case an overlapping decal texel is detected, the corresponding entry in this buﬀer is set to 1, so that on one hand this decal won't prevent more recent decals to be added into the decal lookup map and on other hand such decals won't be rendered in the shading pass. Since we store the index of each decal at 8 bits into a decal lookup map texel, this buﬀer needs only 256 entries. The decal index is combined with the decal texture coordinates and a fading value and stored as DXGI_FORMAT_R8G8B8A8_UNORM. If typed UAV reads of this texture format are not supported, we can pack all values into a bitmask, using a 32 bit integer and perform manual bilinear ﬁltering later on. The corresponding pixel shader is shown in Listing 5.3. Since for the proposed system the texture coordinates of the target mesh have to be unique, i.e., each triangle of the mesh has to map to a diﬀerent texture area, we never run into race conditions where diﬀerent pixel shader threads write into the same texel location.

84

5. Skinned Decals

void main(VS_Output input) {
int2 outputPos = int2(input.position.xy); uint prevDecalIndex = uint(decalLookupMap.Load(outputPos).w
* 255.0f + 0.0001f);
// If target texel already contains decal information (index > 0) // that comes from a different, valid decal, current decal is // marked in decal validity buffer as invalid and the // corresponding texel is discarded. if ((prevDecalIndex > 0) && (prevDecalIndex !=
constBuffer.decalIndex) && (decalValidityBuffer[prevDecalIndex] == 0)) { decalValidityBuffer[constBuffer.decalIndex] = 1; return; }
// Fade out fragments which have an angle greater than 60 degrees // to surface normal to avoid projection artifacts. float cosAngle = dot(constBuffer.normal, normalize(input.normal)); float angleFade = saturate(cosAngle - 0.5f) * 2.0f;
// Fade out fragments towards near/far clip plane to avoid // clipping artifacts. float distFade = abs(0.5f - input.position.z) * 2.0f; distFade = 1.0f - pow(distFade, 4.0f);
// Combine distance- and angle-based fading values. float fade = distFade * angleFade;
// output into DXGI_FORMAT_R8G8B8A8_UNORM decalLookupMap[outputPos] = float4(input.decalTC.xy, fade,
float(customUB.decalIndex) / 255.0f); }
Listing 5.3. Pixel shader for updating decal lookup map.

5.3.3 Apply skinned decals
With the help of the decal lookup map and decal validity buﬀer, skinned decals can be applied in the case of forward rendering during regular shading or in the case of deferred rendering when writing into the G-Buﬀers. The pixel shader code for this is shown in Listing 5.4.

5.3 Implementation

85

// fetch decal texture coordinates and fading value using // bilinear texture filtering float3 decalTCFade = decalLookupMap.
SampleLevel(bilinearSampler, meshTC, 0.0f).xyz;
// fetch decal indices for corresponding 2x2 pixel area float4 decalIndices = decalLookupMap.
GatherAlpha(bilinearSampler, meshTC);
// calculate decal UV derivatives float2 derivX = ddx(decalTCFade.xy); float2 derivY = ddy(decalTCFade.xy);
// determine if valid decal info can be retrieved for // processed fragment uint decalIndex = uint((decalIndices[0] * 255.0f) + 0.0001f); float valid = ((decalIndex > 0)
&& (decalIndices[0] == decalIndices[1]) && (decalIndices[0] == decalIndices[2]) && (decalIndices[0] == decalIndices[3]) && (decalValidityBuffer[decalIndex] == 0)) ? 1.0f : 0.0f;
// only add decal when all fragments of processed quad have valid // decal info to ensure correct UV derivatives [branch] if ((valid == 1.0f) && (ddx(valid) == 0.0f) && (ddy(valid) == 0.0f)) {
// fetch decal diffuse texture uint diffuseIndex = constBuffer.decals[decalIndex].diffuseIndex; float4 decalDiffuse =
decalDiffuseMaps[NonUniformResourceIndex(diffuseIndex)]. SampleGrad(trilinearSampler, decalTCFade.xy, derivX, derivY);
float decalAlphaMask = decalDiffuse.a; float decalFade = baseAlphaMask * decalAlphaMask * decalTCFade.z;
// blend decal diffuse with base diffuse of mesh baseDiffuse = lerp(baseDiffuse, decalDiffuse, decalFade);
// fetch and blend normal, specular, roughness, etc., // textures accordingly }
Listing 5.4. Pixel shader code for applying skinned decals.

86

5. Skinned Decals

To be able to select the correct mipmap from the decal textures, derivatives of the decal texture coordinates are calculated. This has to be done outside of the branch where valid decals are applied, since derivatives require valid values across the entire 22 processed pixel quad. A decal texel is only considered as valid and added on top of the target mesh, when all decal indices in a 22 pixel area of the decal lookup map are valid and equal. To ensure that decal texture coordinate derivatives for trilinear ﬁltering are valid as well, we need to check the validity of the entire 22 processed pixel quad. For this the derivatives of the validity value are used.
The decal index is used to index into a common GPU buﬀer and retrieve decal speciﬁc information. In the case of DirectX 12 this information can contain indices into a common DirectX 12 shader-resource descriptor table to select the corresponding decal textures. For platforms, where textures can't be dynamically indexed, either a texture array or a texture atlas can be used instead.
Since at the border of decals we can't perform bilinear texture ﬁltering, artifacts in form of jaggies will be visible. Such artifacts can be avoided by adding a few texels wide border to the alpha mask texture of a decal that contains an alpha value of zero. Furthermore not all areas on a skinned mesh are a good ﬁt for a decal, especially areas that contain a UV seam or are extremely deformed under skinning, as for example the area around a character elbow. Decals on top of such mesh areas can be easily faded out by an alpha mask texture that is stored alongside the diﬀuse, normal, etc. mesh textures. Mesh areas that don’t have a unique texture coordinate mapping have to be masked out as well.
5.4 Pros and Cons
The following lists give an overview of the pros and cons of the proposed system.
Pros
 Supports both, forward and deferred rendering systems.  Supports static, dynamic, skinned and transparent meshes.  Additional geometry has to be only rendered once when adding a new decal.  Each mesh instance requires only one additional decal lookup map texture, re-
gardless of how many textures the mesh uses.  Textures don’t need to be decompressed.  Texture mipmaps don't need to be generated at runtime.  Artifacts from wrong mipmap selection at geometry edges, as often visible with
deferred decals, are avoided.  Supports arbitrary blending of all decal attributes with underlying mesh.  Decals can be also used to perform displacement mapping or to cut out holes.  Decals can be selectively removed.  Decal texture coordinates can be animated.

5.5 Results

87

Cons
 One decal lookup map and decal validity buﬀer required per mesh instance.  Overlapping decals not supported. However, automatically ensuring that decals
don’t overlap can be also considered as desirable to avoid decals stacking up on each other and causing a performance impact.  Texture coordinates of the target mesh need to be unique, i.e., each triangle of the mesh has to map to a diﬀerent texture area.

5.5 Results
In Figure 5.2, a decal was dynamically added on top of an animated, skinned mesh and rendered once as a deferred decal and once with the proposed technique.

Figure 5.2. The images show a decal dynamically added on top of an animated, skinned model, authored by KatsBits [2014]. The images on the left side show the animation frame where the decal was added, the images on the right side a pose, couple of animation frames later. The images on the top were rendered with the proposed technique, the images on the bottom with the deferred decal technique.

88

5. Skinned Decals

One can observe that while the deferred decal “swims” on the top of the underlying surface, not stretching accordingly, the skinned decal stretches and follows the underlying surface correctly. Thus, in contrast to the skinned decal, the deferred decal breaks the illusion of dynamically changing the target surface. It is also visible, that the skinned decal has the same rendering quality as the deferred decal.
5.6 Conclusion
We presented a system to eﬃciently add high quality, dynamic decals on top of skinned meshes that correctly follow the underlying mesh surface. It supports common rendering systems and can be easily integrated into existing rendering engines.
Bibliography
BRONX. 2011. Deferred decals. Blog post, URL: http://broniac.blogspot.ca/2011/06/deferreddecals.html. KATSBITS. 2014. URL: http://www.katsbits.com/download/models/md5-example.php. KRASSNIGG, J. 2010. A Deferred Decal Rendering Technique. In Game Engine Gems 1, Jones and Bartlett, pp. 271–280. PERSSON, E. 2011. Volume Decals. In GPU Pro 2, A K Peters, pp. 115–120.

