About This E-Book
EPUB is an open, industry-standard format for e-books. However, support for EPUB and its many features varies across reading devices and applications. Use your device or app settings to customize the presentation to your liking. Settings that you can customize often include font, font size, single or double column, landscape or portrait mode, and figures that you can click or tap to enlarge. For additional information about the settings and features on your reading device or app, visit the device manufacturer’s Web site.
Many titles include programming code or configuration examples. To optimize the presentation of these elements, view the e-book in single-column, landscape mode and adjust the font size to the smallest setting. In addition to presenting code and configurations in the reflowable text format, we have included images of the code that mimic the presentation found in the print book; therefore, where the reflowable format may compromise the presentation of the code listing, you will see a “Click here to view code image” link. Click the link to view the print-fidelity code image. To return to the previous page viewed, click the Back button on your device or app.

Vulkan™ Programming Guide
The Official Guide to Learning Vulkan
Graham Sellers With contributions from John Kessenich
Boston • Columbus • Indianapolis • New York • San Francisco • Amsterdam • Cape Town Dubai • London • Madrid • Milan • Munich • Paris • Montreal • Toronto • Delhi • Mexico City
São Paulo • Sydney • Hong Kong • Seoul • Singapore • Taipei • Tokyo

Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals.
The author and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein.
For information about buying this title in bulk quantities, or for special sales opportunities (which may include electronic versions; custom cover designs; and content particular to your business, training goals, marketing focus, or branding interests), please contact our corporate sales department at corpsalespearsoned.com or (800) 382-3419.
For government sales inquiries, please contact governmentsales@pearsoned.com.
For questions about sales outside the U.S., please contact intlcs@pearson.com.
Visit us on the Web: informit.com/aw
Library of Congress Control Number: 2016948832
Copyright © 2017 Pearson Education, Inc.
All rights reserved. Printed in the United States of America. This publication is protected by copyright, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, request forms and the appropriate contacts within the Pearson Education Global Rights & Permissions Department, please visit www.pearsoned.com/permissions/.
Vulkan and the Vulkan logo are trademarks of the Khronos Group Inc.
ISBN-13: 978-0-13-446454-1 ISBN-10: 0-13-446454-0 Text printed in the United States. 1 16

For you, the reader. —Graham Sellers

Contents
Figures
Tables
Listings
About This Book
About the Sample Code Errata
Acknowledgments
About the Author
1 Overview of Vulkan Introduction Instances, Devices, and Queues The Vulkan Instance Vulkan Physical Devices Physical Device Memory Device Queues Creating a Logical Device Object Types and Function Conventions Managing Memory Multithreading in Vulkan Mathematical Concepts Vectors and Matrices Coordinate Systems Enhancing Vulkan Layers Extensions Shutting Down Cleanly Summary
2 Memory and Resources Host Memory Management Resources Buffers Formats and Support Images Resource Views Destroying Resources Device Memory Management Allocating Device Memory Host Access to Device Memory

Binding Memory to Resources Sparse Resources Summary
3 Queues and Commands Device Queues Creating Command Buffers Recording Commands Recycling Command Buffers Submission of Commands Summary
4 Moving Data Managing Resource State Pipeline Barriers Global Memory Barriers Buffer Memory Barriers Image Memory Barriers Clearing and Filling Buffers Clearing and Filling Images Copying Image Data Copying Compressed Image Data Stretching Images Summary
5 Presentation Presentation Extension Presentation Surfaces Presentation on Microsoft Windows Presentation on Xlib-Based Platforms Presentation with Xcb Swap Chains Full-Screen Surfaces Performing Presentation Cleaning Up Summary
6 Shaders and Pipelines An Overview of GLSL An Overview of SPIR-V Representation of SPIR-V Handing SPIR-V to Vulkan Pipelines Compute Pipelines Creating Pipelines Specialization Constants Accelerating Pipeline Creation

Binding Pipelines Executing Work Resource Access in Shaders
Descriptor Sets Binding Resources to Descriptor Sets Binding Descriptor Sets Uniform, Texel, and Storage Buffers Push Constants Sampled Images Summary
7 Graphics Pipelines The Logical Graphics Pipeline Renderpasses The Framebuffer Creating a Simple Graphics Pipeline Graphics Shader Stages Vertex Input State Input Assembly Tessellation State Viewport State Rasterization State Multisample State Depth and Stencil State Color Blend State Dynamic State Summary
8 Drawing Getting Ready to Draw Vertex Data Indexed Draws Index-Only Rendering Reset Indices Instancing Indirect Draws Summary
9 Geometry Processing Tessellation Tessellation Configuration Tessellation Variables Tessellation Example: Displacement Mapping Geometry Shaders Cutting Primitives Geometry Shader Instancing Programmable Point Size

Line Width and Rasterization User Clipping and Culling The Viewport Transformation Summary
10 Fragment Processing Scissor Testing Depth and Stencil Operations Depth Testing Stencil Testing Early Fragment Tests Multisample Rendering Sample Rate Shading Multisample Resolves Logic Operations Fragment Shader Outputs Color Blending Summary
11 Synchronization Fences Events Semaphores Summary
12 Getting Data Back Queries Executing Queries Timing Queries Reading Data with the Host Summary
13 Multipass Rendering Input Attachments Attachment Contents Attachment Initialization Render Areas Preserving Attachment Content Secondary Command Buffers Summary
Appendix: Vulkan Functions
Glossary
Index

Figures
Figure 1.1 Vulkan Hierarchy of Instance, Device, and Queue
Figure 2.1 Mipmap Image Layout Figure 2.2 Memory Layout of LINEAR Tiled Images Figure 2.3 Gamma Curves for sRGB (Top) and Simple Powers (Bottom)
Figure 2.4 Cube Map Construction Figure 2.5 Host and Device Memory
Figure 4.1 Data Layout of Images Stored in Buffers
Figure 6.1 Descriptor Sets and Pipeline Sets Figure 6.2 Linear Sampling Figure 6.3 Effect of Sampling Modes
Figure 7.1 The Full Vulkan Graphics Pipeline Figure 7.2 Strip (Left) and Fan (Right) Topologies Figure 7.3 Triangles with Adjacency Topology Figure 7.4 Triangle Strip with Adjacency Topology
Figure 8.1 Index Data Flow Figure 8.2 The Effect of Primitive Restart on Triangle Strips Figure 8.3 Many Instanced Cubes
Figure 9.1 Quad Tessellation Figure 9.2 Triangle Tessellation Figure 9.3 Isoline Tessellation Figure 9.4 Tessellation Spacing Modes Figure 9.5 Result of Tessellated Displacement Mapping Figure 9.6 Rasterization of Strict Lines Figure 9.7 Rasterization of Nonstrict Lines Figure 9.8 Clipping Against a Viewport
Figure 10.1 Standard Sample Locations
Figure 13.1 Data Flow for a Simple Deferred Renderer Figure 13.2 Serial Dependency of Translucent on Opaque Geometry Figure 13.3 Parallel Rendering of Translucent and Opaque Geometry

Tables
Table 2.1 Sparse Texture Block Sizes Table 6.1 Pipeline Resource Limits Table 6.2 Texture Comparison Functions Table 7.1 Dynamic and Static State Validity Table 9.1 GLSL and SPIR-V Tessellation Modes Table 9.2 GLSL and SPIR-V Tessellation Winding Order Table 10.1 Depth Comparison Functions Table 10.2 Stencil Operations Table 10.3 Logic Operations Table 10.4 Blend Equations Table 10.5 Blend Factors

Listings
Listing 1.1 Creating a Vulkan Instance Listing 1.2 Querying Physical Device Properties Listing 1.3 Creating a Logical Device Listing 1.4 Querying Instance Layers Listing 1.5 Querying Instance Extensions
Listing 2.1 Declaration of a Memory Allocator Class Listing 2.2 Implementation of a Memory Allocator Class Listing 2.3 Creating a Buffer Object Listing 2.4 Creating an Image Object Listing 2.5 Choosing a Memory Type for an Image
Listing 3.1 Example of Using vkCmdCopyBuffer()
Listing 4.1 Image Memory Barrier Listing 4.2 Filling a Buffer with Floating-Point Data
Listing 5.1 Creating a Swap Chain Listing 5.2 Transitioning an Image to Present Source
Listing 6.1 Simplest Possible GLSL Shader Listing 6.2 Simplest SPIR-V Listing 6.3 Local Size Declaration in a Compute Shader (GLSL) Listing 6.4 Local Size Declaration in a Compute Shader (SPIR-V) Listing 6.5 Specialization Constants in GLSL Listing 6.6 Specialization Constants in SPIR-V Listing 6.7 Saving Pipeline Cache Data to a File Listing 6.8 Declaring Resources in GLSL Listing 6.9 Declaring Resources in SPIR-V Listing 6.10 Creating a Pipeline Layout Listing 6.11 Declaring Uniform and Shader Blocks in GLSL Listing 6.12 Declaring Uniform and Shader Blocks in SPIR-V Listing 6.13 Declaring Texel Buffers in GLSL Listing 6.14 Declaring Texel Buffers in SPIR-V Listing 6.15 Declaring Push Constants in GLSL Listing 6.16 Declaring Push Constants in SPIR-V

Listing 7.1 Creating a Simple Renderpass Listing 7.2 Creating a Simple Graphics Pipeline Listing 7.3 Describing Vertex Input Data Listing 7.4 Declaring Inputs to a Vertex Shader (GLSL) Listing 7.5 Declaring Inputs to a Vertex Shader (SPIR-V)
Listing 8.1 Separate Vertex Attribute Setup Listing 8.2 Indexed Cube Data Listing 8.3 Using the Vertex Index in a Shader Listing 8.4 Using the Instance Index in a Shader Listing 8.5 Draw Index Used in a Shader
Listing 9.1 Trivial Tessellation Control Shader (GLSL) Listing 9.2 Trivial Tessellation Control Shader (SPIR-V) Listing 9.3 Declaring Outputs in Tessellation Control Shaders (GLSL) Listing 9.4 Declaring Outputs in Tessellation Control Shaders (SPIR-V) Listing 9.5 Accessing gl_TessCoord in Evaluation Shader (GLSL) Listing 9.6 Accessing gl_TessCoord in Evaluation Shader (SPIR-V) Listing 9.7 Descriptor Setup for Displacement Mapping Listing 9.8 Vertex Shader for Displacement Mapping Listing 9.9 Tessellation Control Shader for Displacement Mapping Listing 9.10 Tessellation Evaluation Shader for Displacement Mapping Listing 9.11 Tessellation State Creation Information Listing 9.12 Minimal Geometry Shader (GLSL) Listing 9.13 Minimal Geometry Shader (SPIR-V) Listing 9.14 Declaring gl_PerVertex in a GLSL Geometry Shader Listing 9.15 Reading gl_PerVertex in a SPIR-V Geometry Shader Listing 9.16 Declaring an Output Block in GLSL Listing 9.17 Pass-Through GLSL Geometry Shader Listing 9.18 Pass-Through SPIR-V Geometry Shader Listing 9.19 Cutting Strips in a Geometry Shader Listing 9.20 Instanced GLSL Geometry Shader Listing 9.21 Use of gl_PointSize in GLSL Listing 9.22 Decorating an Output with PointSize Listing 9.23 Use of gl_ClipDistance in GLSL Listing 9.24 Decorating Outputs with ClipDistance Listing 9.25 Use of gl_CullDistance in GLSL Listing 9.26 Decorating Outputs with CullDistance

Listing 9.27 Using Multiple Viewports in a Geometry Shader (GLSL)
Listing 10.1 Declaring an Output in a Fragment Shader (GLSL) Listing 10.2 Declaring an Output in a Fragment Shader (SPIR-V) Listing 10.3 Several Outputs in a Fragment Shader (GLSL) Listing 10.4 Several Outputs in a Fragment Shader (SPIR-V)
Listing 11.1 Setup for Four-Fence Synchronization Listing 11.2 Loop Waiting on Fences for Synchronization Listing 11.3 Cross-Queue Submission with Semaphores
Listing 12.1 C Structure for All Pipeline Statistics Listing 12.2 Moving a Buffer to Host-Readable State
Listing 13.1 Deferred Shading Renderpass Setup Listing 13.2 Translucency and Deferred Shading Setup

About This Book
This book is about Vulkan. Vulkan is an application programming interface (API) for controlling devices such as graphics processing units (GPUs). Although Vulkan is a logical successor to OpenGL, it is quite different from OpenGL in form. One of the things that experienced practitioners will notice about Vulkan is that it is very verbose. You need to write a lot of application code to get Vulkan to do anything useful, let alone anything remarkable. Many of the things that an OpenGL driver would do are now the responsibility of the Vulkan application writer. These things include synchronization, scheduling, memory management, and so on. As such, you will find a good deal of this book dedicated to such topics, even though they are general topics applicable to more than just Vulkan.
The intended audience for this book is experienced programmers who are already familiar with other graphics and compute APIs. As such, many graphics-related topics are discussed without deep introduction, there are some forward references, and code samples are incomplete or illustrative in scope rather than being complete programs that you can type in. The sample code available from the book’s website is complete and tested, however, and should serve as a good reference to follow along with.
Vulkan is intended to be used as the interface between large, complex graphics and compute applications and graphics hardware. Many of the features and responsibilities previously assumed by drivers implementing APIs such as OpenGL now fall to the application. Complex game engines, large rendering packages, and commercial middleware are well-suited to this task; they have more information about their specific behavior than any driver could hope to have. Vulkan is not wellsuited to simple test applications; neither is it a suitable aid for teaching graphics concepts.
In the first chapters of this book, we introduce Vulkan and some of the fundamental concepts that frame the API. As we progress through the Vulkan system, we cover more advanced topics, eventually producing a more complex rendering system that shows off some of the unique aspects of Vulkan and demonstrates its capabilities.
In Chapter 1, “Overview of Vulkan,” we provide a brief introduction to Vulkan and the concepts that form its foundation. We cover the basics of creating Vulkan objects and show the basics of getting started with the Vulkan system.
In Chapter 2, “Memory and Resources,” we introduce the memory system of Vulkan, perhaps the most fundamental part of the interface. We show how to allocate memory used by the Vulkan device and by Vulkan drivers and system components running inside your application.
In Chapter 3, “Queues and Commands,” we cover command buffers and introduce the queues to which they are submitted. We show how Vulkan processes work and how your application can build packets of commands to be sent to the device for execution.
In Chapter 4, “Moving Data,” we introduce our first few Vulkan commands, all of which are focused on moving data. We use the concepts first discussed in Chapter 3 to build command buffers that can copy and format data stored in the resources and memory introduced in Chapter 2.
In Chapter 5, “Presentation,” we show how to get images produced by your application onto the screen. Presentation is the term used for interacting with a window system, which is platformspecific, so this chapter delves into some platform-specific topics.

In Chapter 6, “Shaders and Pipelines,” we introduce SPIR-V, the binary shading language used by Vulkan. We also introduce the pipeline object; show how one is constructed using SPIR-V shaders; and then introduce compute pipelines, which can be used to do computation work with Vulkan.
In Chapter 7, “Graphics Pipelines,” we build upon what we covered in Chapter 6 and introduce the graphics pipeline, which includes all of the configuration necessary to render graphical primitives with Vulkan.
In Chapter 8, “Drawing,” we discuss the various drawing commands available in Vulkan, including indexed and nonindexed draws, instancing, and indirect commands. We show how to get data into the graphics pipeline and how to draw more complex geometries than were introduced in Chapter 7.
In Chapter 9, “Geometry Processing,” we dig deeper into the first half of the Vulkan graphics pipeline and take another look at the tessellation and geometry shader stages. We show some of the more advanced things that these stages can do and cover the pipeline up to the rasterization stage.
In Chapter 10, “Fragment Processing,” we pick up where Chapter 9 left off and cover everything that happens during and after rasterization in order to turn your geometry into a stream of pixels that can be displayed to the user.
In Chapter 11, “Synchronization,” we cover the various synchronization primitives available to the Vulkan application, including fences, events, and semaphores. Together, these form the foundation of any application that makes efficient use of the parallel nature of Vulkan.
In Chapter 12, “Getting Data Back,” we reverse the direction of communication used in previous chapters and discuss the issues involved in reading data from Vulkan into your application. We show how to time operations executed by a Vulkan device, how to gather statistics about the operation of Vulkan devices, and how to get data produced by Vulkan back into your application.
Finally, in Chapter 13, “Multipass Rendering,” we revisit a number of topics covered earlier, tying things together to produce a more advanced application—a deferred rendering application using complex multipass architecture and multiple queues for processing.
The appendix to this book contains a table of the command buffer building functions available to Vulkan applications, providing a quick reference to determine their attributes.
Vulkan is a large, complex, and new system. It is extremely difficult to cover every corner of the API in a book of this scope. The reader is encouraged to thoroughly read the Vulkan specification in addition to this book, as well as to read other books on using heterogeneous compute systems and computer graphics with other APIs. Such material will provide a good foundation in the mathematics and other concepts assumed by this book.
About the Sample Code
The sample code that accompanies this book is available from our website (http://www.vulkanprogrammingguide.com). One thing that seasoned users of other graphics APIs will notice is that Vulkan is very verbose. This is primarily because many of the responsibilities historically assumed by drivers have been delegated to your application. In many cases, however, simple boilerplate code will do the job just fine. Therefore, we have created a simple application framework that deals with much of the functionality that will be common to all samples and realworld applications. This does not mean that this book is a tutorial on how to use our framework. This is simply a practical matter of keeping code samples concise.
Of course, as we discuss specific Vulkan functionality throughout the book, we will include snippets of code, many of which may actually come from the book’s sample framework rather than from any

particular example. Some features discussed in the book may not have examples in the code package. This is particularly true of some advanced features that are relevant primarily to large-scale applications. There is no such thing as a short, simple Vulkan example. In many cases, a single example program demonstrates the use of many features. The features that each example uses are listed in that example’s read-me file. Again, there is not a 1:1 correspondence between examples and listings in this book and specific examples in the sample code. It shall be assumed that anyone that files a bug asking for a 1:1 list of which samples go with which chapter has not read this paragraph. Such bugs will be summarily closed, quoting this very sentence.
The sample code is designed to link against the latest official Vulkan SDK from LunarG, which is available from http://lunarg.com/vulkan-sdk/. At the time of writing, the latest SDK version is 1.0.22. Newer versions of the SDK are designed to be backward-compatible with older versions, so we recommend that users obtain the latest available version of the SDK before attempting to compile and run the sample applications. The SDK also comes with some samples of its own, and we suggest running those to verify that the SDK and drivers are installed correctly.
In addition to the Vulkan SDK, you will need a working installation of CMake in order to create the build environment for the samples. You will also need an up-to-date compiler. The code samples make use of several C++11 features and rely heavily on the C++ standard libraries for things like threading and synchronization primitives. These features are known to be problematic in early versions of various compiler runtimes, so please make sure that your compilers are up to date. We have tested with Microsoft Visual Studio 2015 on Windows and with GCC 5.3 on Linux. The samples have been tested on 64-bit Windows 7, Windows 10, and Ubuntu 16.10 with recent drivers from AMD, Intel, and NVIDIA.
It should be noted that Vulkan is a cross-platform, cross-vendor, and cross-device system. Many of these samples should work on Android and other mobile platforms. We hope to port the samples to as many of these platforms as possible in the future and would very much appreciate help and contributions from you, the reader.
Errata
Vulkan is a new technology. At the time of writing, the specification has been available for only a matter of weeks. Although the author and contributor had a hand in creating the Vulkan specification, it’s large and complex and had many contributors. Some of the code in the book is not fully tested, and although it is believed to be correct, it may contain errors. As we were putting the samples together, available Vulkan implementations still had bugs, the validation layers didn’t catch as many errors as they could, and the specification itself had gaps and unclear sections. Like the readers, we are still learning Vulkan, so although this text was edited for technical accuracy, we depend on readers to view any updates by visiting this book’s website:
http://www.vulkanprogrammingguide.com
Register your copy of VulkanTM Programming Guide at informit.com for convenient access to downloads, updates, and corrections as they become available. To start the registration process, go to informit.com/register and log in or create an account. Enter the product ISBN (9780134464541) and click Submit. Once the process is complete, you will find any available bonus content under “Registered Products.”

Acknowledgments
First and foremost, I’d like to acknowledge the members of the Vulkan working group. Through a tireless and extremely long process, we produced what I believe to be a very solid foundation for computer graphics and compute acceleration in the years to come. I would especially like to recognize the contributions of my peers at AMD who developed the original Mantle specification, from which Vulkan was derived. I would like to thank our reviewers, Dan Ginsburg and Chris “Xenon” Hanson. Thank you for your valuable feedback, without which this book would certainly contain more errors and omissions than it does. I would also like to thank my colleague Mais Alnasser, who provided excellent feedback and contributed to the quality of this book. Further thanks are due to the rest of the Vulkan team at AMD, whose work allowed me to test much of the sample code before access to Vulkan was available to the wider public. The cover image was produced by Dominic Agoro-Ombaka of Agoro Design (http://agorodesign.com/) on short notice. Thanks to him for delivering on a tight schedule. A huge thank you goes to my editor, Laura Lewin, and the rest of the team at Addison-Wesley for allowing me to repeatedly slip the schedule, make late edits, deliver work in an ad-hoc manner, and generally be a pain to work with. I appreciate your trust in me with this project. Finally, I need to thank my family—my wife, Chris, and my children, Jeremy and Emily. “Dad, are you still writing your book?” has become a regular chant in our house. I appreciate your patience, love, and support as I’ve crunched out a whole new book over the last few months.
—Graham Sellers

About the Author
Graham Sellers is a software architect at AMD who oversees the development of the OpenGL and Vulkan drivers for AMD’s Radeon and FirePro products. His passion for computers and technology began at a young age with a BBC Micro, followed by a long line of 8- and 16-bit home computers that he still enjoys working with. He earned his master’s in engineering from the University of Southampton, England, and now lives in Orlando, Florida, with his wife and two children.

Chapter 1. Overview of Vulkan
What You’ll Learn in This Chapter
• What Vulkan is and the fundamentals behind it
• How to create a minimal Vulkan application
• The terminology and concepts used in the remainder of this book
In this chapter, we introduce Vulkan and explain what it is. We introduce some of the fundamental concepts behind the API, including initialization, object lifetimes, the Vulkan instance, and logical and physical devices. By the end of the chapter, we produce a simple Vulkan application that can initialize the Vulkan system, discover available Vulkan devices and show their properties and capabilities, and finally shut down cleanly.
Introduction
Vulkan is a programming interface for graphics and compute devices. A Vulkan device typically consists of a processor and a number of fixed-function hardware blocks to accelerate operations used in graphics and compute. The processor in the device is usually a very wide multithreaded processor and so the computational model in Vulkan is heavily based on parallel computing. The Vulkan device also has access to memory that may or may not be shared with the main processor upon which your application is running. Vulkan also exposes this memory to you.
Vulkan is an explicit API. That is, almost everything is your responsibility. A driver is a piece of software that takes the commands and data forming the API and translates them into something that hardware can understand. In older APIs such as OpenGL, drivers would track the state of a lot of objects, manage memory and synchronization for you, and check for errors in your application as it ran. This is great for developers but burns valuable CPU time once your application is debugged and running correctly. Vulkan addresses this by placing almost all state tracking, synchronization, and memory management into the hands of the application developer and by delegating correctness checks to layers that must be enabled. They do not participate in the execution of your application under normal circumstances.
For these reasons, Vulkan is both very verbose and somewhat fragile. You need to do an awful lot of work to get Vulkan running well, and incorrect usage of the API can often lead to graphical corruption or even program crashes where in older APIs you would have received a helpful error message. In exchange for this, Vulkan provides more control over the device, a clean threading model, and much higher performance than the APIs that it supersedes.
Further, Vulkan has been designed to be more than a graphics API. It can be used for heterogeneous devices such as graphics processing units (GPUs), digital signal processors (DSPs), and fixedfunction hardware. Functionality is divided into coarse-grained, broadly overlapping categories. The current edition of Vulkan defines the transfer category, which is used for copying data around; the compute category, which is used for running shaders over compute workloads; and the graphics category, which includes rasterization, primitive assembly, blending, depth and stencil tests, and other functionality that will be familiar to graphics programmers.

To the extent that support for each category is optional, it’s possible to have a Vulkan device that doesn’t support graphics at all. As a consequence, even the APIs to put pictures onto a display device (which is called presentation) are not only optional, but are provided as extensions to Vulkan rather than being part of the core API.
Instances, Devices, and Queues
Vulkan includes a hierarchy of functionality, starting at the top level with the instance, which aggregates all Vulkan-capable devices together. Each device then exposes one or more queues. It is the queues that perform the work that your application requests.
The Vulkan instance is a software construct that logically separates the state of your application from other applications or libraries running within the context of your application. The physical devices in the sytem are presented as members of the instance, each of which has certain capabilities, including a selection of available queues.
A physical device usually represents a single piece of hardware or a collection of hardware that is interconnected. There is a fixed, finite number of physical devices in any system unless that system supports reconfiguration such as hot-plug. A logical device, which is created by the instance, is the software construct around a physical device and represents a reservation of resources associated with a particular physical device. This includes a possible subset of the available queues on the physical device. It is possible to create multiple logical devices representing a single physical device, and it is the logical device that your application will spend most of its time interacting with.
Figure 1.1 illustrates this hierarchy. In the figure, the application has created two Vulkan instances. There are three physical devices in the system that are available to both instances. After enumeration, the application creates one logical device on the first physical device, two logical devices for the second device, and another for the third. Each logical device enables a different subset of its corresponding physical device’s queues. In practice, most Vulkan applications won’t be nearly this complex and will simply create a single logical device for one of the physical devices in the system, using a single instance. Figure 1.1 only serves to demonstrate the flexibility of the Vulkan system.

Figure 1.1: Vulkan Hierarchy of Instance, Device, and Queue
The following subsections discuss how to create the Vulkan instance, query the physical devices in the system, attach a logical device corresponding to one of them, and finally retrieve handles to the queues exposed by the device.

The Vulkan Instance
Vulkan can be seen as a subsystem of your application. Once you link your application to the Vulkan libraries and initialize it, it tracks some state. Because Vulkan doesn’t introduce any global state into your application, all tracked state must be stored in an object that you provide. This is the instance object and is represented by a VkInstance object. To construct one, we’ll call our first Vulkan function, vkCreateInstance(), the prototype of which is
Click here to view code image

VkResult vkCreateInstance ( const VkInstanceCreateInfo* const VkAllocationCallbacks* VkInstance*

pCreateInfo, pAllocator, pInstance);

This declaration is typical of a Vulkan function. Where more than a handful of parameters are to be passed to Vulkan, functions often take pointers to structures. Here, pCreateInfo is a pointer to an

instance of the VkInstanceCreateInfo structure that contains the parameters describing the new Vulkan instance. The definition of VkInstanceCreateInfo is
Click here to view code image

typedef struct VkInstanceCreateInfo {

VkStructureType

sType;

const void*

pNext;

VkInstanceCreateFlags

flags;

const VkApplicationInfo* pApplicationInfo;

uint32_t

enabledLayerCount;

const char* const*

ppEnabledLayerNames;

uint32_t

enabledExtensionCount;

const char* const*

ppEnabledExtensionNames;

} VkInstanceCreateInfo;

The first member in almost every Vulkan structure that is used to pass parameters to the API is the sType field, which tells Vulkan what type of structure this is. Each structure in the core API and in any extension has an assigned structure tag. By inspecting this tag, Vulkan tools, layers, and drivers can determine the type of the structure for validation purposes and for use in extensions. Further, the pNext field allows a linked list of structures to be passed to the function. This allows the set of parameters to be extended without needing to replace the core structure wholesale in an extension. Because we are using the core instance creation structure here, we pass VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO in the sType field and simply set pNext to nullptr.
The flags field of VkInstanceCreateInfo is reserved for future use and should be set to zero. The next field, pApplicationInfo, is an optional pointer to another structure describing your application. You can set this to nullptr, but a well-behaved application should fill this in with something useful. pApplicationInfo points to an instance of the VkApplicationInfo structure, the definition of which is
Click here to view code image

typedef struct VkApplicationInfo {

VkStructureType sType;

const void*

pNext;

const char*

pApplicationName;

uint32_t

applicationVersion;

const char*

pEngineName;

uint32_t

engineVersion;

uint32_t

apiVersion;

} VkApplicationInfo;

Again, we see the sType and pNext fields in this structure. sType should be set to VK_STRUCTURE_TYPE_APPLICATION_INFO, and we can leave pNext as nullptr. pApplicationName is a pointer to a nul-terminated1 string containing the name of your application, and applicationVersion is the version of the application. This allows tools and drivers to make decisions about how to treat your application without needing to guess2 which application is running. Likewise, pEngineName and engineVersion contain the name and version, respectively, of the engine or middleware that your application is based on.

1. Yes, really, nul. The ASCII character whose literal value is zero is officially called NUL. Now, stop telling me to change it to NULL. That’s a pointer, not a name of a character.
2. What’s best for one application might be different from what’s best for another. Also, applications are written by humans, and humans write code with bugs. To optimize fully or work around application bugs, drivers would sometimes use executable names or even application behavior to guess at which application was running and alter behavior appropriately. While not ideal, this new mechanism at least removes the guesswork.
Finally, apiVersion contains the version of the Vulkan API that your application is expecting to run on. This should be set to the absolute minimum version of Vulkan that your application requires to run—not just to the version of the header that you happen to have installed. This allows the widest possible assortment of devices and platforms to run your application, even if updates to their Vulkan implementations might not be available.
Returning to the VkInstanceCreateInfo structure, we see the enabledLayerCount and ppEnabledLayerNames fields. These are the count of the number of instance layers that you wish to enable and their names, respectively. Layers are used to intercept the Vulkan API and provide logging, profiling, debugging, or other additional features. If no layers are needed, simply set enabledLayerCount to zero and leave ppEnabledLayerNames as nullptr. Likewise, enabledExtensionCount is the count of the number of extensions you wish to enable,3 and ppEnabledExtensionNames is a list of their names. Again, if we’re not using any extensions, we can set these fields to zero and nullptr, respectively.
3. As with OpenGL, Vulkan supports extensions as a central part of the API. However, in OpenGL, we would create a context, query the supported extensions, and then start using them. This meant that drivers would need to assume that your application might suddenly start using an extension at any time and be ready for it. Further, it couldn’t tell which extensions you were looking for, which made the process even more difficult. In Vulkan, applications are required to opt in to extensions and explicitly enable them. This allows drivers to disable extensions that aren’t in use and makes it harder for applications to accidentally start using functionality that’s part of an extension they weren’t intending to enable.
Finally, returning to the vkCreateInstance() function, the pAllocator parameter is a pointer to a host memory allocator that your application can supply in order to manage the host memory that the Vulkan system uses. Setting this to nullptr causes the Vulkan system to use its own internal allocator, which is what we will do here. Application-managed host memory will be covered in Chapter 2, “Memory and Resources.”
Assuming the vkCreateInstance() function succeeds, it will return VK_SUCCESS and place a handle to the new instance in the variable pointed to by the pInstance parameter. A handle is the value by which objects are referenced. Vulkan handles are always 64 bits wide, regardless of the bitness of the host system. Once we have a handle to our Vulkan instance, we can use it to call other instance functions.

Vulkan Physical Devices
Once we have an instance, we can use it to discover Vulkan-compatible devices installed in the system. Vulkan has two types of devices: physical and logical. Physical devices are normally parts of the system—a graphics card, accelerator, DSP, or other component. There are a fixed number of physical devices in a system, and each has a fixed set of capabilities. A logical device is a software abstraction of a physical device, configured in a way that is specified by the application. The logical device is the one that your application will spend most of its time dealing with, but before we can create a logical device, we must discover the connected physical devices. To do this, we call the vkEnumeratePhysicalDevices() function, the prototype of which is
Click here to view code image

VkResult vkEnumeratePhysicalDevices ( VkInstance uint32_t* VkPhysicalDevice*

instance, pPhysicalDeviceCount, pPhysicalDevices);

The first parameter to the vkEnumeratePhysicalDevices() function, instance, is the instance we created earlier. Next, the pPhysicalDeviceCount parameter is a pointer to an unsigned integer variable that is both an input and an output. As an output, Vulkan writes the number of physical devices in the system into it. As an input, it should be preinitialized with the maximum number of devices your application can handle. The pPhysicalDevices parameter is a pointer to an array of this number of VkPhysicalDevice handles.
If you just want to know how many devices there are in the system, set pPhysicalDevices to nullptr, and Vulkan will ignore the initial value of pPhysicalDeviceCount, simply overwriting it with the number of supported devices. You can dynamically adjust the size of your VkPhysicalDevice array by calling vkEnumeratePhysicalDevices() twice, the first time with only pPhysicalDevices set to nullptr (although pPhysicalDeviceCount must still be a valid pointer) and the second time with pPhysicalDevices set to an array that has been appropriately sized for the number of physical devices reported by the first call.
Assuming there are no problems, vkEnumeratePhysicalDevices() returns VK_SUCCESS and deposits the number of recognized physical devices in pPhysicalDeviceCount along with their handles in pPhysicalDevices. Listing 1.1 shows an example of constructing the VkApplicationInfo and VkInstanceCreateInfo structures, creating the Vulkan instance, querying it for the number of supported devices, and finally querying the physical device handles themselves. This is a slightly simplified version of vkapp::init from the example framework.

Listing 1.1: Creating a Vulkan Instance

Click here to view code image
VkResult vkapp::init() {
VkResult result = VK_SUCCESS; VkApplicationInfo appInfo = { }; VkInstanceCreateInfo instanceCreateInfo = { };

// A generic application info structure

appInfo.sType = VK_STRUCTURE_TYPE_APPLICATION_INFO; appInfo.pApplicationName = "Application"; appInfo.applicationVersion = 1; appInfo.apiVersion = VK_MAKE_VERSION(1, 0, 0);

// Create the instance. instanceCreateInfo.sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO; instanceCreateInfo.pApplicationInfo = &appInfo;

result = vkCreateInstance(&instanceCreateInfo, nullptr, &m_instance);

if (result == VK_SUCCESS) {
// First figure out how many devices are in the system. uint32_t physicalDeviceCount = 0; vkEnumeratePhysicalDevices(m_instance, &physicalDeviceCount, nullptr);

if (result == VK_SUCCESS) {
// Size the device array appropriately and get the physical // device handles. m_physicalDevices.resize(physicalDeviceCount); vkEnumeratePhysicalDevices(m_instance,
&physicalDeviceCount, &m_physicalDevices[0]); } } return result; }

The physical device handle is used to query the device for its capabilities and ultimately to create the logical device. The first query we’ll perform is vkGetPhysicalDeviceProperties(), which fills in a structure describing all the properties of the physical device. Its prototype is
Click here to view code image

void vkGetPhysicalDeviceProperties ( VkPhysicalDevice VkPhysicalDeviceProperties*

physicalDevice, pProperties);

When you call vkGetPhysicalDeviceProperties(), pass one of the handles returned from vkEnumeratePhysicalDevices() in the physicalDevice parameter, and in pProperties, pass a pointer to an instance of the VkPhysicalDeviceProperties structure. This is a large structure that contains a large number of fields describing the properties of the physical device. Its definition is
Click here to view code image

typedef struct VkPhysicalDeviceProperties {

uint32_t

apiVersion;

uint32_t

driverVersion;

uint32_t

vendorID;

uint32_t

deviceID;

VkPhysicalDeviceType char
uint8_t VkPhysicalDeviceLimits VkPhysicalDeviceSparseProperties } VkPhysicalDeviceProperties;

deviceType; deviceName
[VK_MAX_PHYSICAL_DEVICE_NAME_SIZE]; pipelineCacheUUID[VK_UUID_SIZE]; limits; sparseProperties;

The apiVersion field contains the highest version of Vulkan supported by the device, and the driverVersion field contains the version of the driver used to control the device. This is vendorspecific, so it doesn’t make sense to compare driver versions across vendors. The vendorID and deviceID fields identify the vendor and the device, and are usually PCI vendor and device identifiers.4
4. There is no official central repository of PCI vendor or device identifiers. The PCI SIG (http://pcisig.com/) assigns vendor identifiers to its members, and those members assign device identifiers to their products. A fairly comprehensive list in both human- and machine-readable forms is available from http://pcidatabase.com/.
The deviceName field will contain a human-readable string naming the device. The pipelineCacheUUID field is used for pipeline caching, which we will cover in Chapter 6, “Shaders and Pipelines.”
In addition to the properties just listed, the VkPhysicalDeviceProperties structure embeds VkPhysicalDeviceLimits and VkPhysicalDeviceSparseProperties, which contain the minimum and maximum limits for the physical device and properties related to sparse textures. There’s a lot of information in these structures, and we’ll cover the fields separately as the related features are discussed rather than enumerating them all here.
In addition to core features, some of which have optionally higher limits or bounds, Vulkan has a number of optional features that may be supported by a physical device. If a device advertises support for a feature, it must still be enabled (much like an extension), but once enabled, that feature becomes a first-class citizen of the API just like any core feature. To determine which features a physical device supports, call vkGetPhysicalDeviceFeatures(), the prototype of which is
Click here to view code image

void vkGetPhysicalDeviceFeatures ( VkPhysicalDevice VkPhysicalDeviceFeatures*

physicalDevice, pFeatures);

Again, the VkPhysicalDeviceFeatures structure is very large and has a Boolean field for each optional feature supported by Vulkan. There are too many fields to list and describe individually here, but the sample application presented at the end of this chapter reads the feature set and prints its content.

Physical Device Memory
In many cases, a Vulkan device is either a separate physical piece of hardware to the main host processor or works sufficiently differently that it will access memory in specialized ways. Device memory in Vulkan refers to memory that is accessible to the device and usable as a backing store for textures and other data. Memory is classified into types, each of which has a set of properties, such as caching flags and coherency behavior between host and device. Each type of memory is then backed by one of the device’s heaps, of which there may be several.
To query the configuration of heaps and the memory types supported by the device, call
Click here to view code image

void vkGetPhysicalDeviceMemoryProperties (

VkPhysicalDevice

physicalDevice,

VkPhysicalDeviceMemoryProperties*

pMemoryProperties);

The resulting memory organization is written into the VkPhysicalDeviceMemoryProperties structure, the address of which is passed in pMemoryProperties. The VkPhysicalDeviceMemoryProperties structure contains the properties of both the device’s heaps and its supported memory types. The definition of this structure is
Click here to view code image

typedef struct VkPhysicalDeviceMemoryProperties {

uint32_t

memoryTypeCount;

VkMemoryType memoryTypes[VK_MAX_MEMORY_TYPES];

uint32_t

memoryHeapCount;

VkMemoryHeap memoryHeaps[VK_MAX_MEMORY_HEAPS];

} VkPhysicalDeviceMemoryProperties;

The number of memory types is reported in the memoryTypeCount field. The maximum number of memory types that might be reported is the value of VK_MAX_MEMORY_TYPES, which is defined to be 32. The memoryTypes array contains memoryTypeCount VkMemoryType structures describing each of the memory types. The definition of VkMemoryType is
Click here to view code image

typedef struct VkMemoryType {

VkMemoryPropertyFlags propertyFlags;

uint32_t

heapIndex;

} VkMemoryType;

This is a simple structure containing only a set of flags and the memory type’s heap index. The flags field describes the type of memory and is made up of a combination of the VkMemoryPropertyFlagBits flags. The meanings of the flags are as follows:
• VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT means that the memory is local to (that is, physically connected to) the device. If this bit is not set, then the memory can be assumed to be local to the host.
• VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT means that memory allocations made with this type can be mapped and read or written by the host. If this bit is not set then memory of this type cannot be directly accessed by the host and is rather for exclusive use by the device.

• VK_MEMORY_PROPERTY_HOST_COHERENT_BIT means that when this type of memory is concurrently accessed by both the host and device, those accesses will be coherent between the two clients. If this bit is not set, then the device or host may not see the results of writes performed by each until caches are explicitly flushed.
• VK_MEMORY_PROPERTY_HOST_CACHED_BIT means that data in this type of memory is cached by the host. Read accesses to this type of memory are typically faster than they would be if this bit were not set. However, access by the device may have slightly higher latency, especially if the memory is also coherent.
• VK_MEMORY_PROPERTY_LAZILY_ALLOCATED_BIT means that memory allocated with this type doesn’t necessarily consume space from the associated heap immediately and that a driver might defer physical memory allocation until the memory object is used to back a resource.
Each memory type reports the heap from which it consumes space in the heapIndex field of the VkMemoryType structure. This is an index into the memoryHeaps array returned in the VkPhysicalDeviceMemoryProperties structure from the call to vkGetPhysicalDeviceMemoryProperties(). Each element of the memoryHeaps array describes one of the device’s memory heaps. The definition of this structure is
Click here to view code image

typedef struct VkMemoryHeap {

VkDeviceSize

size;

VkMemoryHeapFlags flags;

} VkMemoryHeap;

Again, this is a simple structure. It contains only the size, in bytes, of the heap and some flags describing the heap. In Vulkan 1.0, the only defined flag is VK_MEMORY_HEAP_DEVICE_LOCAL_BIT. If this bit is set, then the heap is local to the device. This corresponds to the similarly named flag describing memory types.

Device Queues
Vulkan devices execute work that is submitted to queues. Each device will have one or more queues, and each of those queues will belong to one of the device’s queue families. A queue family is a group of queues that have identical capabilities but are able to run in parallel. The number of queue families, the capabilities of each family, and the number of queues belonging to each family are all properties of the physical device. To query the device for its queue families, call vkGetPhysicalDeviceQueueFamilyProperties(), the prototype of which is
Click here to view code image

void vkGetPhysicalDeviceQueueFamilyProperties (

VkPhysicalDevice

physicalDevice,

uint32_t*

pQueueFamilyPropertyCount,

VkQueueFamilyProperties*

pQueueFamilyProperties);

vkGetPhysicalDeviceQueueFamilyProperties() works somewhat like vkEnumeratePhysicalDevices() in that it is expected that you call it twice. The first time, you pass nullptr as pQueueFamilyProperties, and in pQueueFamilyPropertyCount, you pass a pointer to a variable that will be overwritten with

the number of queue families supported by the device. You can use this number to appropriately size an array of VkQueueFamilyProperties. Then, on the second call, pass this array in pQueueFamilyProperties, and Vulkan will fill it with the properties of the queues. The definition of VkQueueFamilyProperties is
Click here to view code image

typedef struct VkQueueFamilyProperties {

VkQueueFlags queueFlags;

uint32_t

queueCount;

uint32_t

timestampValidBits;

VkExtent3D

minImageTransferGranularity;

} VkQueueFamilyProperties;

The first field in this structure, queueFlags, describes the overall capabilities of the queue. This field is made up of a combination of the VkQueueFlagBits bits, the meanings of which are as follows:
• If VK_QUEUE_GRAPHICS_BIT is set, then queues in this family support graphics operations such as drawing points, lines, and triangles.
• If VK_QUEUE_COMPUTE_BIT is set, then queues in this family support compute operations such as dispatching compute shaders.
• If VK_QUEUE_TRANSFER_BIT is set, then queues in this family support transfer operations such as copying buffer and image contents.
• If VK_QUEUE_SPARSE_BINDING_BIT is set, then queues in this family support memory binding operations used to update sparse resources.
The queueCount field indicates the number of queues in the family. This might be set to 1, or it could be substantially higher if the device supports multiple queues with the same basic functionality.
The timestampValidBits field indicates how many bits are valid when timestamps are taken from the queue. If this value is zero, then the queue doesn’t support timestamps. If it’s nonzero, then it’s guaranteed to be at least 36 bits. Furthermore, if the timestampComputeAndGraphics field of the device’s VkPhysicalDeviceLimits structure is VK_TRUE, then all queues supporting either VK_QUEUE_GRAPHICS_BIT or VK_QUEUE_COMPUTE_BIT are guaranteed to support timestamps with at least 36 bits of resolution. In this case, there’s no need to check each queue individually.
Finally, the minImageTimestampGranularity field specifies the units with which the queue supports image transfers (if at all).
Note that it might be the case that a device reports more than one queue family with apparently identical properties. Queues within a family are essentially identical. Queues in different families may have different internal capabilities that can’t be expressed easily in the Vulkan API. For this reason, an implementation might choose to report similar queues as members of different families. This places additional restrictions on how resources are shared between those queues, which might allow the implementation to accommodate those differences.
Listing 1.2 illustrates how to query the physical device’s memory properties and queue family properties. You will need to retrieve the queue family properties before creating the logical device, as discussed in the next section.

Listing 1.2: Querying Physical Device Properties

Click here to view code image
uint32_t queueFamilyPropertyCount; std::vector<VkQueueFamilyProperties> queueFamilyProperties; VkPhysicalDeviceMemoryProperties physicalDeviceMemoryProperties;
// Get the memory properties of the physical device. vkGetPhysicalDeviceMemoryProperties(m_physicalDevices[deviceIndex],
&physicalDeviceMemoryProperties);
// First determine the number of queue families supported by the physical // device. vkGetPhysicalDeviceQueueFamilyProperties(m_physicalDevices[0],
&queueFamilyPropertyCount, nullptr);
// Allocate enough space for the queue property structures. queueFamilyProperties.resize(queueFamilyPropertyCount);
// Now query the actual properties of the queue families. vkGetPhysicalDeviceQueueFamilyProperties(m_physicalDevices[0],
&queueFamilyPropertyCount, queueFamilyProperties.data());

Creating a Logical Device
After enumerating all of the physical devices in the system, your application should choose a device and create a logical device corresponding to it. The logical device represents the device in an initialized state. During creation of the logical device, you get to opt in to optional features, turn on extensions you need, and so on. Creating the logical device is performed by calling vkCreateDevice(), the prototype of which is
Click here to view code image

VkResult vkCreateDevice ( VkPhysicalDevice const VkDeviceCreateInfo* const VkAllocationCallbacks* VkDevice*

physicalDevice, pCreateInfo, pAllocator, pDevice);

The physical device to which the new logical device corresponds is passed in physicalDevice. The information about the new logical device is passed in an instance of the VkDeviceCreateInfo structure through the pCreateInfo structure. The definition of VkDeviceCreateInfo is
Click here to view code image

typedef struct VkDeviceCreateInfo { VkStructureType const void* VkDeviceCreateFlags uint32_t

sType; pNext; flags; queueCreateInfoCount;

const VkDeviceQueueCreateInfo* uint32_t const char* const* uint32_t const char* const* const VkPhysicalDeviceFeatures* } VkDeviceCreateInfo;

pQueueCreateInfos; enabledLayerCount; ppEnabledLayerNames; enabledExtensionCount; ppEnabledExtensionNames; pEnabledFeatures;

The sType field of the VkDeviceCreateInfo structure should be set to VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO. As usual, unless you’re using extensions, pNext should be set to nullptr. In the current version of Vulkan, no bits are defined for the flags field of the structure, so set this to zero too.
Next comes the queue creation information. pQueueCreateInfos is a pointer to an array of one or more VkDeviceQueueCreateInfo structures, each of which allows the specification of one or more queues. The number of structures in the array is given in queueCreateInfoCount. The definition of VkDeviceQueueCreateInfo is
Click here to view code image

typedef struct VkDeviceQueueCreateInfo {

VkStructureType

sType;

const void*

pNext;

VkDeviceQueueCreateFlags flags;

uint32_t

queueFamilyIndex;

uint32_t

queueCount;

const float*

pQueuePriorities;

} VkDeviceQueueCreateInfo;

The sType field for VkDeviceQueueCreateInfo is VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO. There are currently no flags defined for use in the flags field, so it should be set to zero. The queueFamilyIndex field specifies the family of the queues you want to create. This is an index into the array of queue families returned from vkGetPhysicalDeviceQueueFamilyProperties(). To create queues in this family, set queueCount to the number of queues you want to use. Of course, the device must support at least this many queues in the family you choose.
The pQueuePriorities field is an optional pointer to an array of floating point values representing the relative priority of work submitted to each of the queues. These numbers are normalized numbers in the range of 0.0 to 1.0. Queues with higher priority may be allocated more processing resources or scheduled more aggressively than queues with lower priority. Setting pQueuePriorities to nullptr has the effect of leaving the queues at the same, default priority.
The requested queues are sorted in order of priority and then assigned device-dependent relative priorities. The number of discrete priorities that a queue may take on is a device-specific parameter. You can determine this by checking the discreteQueuePriorities field of the VkPhysicalDeviceLimits structure returned from a call to vkGetPhysicalDeviceProperties(). For example, if a device supports only low- and highpriority workloads, this field will be 2. All devices support at least two discrete priority levels. However, if a device supports arbitrary priorities, then this field could be much higher. Regardless of

the value of discreteQueuePriorities, the relative priorities of the queue are still expressed as floating-point values.
Returning to the VkDeviceCreateInfo structure, the enabledLayerCount, ppEnabledLayerNames, enabledExtensionCount, and ppEnabledExtensionNames fields are for enabling layers and extensions. We will cover both of these topics later in this chapter. For now, we’ll set both enabledLayerCount and enabledExtensionCount to zero and both ppEnabledLayerNames and ppEnabedExtensionNames to nullptr.
The final field of VkDeviceCreateInfo, pEnabledFeatures, is a pointer to an instance of the VkPhysicalDeviceFeatures structure that specifies which of the optional features that your application wishes to use. If you don’t want to use any optional features, you can simply set this to nullptr. However, Vulkan in this form is relatively limited, and much of its interesting functionality will be disabled.
To determine which of the optional features the device supports, call vkGetPhysicalDeviceFeatures() as discussed earlier. vkGetPhysicalDeviceFeatures() writes the set of features supported by the device into an instance of the VkPhysicalDeviceFeatures structure that you pass in. By simply querying the phyiscal device’s features and then passing the very same VkPhysicalDeviceFeatures structure back to vkCreateDevice(), you enable every optional feature that the device supports and do not request features that the device does not support.
Simply enabling every supported feature, however, may come with some performance impact. For some features, a Vulkan implementation may need to allocate extra memory, track additional state, configure hardware slightly differently, or perform some other operation that otherwise costs your application. It’s not a good idea to enable features that won’t be used. In an optimized application, you should query the supported features from the device; then, from the supported features, enable the specific features that your application requires.
Listing 1.3 shows a simple example of querying the device for its supported features, setting up a list of features that the application requires. Support for tessellation and geometry shaders is absolutely required, and support for multidraw indirect is enabled if it is supported by the device. The code then creates a device using a single instance of its first queue.

Listing 1.3: Creating a Logical Device

Click here to view code image
VkResult result; VkPhysicalDeviceFeatures supportedFeatures; VkPhysicalDeviceFeatures requiredFeatures = {};

vkGetPhysicalDeviceFeatures(m_physicalDevices[0], &supportedFeatures);

requiredFeatures.multiDrawIndirect supportedFeatures.multiDrawIndirect; requiredFeatures.tessellationShader requiredFeatures.geometryShader

=
= VK_TRUE; = VK_TRUE;

const VkDeviceQueueCreateInfo deviceQueueCreateInfo =

{ VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO, nullptr, 0, 0, 1, nullptr
}; const VkDeviceCreateInfo deviceCreateInfo = {
VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO, nullptr,

// sType // pNext // flags // queueFamilyIndex // queueCount // pQueuePriorities
// sType // pNext

0, 1, queueCreateInfoCount

// flags //

&deviceQueueCreateInfo, 0, nullptr, 0, enabledExtensionCount nullptr, ppEnabledExtensionNames &requiredFeatures };

// pQueueCreateInfos // enabledLayerCount // ppEnabledLayerNames //
//
// pEnabledFeatures

result = vkCreateDevice(m_physicalDevices[0], &deviceCreateInfo, nullptr, &m_logicalDevice);

After the code in Listing 1.3 has run and successfully created the logical device, the set of enabled features is stored in the requiredFeatures variable. This may be kept for later so that code that can optionally use a feature can check whether it was successfully enabled and fall back gracefully.

Object Types and Function Conventions
Virtually everything in Vulkan is represented as an object that is referred to by a handle. Handles are divided into two broad categories: dispatchable objects and nondispatchable objects. For the most part, this is not relevant to applications and affects only how the API is structured and how systemlevel components such as the Vulkan loader and layers interoperate with those objects.
Dispatchable objects are objects that internally contain a dispatch table. This is the table of functions used by various components to determine which parts of code to execute when your application makes calls to Vulkan. These types of objects are generally heavier-weight constructs and currently consist of the instance (VkInstance), physical device (VkPhysicalDevice), logical device (VkDevice), command buffer (VkCommandBuffer), and queue (VkQueue). All other objects are considered nondispatchable.
The first argument to any Vulkan function is always a dispatchable object. The only exceptions to this rule are the functions related to creating and initializing the instance.

Managing Memory
Vulkan provides two types of memory: host memory and device memory. Objects created by the Vulkan API generally require some amount of host memory. This is where the Vulkan implementation will store the state of the object and any data it needs to implement the Vulkan API. Resource objects such as buffers and images require some amount of device memory. This is the memory where the data stored in the resource is kept.
It is possible for your application to manage host memory for the Vulkan implementation, and it is required that your application manage device memory. To do this, you will need to create a device memory management subsystem. Each resource that you create can be queried for the amount and type of memory it requires for it to be backed. It will be up to your application to allocate the correct amount of memory and attach it to the resource object before it can be used.
In higher-level APIs such as OpenGL, this “magic” is performed by drivers on behalf of your application. However, some applications require a very large number of small resources, and other applications require a smaller number of very large resources. Some applications create and destroy resources over the course of their execution, whereas other applications create all of their resources at startup and do not free them until they are terminated.
The allocation strategies used in these cases might be quite different. There is no one-size-fits-all strategy. An OpenGL driver has no idea how your application will behave and so must adapt allocation strategies to attempt to fit your usage patterns. On the other hand, you, the application developer, know exactly how your application will behave. You can partition resources into longlived and transient groups. You can bucket resources that will be used together into a small number of pooled allocations. You are in the best position to decide the allocation strategies used by your application.
It is important to note that each “live” memory allocation places some cost on the system. Therefore, it is important to keep the number of allocation objects to a minimum. It is recommended that device memory allocators allocate memory in large chunks. Many small resources can be placed inside a much smaller number of device memory blocks. An example of a device memory allocator is discussed in Chapter 2, “Memory and Resources,” which discusses memory allocation in much more detail.
Multithreading in Vulkan
Support for multithreaded applications is an integral part of the design of Vulkan. Vulkan generally assumes that the application will ensure that no two threads are mutating the same object at the same time. This is known as external synchronization. The vast majority of Vulkan commands in the performance-critical portions of Vulkan (such as building command buffers) provide no synchronization at all.
In order to concretely define the threading requirements of various Vulkan commands, each parameter that must be protected from concurrent access by the host is marked as externally synchronized. In some cases, handles to objects or other data are embdedd in data structures, included in arrays, or otherwise passed to commands through some indirect means. Those paramters must also be externally synchronized.
The intention of this is that a Vulkan implementation never needs to take a mutex or use other synchronization primitives internally to protect data structures. This means that multithreaded programs rarely stall or block across threads.

In addition to requiring the host to synchronize access to shared objects when they are used across threads, Vulkan includes a number of higher-level features that are designed specifically to allow threads to perform work without blocking one another. These include the following:
• Host memory allocations can be handled through a host memory allocation structure passed to object creation functions. By using an allocator per thread, the data structures in that allocator don’t need to be protected. Host memory allocators are covered in Chapter 2, “Memory and Resources.”
• Command buffers are allocated from pools, and access to the pool is externally synchronized. If an application uses a separate command pool per thread, command buffers can be allocated from those pools without blocking against one another. Command buffers and pools are covered in Chapter 3, “Queues and Commands.”
• Descriptors are allocated in sets from descriptor pools. Descriptors are the representation of resources as used by shaders running on the device. They are covered in detail in Chapter 6, “Shaders and Pipelines.” If a separate pool is used for each thread, then descriptor sets can be allocated from those pools without the threads blocking one another.
• Second-level command buffers allow the contents of a large renderpass (which must be contained in a single command buffer) to be generated in parallel and then grouped as they’re called from the primary command buffer. Secondary command buffers are covered in detail in Chapter 13, “Multipass Rendering.”
When you are building a very simple, single-threaded application, the requirement to create pools from which to allocate objects may seem like a lot of unnecessary indirection. However, as applications scale in number of threads, these objects are indispensible to achieving high performance.
Throughout the remainder of this book, any special requirements with respect to threading will be noted as the commands are introduced.
Mathematical Concepts
Computer graphics and most heterogeneous compute applications are fairly heavily math-based. Most Vulkan devices are based on extremely powerful computational processors. At the time of writing, even modest mobile processors are capable of providing many gigaflops of processing power, while higher-end desktop and workstation processors deliver many teraflops of numbercrunching ability. As a consequence, really interesting applications will build on math-heavy shaders. Further, several fixed-function sections of Vulkan’s processing pipeline are built upon mathematical concepts that are hard-wired into the device and specification.
Vectors and Matrices
One of the fundamental building blocks of any graphics application is the vector. Whether they’re the representations of a position, a direction, a color, or some other quantity, vectors are used throughout the graphics literature. One common form of vector is the homogeneous vector, which is a vector in a space one dimension higher than the quantity it’s representing. These vectors are used to store projective coordinates. Multiplying a homogeneous vector by any scalar produces a new vector representing the same projective coordinate. To project a point vector, divide through by its last component, producing a vector of the form x, y, z, 1.0 (for a four-component vector).

To transform a vector from one coordinate space to another, multiply the vector by a matrix. Just as a point in 3D space is represented as a four-component homogeneous vector, a transformation matrix operating on a 3D homogeneous vector is a 4 × 4 matrix.
A point in 3D space is typically represented as a homogeneous vector of four components conventionally called x, y, z, and w. For a point, the w component generally begins as 1.0 and changes as the vector is transformed through projective matrices. After division through by the w component, the point is projected through whichever transforms it’s been subjected to. If none of the transforms is a projective transform, then w remains 1.0, and division by 1.0 has no effect on the vector. If the vector was subjected to a projective transform, then w will not be equal to 1.0, but dividing through by it will project the point and return w to 1.0.
Meanwhile, a direction in 3D space is also represented as a homogeneous vector whose w component is 0.0. Multiplying a direction vector by a properly constructed 4 × 4 projective matrix will leave the w component at 0.0, and it will have no effect on any of the other components. By simply discarding the additional component, you can put a 3D direction vector through the same transformations as a 4D homogeneous 3D point vector and make it undergo rotations, scales, and other transforms consistently.
Coordinate Systems
Vulkan deals with graphics primitives such as lines and triangles by representing their endpoints or corners as points in 3D space. These primitives are known as vertices. The inputs to the Vulkan system are vertex coordinates in a 3D coordinate space (represented as homogenous vectors with w components of 1.0) relative to the origin of the object of which they are part. This coordinate space is known as object space or sometimes model space.
Typically, the first shaders in the pipeline will transform this vertex into view space, which is a position relative to the viewer. This transformation is performed by multiplying the vertex’s position vector by a transformation matrix. This is often called the object-to-view matrix or the model-view matrix.
Sometimes, absolute coordinates of a vertex are required, such as when finding the coordinate of a vertex relative to some other object. This global space is known as world space and is the position of vertices relative to a global origin.
From view space, the positions of vertices are transformed into clip space. This is the final space used by the geometry-processing part of Vulkan and is the space into which vertices are typically transformed when pushing them into the projective space used by typical 3D applications. Clip space is so called because it is the coordinate space in which most implementations perform clipping, which removes sections of primitives that lie outside the visible region being rendered.
From clip space, vertex positions are normalized by dividing through by their w components. This yields a coordinate space called normalized device coordinates, or NDC, and the process is often called the perspective divide. In this space, the visible part of the coordinate system is from −1.0 to 1.0 in the x and y directions and from 0.0 to 1.0 in the z direction. Anything outside this region will be clipped away prior to perspective division.
Finally, a vertex’s normalized device coordinate is transformed by the viewport, which describes how NDC maps into a window or image into which the picture is being rendered.

Enhancing Vulkan
Although the core API specification of Vulkan is quite extensive, it’s by no means all-encompassing. Some functionality is optional, while yet more is available in the form of layers (which modify or enhance existing behavior) and extensions (which add new functionality to Vulkan). Both enhancement mechanisms are described in the following sections.

Layers
Layers are features of Vulkan that allow its behavior to be modified. Layers generally intercept all or part of Vulkan and add functionality such as logging, tracing, providing diagnostics, profiling, and so on. A layer can be added at the instance level, in which case it affects the whole Vulkan instance and possibly every device created by it. Alternatively, the layer can be added at the device level, in which case it affects only the device for which it is enabled.
To discover the layers available to an instance on a system, call vkEnumerateInstanceLayerProperties(), the prototype of which is
Click here to view code image

VkResult vkEnumerateInstanceLayerProperties (

uint32_t*

pPropertyCount,

VkLayerProperties*

pProperties);

If pProperties is nullptr, then pPropertyCount should point to a variable that will be overwritten with the count of the number of layers available to Vulkan. If pProperties is not nullptr, then it should point to an array of VkLayerProperties structures that will be filled with information about the layers registered with the system. In this case, the initial value of the variable pointed to by pPropertyCount is the length of the array pointed to by pProperties, and this variable will be overwritten with the number of entries in the array overwritten by the command.
Each element of the pProperties array is an instance of the VkLayerProperties structure, the definition of which is
Click here to view code image

typedef struct VkLayerProperties {

char

layerName[VK_MAX_EXTENSION_NAME_SIZE];

uint32_t specVersion;

uint32_t implementationVersion;

char

description[VK_MAX_DESCRIPTION_SIZE];

} VkLayerProperties;

Each layer has a formal name that is stored in the layerName member of the VkLayerProperties structure. As the specification for each layer might be improved, clarified, or appended to over time, the version of the layer implementation is reported in specVersion.
As specifications are improved over time, so too are implementations of those specifications. The implementation version is stored in the implementationVersion field of the VkLayerProperties structure. This allows implementations to improve performance, fix bugs, implement a wider set of optional features, and so on. An application writer may recognize a

particular implementation of a layer and choose to use it only if the version of that implementation is past a certain version where, for example, a critical bug was known to be fixed.
Finally, a human-readable string describing the layer is stored in description. The only purpose of this field is for logging or display in a user interface, and it is for informational purposes only.
Listing 1.4 illustrates how to query the instance layers supported by the Vulkan system.

Listing 1.4: Querying Instance Layers

Click here to view code image
uint32_t numInstanceLayers = 0; std::vector<VkLayerProperties> instanceLayerProperties;

// Query the instance layers. vkEnumerateInstanceLayerProperties(&numInstanceExtensions,
nullptr);

// If there are any layers, query their properties. if (numInstanceLayers != 0) {
instanceLayerProperties.resize(numInstanceLayers); vkEnumerateInstanceLayerProperties(nullptr,
&numInstanceLayers, instanceLayerProperties.data()); }

As mentioned, it is not only at the instance level that layers can be injected. Layers can also be applied at the device level. To determine which layers are available to devices, call vkEnumerateDeviceLayerProperties(), the prototype of which is
Click here to view code image

VkResult vkEnumerateDeviceLayerProperties (

VkPhysicalDevice

physicalDevice,

uint32_t*

pPropertyCount,

VkLayerProperties*

pProperties);

The layers available to each physical device in a system may be different, so each physical device can report a different set of layers. The physical device whose layers to query is passed in physicalDevice. The pPropertyCount and pProperties parameters to vkEnumerateDeviceLayerProperties() behave similarly to the identically named parameters to vkEnumerateInstanceLayerProperties(). Device layers are also described by instances of the VkLayerProperties structure.
To enable a layer at the instance level, include its name in the ppEnabledLayerNames field of the VkInstanceCreateInfo structure used to create the instance. Likewise, to enable a layer when creating a logical device corresponding to a physical device in the system, include the layer name in the ppEnabledLayerNames member of the VkDeviceCreateInfo used to create the device.
Several layers are included in the offical SDK, most of which are related to debugging, parameter validation, and logging. These include the following:

• VK_LAYER_LUNARG_api_dump prints Vulkan calls and their parameters and values to the console.
• VK_LAYER_LUNARG_core_validation performs validation on parameters and state used in descriptor sets, pipeline state, and dynamic state; validates the interfaces between SPIR-V modules and the graphics pipeline; and tracks and validates usage of GPU memory used to back objects.
• VK_LAYER_LUNARG_device_limits ensures that values passed to Vulkan commands as arguments or data structure members fall within the device’s supported feature set limits.
• VK_LAYER_LUNARG_image validates that image usage is consistent with supported formats.
• VK_LAYER_LUNARG_object_tracker performs tracking on Vulkan objects, attempting to catch leaks, use-after-free errors, and other invalid object usage.
• VK_LAYER_LUNARG_parameter_validation confirms that all parameter values passed to Vulkan functions are valid.
• VK_LAYER_LUNARG_swapchain performs validation on functionality provided by the WSI (Window System Integration) extensions described in Chapter 5, “Presentation.”
• VK_LAYER_GOOGLE_threading ensures valid usage of Vulkan commands with respect to threading, ensuring that no two threads access the same object at the same time when they shouldn’t.
• VK_LAYER_GOOGLE_unique_objects ensures that every object will have a unique handle for easier tracking by the application, avoiding cases where an implementation might deduplicate handles that represent objects with the same parameters.
In addition to this, a large number of separate layers are grouped into a larger, single layer called VK_LAYER_LUNARG_standard_validation, making it easy to turn on. The book’s application framework enables this layer when built in debug mode, leaving all layers disabled when built in release mode.
Extensions
Extensions are fundamental to any cross-platform, open API such as Vulkan. They allow implementers to experiment, innovate, and ultimately push technology forward. Eventually, useful features originally introduced as extensions make their way into future versions of the API after having been proved in the field. However, extensions are not without cost. Some may require implementations to track additional state, make additional checks during command buffer build, or come with some performance penalty even when the extension is not in direct use. Therefore, extensions must be explicitly enabled by an application before they can be used. This means that applications that don’t use an extension don’t pay for it in terms of performance or complexity and that it’s almost impossible to accidentally use features from an extension, which improves portability.
Extensions are divided into two categories: instance extensions and device extensions. An instance extension is one that generally enhances the entire Vulkan system on a platform. This type of extension is either supplied via a device-independent layer or is simply an extension that is exposed by every device on the system and promoted into an instance. Device extensions are extensions that extend the capabilities of one or more devices in the sytem but aren’t necessarily available on every device.

Each extension can define new functions, new types, structures, enumerations, and so on. Once enabled, an extension can be considered part of the API that is available to the application. Instance extensions must be enabled when the Vulkan instance is created, and device extensions must be enabled when the device is created. These leaves us with a chicken-and-egg situation: How do we know which extensions are supported before initializing a Vulkan instance?
Querying the supported instance extensions is one of the few pieces of Vulkan functionality that may be used before a Vulkan instance is created. This is performed using the vkEnumerateInstanceExtensionProperties() function, the prototype of which is
Click here to view code image

VkResult vkEnumerateInstanceExtensionProperties (

const char*

pLayerName,

uint32_t*

pPropertyCount,

VkExtensionProperties*

pProperties);

pLayerName is the name of a layer that might provide extensions. For now, set this to nullptr. pPropertyCount is a pointer to a variable containing the count of the number of instance extensions to query Vulkan about, and pProperties is a pointer to an array of VkExtensionProperties structures that will be filled with information about the supported extensions. If pProperties is nullptr, then the initial value of the variable pointed to by pPropertyCount is ignored and is overwritten with the number of instance extensions supported.
If pProperties is not nullptr, then the number of entries in the array is assumed to be in the variable pointed to by pPropertyCount, and up to this many entries of the array are populated with information about the supported extensions. The variable pointed to by pPropertyCount is then overwritten with the number of entries actually written to pProperties.
To correctly query all of the supported instance extensions, call vkEnumerateInstanceExtensionProperties() twice. The first time, call it with pProperties set to nullptr to retrieve the number of supported instance extensions. Then appropriately size an array to receive the extension properties and call vkEnumerateInstanceExtensionProperties() again, this time passing the address of the array in pProperties. Listing 1.5 demonstrates how to do this.

Listing 1.5: Querying Instance Extensions

Click here to view code image
uint32_t numInstanceExtensions = 0; std::vector<VkExtensionProperties> instanceExtensionProperties;

// Query the instance extensions. vkEnumerateInstanceExtensionProperties(nullptr,
&numInstanceExtensions, nullptr);

// If there are any extensions, query their properties. if (numInstanceExtensions != 0) {
instanceExtensionProperties.resize(numInstanceExtensions);

vkEnumerateInstanceExtensionProperties(nullptr, &numInstanceExtensions, instanceExtensionProperties.data());
}

After the code in Listing 1.5 has completed execution, instanceExtensionProperties will contain a list of the extensions supported by the instance. Each element of the array of VkExtensionProperties describes one extension. The definition of VkExtensionProperties is
Click here to view code image

typedef struct VkExtensionProperties {

char

extensionName[VK_MAX_EXTENSION_NAME_SIZE];

uint32_t specVersion;

} VkExtensionProperties;

The VkExtensionProperties structure simply contains the name of the extension and the version of that extension. Extensions may add functionality over time as new revisions of the extension are produced. The specVersion field allows updates to extensions without the need to create an entirely new extension in order to add minor functionality. The name of the extension is stored in extensionName.
As you saw earlier, when creating the Vulkan instance, the VkInstanceCreateInfo structure has a member called ppEnabledExtensionNames, which is a pointer to an array of strings naming the extensions to enable. If the Vulkan system on a platform supports an extension, that extension will be included in the array returned from vkEnumerateInstanceExtensionProperties(), and its name can be passed to vkCreateInstance() via the ppEnabledExtensionNames field of the VkInstanceCreateInfo structure.
Querying the support for device extensions is a similar process. To do this, call vkEnumerateDeviceExtensionProperties(), whose prototype is
Click here to view code image

VkResult vkEnumerateDeviceExtensionProperties (

VkPhysicalDevice

physicalDevice,

const char*

pLayerName,

uint32_t*

pPropertyCount,

VkExtensionProperties*

pProperties);

The prototype of vkEnumerateDeviceExtensionProperties() is almost identical to that of vkEnumerateInstanceExtensionProperties(), with the addition of a physicalDevice parameter. The physicalDevice parameter is the handle to the device whose extensions to query. As with vkEnumerateInstanceExtensionProperties(), vkEnumerateDeviceExtensionProperties() overwrites pPropertyCount with the number of supported extensions if pProperties is nullptr, and if pProprties is not nullptr, it fills that array with information about the supported extensions. The same VkExtensionProperties structure is used for device extensions and for instance extensions.

When you are creating the physical device, the ppEnabledExtensionNames field of the VkDeviceCreateInfo structure may contain a pointer to one of the strings returned from vkEnumerateDeviceExtensionProperties().
Some extensions provide new functionality in the form of additional entry points that you can call. These are exposed as function pointers, the values of which you must query either from the instance or from the device after enabling the extension. Instance functions are functions that are valid for the entire instance. If an extension extends instance-level functionality, you should use an instance-level function pointer to access the new features.
To retrieve an instance-level function pointer, call vkGetInstanceProcAddr(), the prototype of which is
Click here to view code image

PFN_vkVoidFunction vkGetInstanceProcAddr (

VkInstance

instance,

const char*

pName);

The instance parameter is the handle to the instance for which to retrieve a new function pointer. If your application does use more than one Vulkan instance, then the function pointer returned from this command is valid only for objects owned by the referenced instance. The name of the function is passed in pName, which is a nul-terminated UTF-8 string. If the function name is recognized and the extension is enabled, the return value from vkGetInstanceProcAddr() is a function pointer that you can call from your application.
The PFN_vkVoidFunction is a type definition for a pointer to a function of the following declaration:
Click here to view code image

VKAPI_ATTR void VKAPI_CALL vkVoidFunction(void);

There are no functions in Vulkan that have this particular signature, and it is unlikely that an extension would introduce such a function. In all likelihood, you will need to cast the resulting function pointer to a pointer of the appropriate signature before you can call it.
Instance-level function pointers are valid for any object owned by the instance, assuming the device that created the object (or the device itself, if the function dispatches on the device) supports the extension and the extension is enabled for that device. Because each device might be implemented inside a different Vulkan driver, instance function pointers must dispatch though a layer of indirection to land in the correct module. Managing this indirection may incur some overhead; to avoid this, you can get a device-specific function pointer that goes directly to the appropriate driver.
To get a device-level function pointer, call vkGetDeviceProcAddr(), the prototype of which is
Click here to view code image

PFN_vkVoidFunction vkGetDeviceProcAddr (

VkDevice

device,

const char*

pName);

The device with which the function pointer will be used is passed in device. Again, the name of the function you are querying is passed as a nul-terminated UTF-8 string in pName. The resulting function pointer is valid only with the device specified in device. device must refer to a device

that supports the extension that provides the new function and for which the extension has been enabled.
The function pointer produced by vkGetDeviceProcAddr() is specific to device. Even if the same physical device is used to create two or more logical devices with the exact same parameters, you must use the resulting function pointers only with the devices from which they were queried.

Shutting Down Cleanly
Before your program exits, you should clean up after yourself. In many cases, an operating system will clean up resources that you’ve allocated when your application terminates. However, it will not always be the case that the end of your code is the end of the application. If you are writing a component of a larger application, for example, that application may terminate rendering or compute operations using Vulkan without actually exiting.
When cleaning up, it is generally good practice to do the following:
• Complete or otherwise terminate all work that your application is doing both on the host and the device, in all threads related to Vulkan.
• Destroy objects in the reverse order from the order in which they were created.
The logical device is likely to be the last object (aside from objects used at runtime) that you created during initialization of your application. Before destroying the device, you should ensure that it is not executing any work on behalf of your application. To do this, call vkDeviceWaitIdle(), the prototype of which is
Click here to view code image

VkResult vkDeviceWaitIdle ( VkDevice

device);

The handle to the device is passed in device. When vkDeviceWaitIdle() returns, all work submitted to the device on behalf of your application is guaranteed to have completed—unless, of course, you submit more work to the device in the meantime. You should ensure that any other threads that might be submitting work to the device have been terminated.
Once you have ensured that the device is idle, you can safely destroy it. To do this, call vkDestroyDevice(), the prototype of which is
Click here to view code image

void vkDestroyDevice ( VkDevice const VkAllocationCallbacks*

device, pAllocator);

The handle to the device to destroy is passed in the device parameter, access to which must be externally synchronized. Note that access to a device does not need to be externally synchronized with respect to any other command. The application should ensure, however, that a device is not destroyed while any other command accessing it is still executing in another thread.
pAllocator should point to an allocation structure that is compatible with the one used to create the device. Once the device object has been destroyed, no more commands can be submitted to it. Further, it is no longer possible to use the device handle as an argument to any function, including

other object-destruction functions that take a device handle as their first argument. This is another reason why you should destroy objects in reverse order from the order in which they were created.
Once all devices associated with a Vulkan instance have been destroyed, it is safe to destroy the instance. This is accomplished by calling the vkDestroyInstance() function, whose prototype is
Click here to view code image

void vkDestroyInstance ( VkInstance const VkAllocationCallbacks*

instance, pAllocator);

The handle to the instance to destroy is passed in instance and, as with vkDestroyDevice(), a pointer to an allocation structure compatible with the one with which the instance was allocated should be passed in pAllocator. If the pAllocator parameter to vkCreateInstance() was nullptr, then the pAllocator parameter to vkDestroyInstance() should be too.
Note that it’s not necessary to destroy the physical devices. Physical devices are not created with a dedicated creation function as logical devices are. Rather, physical devices are returned from a call to vkEnumeratePhysicalDevices() and are considered to be owned by the instance. Therefore, when the instance is destroyed, that instance’s resources associated with each physical device are freed as well.

Summary
This chapter introduced you to Vulkan. You have seen how the entirety of Vulkan state is contained within an instance. The instance provides access to physical devices, and each physical device exposes a number of queues that can be used to do work. You have seen how to create a logical device corresponding to the physical device. You have seen how Vulkan can be extended, how to determine the extensions available to the instance and to a device, and how to enable those extensions. You have seen how to cleanly shut down the Vulkan system by waiting for a device to complete work submitted by your application, destroying the device handles, and finally destroying the instance handle.

Chapter 2. Memory and Resources

What You’ll Learn in This Chapter • How Vulkan manages host and device memory • How to manage memory effectively in your application • How Vulkan uses images and buffers to consume and produce data

Memory is fundamental to the operation of virtually all computing systems, including Vulkan. In Vulkan, there are two fundamental types of memory: host memory and device memory. All resources upon which Vulkan operates must be backed by device memory, and it is the application’s responsibility to manage this memory. Further, memory is used to store data structures on the host. Vulkan provides the opportunity for your application to manage this memory too. In this chapter, you’ll learn about the mechanisms through which you can manage memory used by Vulkan.

Host Memory Management
Whenever Vulkan creates new objects, it might need memory to store data related to them. For this, it uses host memory, which is regular memory accessible to the CPU that might be returned from a call to malloc or new, for example. However, beyond a normal allocator, Vulkan has particular requirements for some allocations. Most notably, it expects allocations to be aligned correctly. This is because some high-performance CPU instructions work best (or only) on aligned memory addresses. By assuming that allocations storing CPU-side data structures are aligned, Vulkan can use these highperformance instructions unconditionally, providing substantial performance advantages.
Because of these requirements, Vulkan implementations will use advanced allocators to satisfy them. However, it also provides the opportunity for your application to replace the allocators for certain, or even all, operations. This is performed through the pAllocator parameter available in most device creation functions. For example, let’s revisit the vkCreateInstance() function, which is one of the first that your application might call. Its prototype is
Click here to view code image

VkResult vkCreateInstance ( const VkInstanceCreateInfo* const VkAllocationCallbacks* VkInstance*

pCreateInfo, pAllocator, pInstance);

The pAllocator parameter is a pointer to a VkAllocationCallbacks structure. Until now, we’ve been setting pAllocator to nullptr, which tells Vulkan to use its own internal allocator rather than rely on our application. The VkAllocationCallbacks structure encapsulates a custom memory allocator that we can provide. The definition of the structure is
Click here to view code image

typedef struct VkAllocationCallbacks { void* PFN_vkAllocationFunction

pUserData; pfnAllocation;

PFN_vkReallocationFunction PFN_vkFreeFunction PFN_vkInternalAllocationNotification PFN_vkInternalFreeNotification } VkAllocationCallbacks;

pfnReallocation; pfnFree; pfnInternalAllocation; pfnInternalFree;

You can see from the definition of VkAllocationCallbacks that the structure is essentially a set of function pointers and an additional void pointer, pUserData. The pointer is for your application’s use. It can point anywhere; Vulkan will not dereference it. In fact, it doesn’t even need to be a pointer. You can put anything in there, so long as it fits into a pointer-size blob. The only thing that Vulkan will do with pUserData is pass it back to the callback functions to which the remaining members of VkAllocationCallbacks point.
pfnAllocation, pfnReallocation, and pfnFree are used for normal, object-level memory management. They are defined as pointers to functions that match the following declarations:
Click here to view code image

void* VKAPI_CALL Allocation( void* size_t size_t VkSystemAllocationScope

pUserData, size, alignment, allocationScope);

void* VKAPI_CALL Reallocation( void* void* size_t size_t VkSystemAllocationScope

pUserData, pOriginal size, alignment, allocationScope);

void VKAPI_CALL Free( void* void*

pUserData, pMemory);

Notice that all three functions take a pUserData parameter as their first argument. This is the same pUserData pointer that’s part of the VkAllocationCallbacks structure. If your application uses data structures to manage memory, this is a good place to put their addresses. One logical thing to do with this is to implement your memory allocator as a C++ class (assuming you’re writing in C++) and then put the class’s this pointer in pUserData.
The Allocation function is responsible for making new allocations. The size parameter gives the size of the allocation, in bytes. The alignment parameter gives the required alignment of the allocation, also in bytes. This is an often-overlooked parameter. It is very tempting to simply hook this function up to a naïve allocator such as malloc. If you do this, you will find that it works for a while but that certain functions might mysteriously crash later. If you provide your own allocator, it must honor the alignment parameter.
The final parameter, allocationScope, tells your application what the scope, or lifetime, of the allocation is going to be. It is one of the VkSystemAllocationScope values, which have the following meanings:

• VK_SYSTEM_ALLOCATION_SCOPE_COMMAND means that the allocation will be live only for the duration of the command that provoked the allocation. Vulkan will likely use this for very short-lived temporary allocations, as it works on a single command.
• VK_SYSTEM_ALLOCATION_SCOPE_OBJECT means that the allocation is directly associated with a particular Vulkan object. This allocation will live at least until the object is destroyed. This type of allocation will only ever be made as part of executing a creation command (one beginning with vkCreate).
• VK_SYSTEM_ALLOCATION_SCOPE_CACHE means that the allocation is associated with some form of internal cache or a VkPipelineCache object.
• VK_SYSTEM_ALLOCATION_SCOPE_DEVICE means that the allocation is scoped to the device. This type of allocation is made when the Vulkan implementation needs memory associated with the device that is not tied to a single object. For example, if the implementation allocates objects in blocks, this type of allocation might be made in response to a request to create a new object, but because many objects might live in the same block, the allocation can’t be tied directly to any specific object.
• VK_SYSTEM_ALLOCATION_SCOPE_INSTANCE means that the allocation is scoped to the instance. This is similar to VK_SYSTEM_ALLOCATION_SCOPE_DEVICE. This type of allocation is typically made by layers or during early parts of Vulkan startup, such as by vkCreateInstance() and vkEnumeratePhysicalDevices().
The pfnInternalAllocation and pfnInternalFree function pointers point to alternate allocator functions that are used when Vulkan makes memory allocations using its own allocators. These callbacks have the same signatures as pfnAllocation and pfnFree, except that pfnInternalAllocation doesn’t return a value and pfnInternalFree shouldn’t actually free the memory. These functions are used only for notification so that your application can keep track of how much memory Vulkan is using. The prototypes of these functions should be
Click here to view code image

void VKAPI_CALL InternalAllocationNotification(

void*

pUserData,

size_t

size,

VkInternalAllocationType

allocationType,

VkSystemAllocationScope

allocationScope);

void VKAPI_CALL InternalFreeNotification( void* size_t VkInternalAllocationType VkSystemAllocationScope

pUserData, size, allocationType, allocationScope);

There’s not much you can do with the information provided through pfnInternalAllocation and pfnInternalFree besides log it and keep track of the total memory usage made by the application. Specifying these function pointers is optional, but if you supply one, you must supply both. If you don’t want to use them, set them both to nullptr.
Listing 2.1 shows an example of how to declare a C++ class that can be used as an allocator that maps the Vulkan allocation callback functions. Because the callback functions used by Vulkan are naked C function pointers, the callback functions themselves are declared as static member functions

of the class, whereas the actual implementations of those functions are declared as regular nonstatic member functions.

Listing 2.1: Declaration of a Memory Allocator Class

Click here to view code image
class allocator { public:
// Operator that allows an instance of this class to be used as a // VkAllocationCallbacks structure inline operator VkAllocationCallbacks() const {
VkAllocationCallbacks result;

result.pUserData = (void*)this; result.pfnAllocation = &Allocation; result.pfnReallocation = &Reallocation; result.pfnFree = &Free; result.pfnInternalAllocation = nullptr; result.pfnInternalFree = nullptr;

return result; };

private:

// Declare the allocator callbacks as static member functions.

static void* VKAPI_CALL Allocation(

void*

pUserData,

size_t

size,

size_t

alignment,

VkSystemAllocationScope

allocationScope);

static void* VKAPI_CALL Reallocation( void* void* size_t size_t VkSystemAllocationScope

pUserData, pOriginal, size, alignment, allocationScope);

static void VKAPI_CALL Free( void* void*

pUserData, pMemory);

// Now declare the nonstatic member functions that will actually perform
// the allocations. void* Allocation(

size_t size_t VkSystemAllocationScope

size, alignment, allocationScope);

void* Reallocation( void* size_t size_t VkSystemAllocationScope

pOriginal, size, alignment, allocationScope);

void Free( void*
};

pMemory);

An example implementation of this class is shown in Listing 2.2. It maps the Vulkan allocation functions to the POSIX aligned_malloc functions. Note that this allocator is almost certainly not better than what most Vulkan implementations use internally and serves only as an example of how to hook the callback functions up to your own code.

Listing 2.2: Implementation of a Memory Allocator Class

Click here to view code image
void* allocator::Allocation( size_t size_t VkSystemAllocationScope
{ return aligned_malloc(size, alignment);
}

size, alignment, allocationScope)

void* VKAPI_CALL allocator::Allocation(

void*

pUserData,

size_t

size,

size_t

alignment,

VkSystemAllocationScope

allocationScope)

{

return static_cast<allocator*>(pUserData)->Allocation(size,

alignment,

allocationScope);

}

void* allocator::Reallocation(

void*

pOriginal,

size_t

size,

size_t

alignment,

VkSystemAllocationScope

allocationScope)

{

return aligned_realloc(pOriginal, size, alignment);

}

void* VKAPI_CALL allocator::Reallocation(

void*

pUserData,

void*

pOriginal,

size_t

size,

size_t

alignment,

VkSystemAllocationScope

allocationScope)

{

return static_cast<allocator*>(pUserData)->Reallocation(pOriginal,

size,

alignment,

allocationScope);

}

void allocator::Free(

void*

pMemory)

{

aligned_free(pMemory);

}

void VKAPI_CALL allocator::Free(

void*

pUserData,

void*

pMemory)

{

return static_cast<allocator*>(pUserData)->Free(pMemory);

}

As can be seen in Listing 2.2, the static member functions simply cast the pUserData parameters back to a class instance and call the corresponding nonstatic member function. Because the nonstatic and static member functions are located in the same compilation unit, the nonstatic member function is likely to be inlined into the static one, making the efficiency of this implementation quite high.

Resources
Vulkan operates on data. Everything else is really secondary to this. Data is stored in resources, and resources are backed by memory. There are two fundamental types of resources in Vulkan: buffers and images. A buffer is a simple, linear chunk of data that can be used for almost anything—data structures, raw arrays, and even image data, should you choose to use them that way. Images, on the other hand, are structured and have type and format information, can be multidimensional, form arrays of their own, and support advanced operations for reading and writing data from and to them.
Both types of resources are constructed in two steps: first the resource itself is created, and then the resource needs to be backed by memory. The reason for this is to allow the application to manage memory itself. Memory management is complex, and it is very difficult for a driver to get it right all the time. What works well for one application might not work well for another. Therefore, it is expected that applications can do a better job of managing memory than drivers can. For example, an application that uses a small number of very large resources and keeps them around for a long time might use one strategy in its memory allocator, while another application that continually creates and destroys small resources might implement another.
Although images are more complex structures, the procedure for creating them is similar to buffers. This section looks at buffer creation first and then moves on to discuss images.

Buffers
Buffers are the simplest type of resource but have a wide variety of uses in Vulkan. They are used to store linear structured or unstructured data, which can have a format or be raw bytes in memory. The various uses for buffer objects will be discussed as we introduce those topics. To create a new buffer object, call vkCreateBuffer(), the prototype of which is

Click here to view code image

VkResult vkCreateBuffer ( VkDevice const VkBufferCreateInfo* const VkAllocationCallbacks* VkBuffer*

device, pCreateInfo, pAllocator, pBuffer);

As with most functions in Vulkan that consume more than a couple of parameters, those parameters are bundled up in a structure and passed to Vulkan via a pointer. Here, the pCreateInfo parameter is a pointer to an instance of the VkBufferCreateInfo structure, the definition of which is
Click here to view code image

typedef struct VkBufferCreateInfo {

VkStructureType

sType;

const void*

pNext;

VkBufferCreateFlags flags;

VkDeviceSize

size;

VkBufferUsageFlags

usage;

VkSharingMode

sharingMode;

uint32_t

queueFamilyIndexCount;

const uint32_t*

pQueueFamilyIndices;

} VkBufferCreateInfo;

The sType for VkBufferCreateInfo should be set to VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO, and the pNext member should be set to nullptr unless you’re using an extension. The flags field of the structure gives Vulkan some information about the properties of the new buffer. In the current version of Vulkan, the only bits defined for use in the flags field are related to sparse buffers, which we will cover later in this chapter. For now, flags can be set to zero.
The size field of VkBufferCreateInfo specifies the size of the buffer, in bytes. The usage field tells Vulkan how you’re going to use the buffer and is a bitfield made up of a combination of members of the VkBufferUsageFlagBits enumeration. On some architectures, the intended usage of the buffer can have an effect on how it’s created. The currently defined bits along with the sections where we’ll discuss them are as follows:
• VK_BUFFER_USAGE_TRANSFER_SRC_BIT and VK_BUFFER_USAGE_TRANSFER_DST_BIT mean that the buffer can be the source or destination, respectively, of transfer commands. Transfer operations are operations that copy data from a source to a destination. They are covered in Chapter 4, “Moving Data.”
• VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT and VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT mean that the buffer can be used to back a uniform or storage texel buffer, respectively. Texel buffers are formatted arrays of texels that can be used as the source or destination (in the case of storage buffers) of reads and writes by shaders running on the device. Texel buffers are covered in Chapter 6, “Shaders and Pipelines.”
• VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT and VK_BUFFER_USAGE_STORAGE_BUFFER_BIT mean that the buffer can be used to back uniform or storage buffers, respectively. As opposed to texel buffers, regular uniform and

storage buffers have no format associated with them and can therefore be used to store arbitrary data and data structures. They are covered in Chapter 6, “Shaders and Pipelines.”
• VK_BUFFER_USAGE_INDEX_BUFFER_BIT and VK_BUFFER_USAGE_VERTEX_BUFFER_BIT mean that the buffer can be used to store index or vertex data, respectively, used in drawing commands. You’ll learn more about drawing commands, including indexed drawing commands, in Chapter 8, “Drawing.”
• VK_BUFFER_USAGE_INDIRECT_BUFFER_BIT means that the buffer can be used to store parameters used in indirect dispatch and drawing commands, which are commands that take their parameters directly from buffers rather than from your program. These are covered in Chapter 6, “Shaders and Pipelines,” and Chapter 8, “Drawing.”
The sharingMode field of VkBufferCreateInfo indicates how the buffer will be used on the multiple command queues supported by the device. Because Vulkan can execute many operations in parallel, some implementations need to know whether the buffer will essentially be used by a single command at a time or potentially by many. Setting sharingMode to VK_SHARING_MODE_EXCLUSIVE says that the buffer will only be used on a single queue, whereas setting sharingMode to VK_SHARING_MODE_CONCURRENT indicates that you plan to use the buffer on multiple queues at the same time. Using VK_SHARING_MODE_CONCURRENT might result in lower performance on some systems, so unless you need this, set sharingMode to VK_SHARING_MODE_EXCLUSIVE.
If you do set sharingMode to VK_SHARING_MODE_CONCURRENT, you need to tell Vulkan which queues you’re going to use the buffer on. This is done using the pQueueFamilyIndices member of VkBufferCreateInfo, which is a pointer to an array of queue families that the resource will be used on. queueFamilyIndexCount contains the length of this array—the number of queue families that the buffer will be used with. When sharingMode is set to VK_SHARING_MODE_EXCLUSIVE, queueFamilyCount and pQueueFamilies are both ignored.
Listing 2.3 demonstrates how to create a buffer object that is 1MiB in size, usable as the source or destination of transfer operations, and used on only one queue family at a time.
Listing 2.3: Creating a Buffer Object
Click here to view code image
static const VkBufferCreateInfo bufferCreateInfo = {
VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO, nullptr, 0, 1024 * 1024, VK_BUFFER_USAGE_TRANSFER_SRC_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT, VK_SHARING_MODE_EXCLUSIVE, 0, nullptr };
VkBuffer buffer = VK_NULL_HANDLE;
vkCreateBuffer(device, &bufferCreateInfo, &buffer);

After the code in Listing 2.3 has run, a new VkBuffer handle is created and placed in the buffer variable. The buffer is not yet fully usable because it first needs to be backed with memory. This operation is covered in “Device Memory Management” later in this chapter.

Formats and Support
While buffers are relatively simple resources and do not have any notion of the format of the data they contain, images and buffer views (which we will introduce shortly) do include information about their content. Part of that information describes the format of the data in the resource. Some formats have special requirements or restrictions on their use in certain parts of the pipeline. For example, some formats might be readable but not writable, which is common with compressed formats.
In order to determine the properties and level of support for various formats, you can call vkGetPhysicalDeviceFormatProperties(), the prototype of which is
Click here to view code image

void vkGetPhysicalDeviceFormatProperties (

VkPhysicalDevice

physicalDevice,

VkFormat

format,

VkFormatProperties*

pFormatProperties);

Because support for particular formats is a property of a physical device rather than a logical one, the physical device handle is specified in physicalDevice. If your application absolutely required support for a particular format or set of formats, you could check for support before even creating the logical device and reject particular physical devices from consideration early in application startup, for example. The format for which to check support is specified in format. If the device recognizes the format, it will write its level of support into the instance of the VkFormatProperties structure pointed to by pFormatProperties. The definition of the VkFormatProperties structure is
Click here to view code image

typedef struct VkFormatProperties { VkFormatFeatureFlags linearTilingFeatures; VkFormatFeatureFlags optimalTilingFeatures; VkFormatFeatureFlags bufferFeatures;
} VkFormatProperties;

All three fields in the VkFormatProperties structure are bitfields made up from members of the VkFormatFeatureFlagBits enumeration. An image can be in one of two primary tiling modes: linear, in which image data is laid out linearly in memory, first by row, then by column, and so on; and optimal, in which image data is laid out in highly optimized patterns that make efficient use of the device’s memory subsystem. The linearTilingFeatures field indicates the level of support for a format in images in linear tiling, the optimalTilingFeatures field indicates the level of support for a format in images in optimal tiling, and the bufferFeatures field indicates the level of support for the format when used in a buffer.
The various bits that might be included in these fields are defined as follows:
• VK_FORMAT_FEATURE_SAMPLED_IMAGE_BIT: The format may be used in read-only images that will be sampled by shaders.

• VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_LINEAR_BIT: Filter modes that include linear filtering may be used when this format is used for a sampled image.
• VK_FORMAT_FEATURE_STORAGE_IMAGE_BIT: The format may be used in read-write images that will be read and written by shaders.
• VK_FORMAT_FEATURE_STORAGE_IMAGE_ATOMIC_BIT: The format may be used in read-write images that also support atomic operations performed by shaders.
• VK_FORMAT_FEATURE_UNIFORM_TEXEL_BUFFER_BIT: The format may be used in a read-only texel buffer that will be read from by shaders.
• VK_FORMAT_FEATURE_STORAGE_TEXEL_BUFFER_BIT: The format may be used in read-write texel buffers that may be read from and written to by shaders.
• VK_FORMAT_FEATURE_STORAGE_TEXEL_BUFFER_ATOMIC_BIT: The format may be used in read-write texel buffers that also support atomic operations performed by shaders.
• VK_FORMAT_FEATURE_VERTEX_BUFFER_BIT: The format may be used as the source of vertex data by the vertex-assembly stage of the graphics pipeline.
• VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT: The format may be used as a color attachment in the color-blend stage of the graphics pipeline.
• VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BLEND_BIT: Images with this format may be used as color attachments when blending is enabled.
• VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT: The format may be used as a depth, stencil, or depth-stencil attachment.
• VK_FORMAT_FEATURE_BLIT_SRC_BIT: The format may be used as the source of data in an image copy operation.
• VK_FORMAT_FEATURE_BLIT_DST_BIT: The format may be used as the destination of an image copy operation.
Many formats will have a number of format support bits turned on. In fact, many formats are compulsory to support. A complete list of the mandatory formats is contained in the Vulkan specification. If a format is on the mandatory list, then it’s not strictly necessary to test for support. However, for completeness, implementations are expected to accurately report capabilities for all supported formats, even mandatory ones.
The vkGetPhysicalDeviceFormatProperties() function really returns only a coarse set of flags indicating whether a format may be used at all under particular scenarios. For images especially, there may be more complex interactions between a specific format and its effect on the level of support within an image. Therefore, to retrieve even more information about the support for a format when used in images, you can call vkGetPhysicalDeviceImageFormatProperties(), the prototype of which is
Click here to view code image

VkResult vkGetPhysicalDeviceImageFormatProperties (

VkPhysicalDevice

physicalDevice,

VkFormat

format,

VkImageType

type,

VkImageTiling

tiling,

VkImageUsageFlags

usage,

VkImageCreateFlags VkImageFormatProperties*

flags, pImageFormatProperties);

Like vkGetPhysicalDeviceFormatProperties(), vkGetPhysicalDeviceImageFormatProperties() takes a VkPhysicalDevice handle as its first parameter and reports support for the format for the physical device rather than for a logical one. The format you’re querying support for is passed in format.
The type of image that you want to ask about is specified in type. This should be one of the image types: VK_IMAGE_TYPE_1D, VK_IMAGE_TYPE_2D, or VK_IMAGE_TYPE_3D. Different image types might have different restrictions or enhancements. The tiling mode for the image is specified in tiling and can be either VK_IMAGE_TILING_LINEAR or VK_IMAGE_TILING_OPTIMAL, indicating linear or optimal tiling, respectively.
The intended use for the image is specified in the usage parameter. This is a bitfield indicating how the image is to be used. The various uses for an image are discussed later in this chapter. The flags field should be set to the same value that will be used when creating the image that will use the format.
If the format is recognized and supported by the Vulkan implementation, then it will write information about the level of support into the VkImageFormatProperties structure pointed to by pImageFormatProperties. The definition of VkImageFormatProperties is
Click here to view code image

typedef struct VkImageFormatProperties {

VkExtent3D

maxExtent;

uint32_t

maxMipLevels;

uint32_t

maxArrayLayers;

VkSampleCountFlags sampleCounts;

VkDeviceSize

maxResourceSize;

} VkImageFormatProperties;

The extent member of VkImageFormatProperties reports the maximum size of an image that can be created with the specified format. For example, formats with fewer bits per pixel may support creating larger images than those with wider pixels. extent is an instance of the VkExtent3D structure, the definition of which is
Click here to view code image

typedef struct VkExtent3D { uint32_t width; uint32_t height; uint32_t depth;
} VkExtent3D;

The maxMipLevels field reports the maximum number of mipmap levels supported for an image of the requested format along with the other parameters passed to vkGetPhysicalDeviceImageFormatProperties(). In most cases, maxMipLevels will either report log2 (max (extent.x, extent.y, extent.z)) for the image when mipmaps are supported or 1 when mipmaps are not supported.

The maxArrayLayers field reports the maximum number of array layers supported for the image. Again, this is likely to be a fairly high number if arrays are supported or 1 if arrays are not supported.
If the image format supports multisampling, then the supported sample counts are reported through the sampleCounts field. This is a bitfield containing one bit for each supported sample count. If bit n is set, then images with 2n samples are supported in this format. If the format is supported at all, at least one bit of this field will be set. It is very unlikely that you will ever see a format that supports multisampling but does not support a single sample per pixel.
Finally, the maxResourceSize field specifies the maximum size, in bytes, that a resource of this format might be. This should not be confused with the maximum extent, which reports the maximum size in each of the dimensions that might be supported. For example, if an implementation reports that it supports images of 16,384 × 16,384 pixels × 2,048 layers with a format containing 128 bits per pixel, then creating an image of the maxium extent in every dimension would produce 8TiB of image data. It’s unlikely that an implementation really supports creating an 8TiB image. However, it might well support creating an 8 × 8 × 2,048 array or a 16,384 × 16,284 nonarray image, either of which would fit into a more moderate memory footprint.

Images
Images are more complex than buffers in that they are multidimensional; have specific layouts and format information; and can be used as the source and destination for complex operations such as filtering, blending, depth or stencil testing, and so on. Images are created using the vkCreateImage() function, the prototype of which is
Click here to view code image

VkResult vkCreateImage ( VkDevice const VkImageCreateInfo* const VkAllocationCallbacks* VkImage*

device, pCreateInfo, pAllocator, pImage);

The device that is used to create the image is passed in the device parameter. Again, the description of the image is passed through a data structure, the address of which is passed in the pCreateInfo parameter. This is a pointer to an instance of the VkImageCreateInfo structure, the definition of which is
Click here to view code image

typedef struct VkImageCreateInfo {

VkStructureType

sType;

const void*

pNext;

VkImageCreateFlags

flags;

VkImageType

imageType;

VkFormat

format;

VkExtent3D

extent;

uint32_t

mipLevels;

uint32_t

arrayLayers;

VkSampleCountFlagBits samples;

VkImageTiling

tiling;

VkImageUsageFlags

usage;

VkSharingMode

sharingMode;

uint32_t const uint32_t* VkImageLayout } VkImageCreateInfo;

queueFamilyIndexCount; pQueueFamilyIndices; initialLayout;

As you can see, this is a significantly more complex structure than the VkBufferCreateInfo structure. The common fields, sType and pNext, appear at the top, as with most other Vulkan structures. The sType field should be set to VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO.
The flags field of VkImageCreateInfo contains flags describing some of the properties of the image. These are a selection of the VkImageCreateFlagBits enumeration. The first three—VK_IMAGE_CREATE_SPARSE_BINDING_BIT, VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT, and VK_IMAGE_CREATE_SPARSE_ALIASED_BIT—are used for controlling sparse images, which are covered later in this chapter.
If VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT is set, then you can create views of the image with a different format from the parent. Image views are essentially a special type of image that shares data and layout with its parent but can override parameters such as format. This allows data in the image to be interpreted in multiple ways at the same time. Using image views is a way to create two different aliases for the same data. Image views are covered later in this chapter. If VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT is set, then the you will be able to create cube map views of it. Cube maps are covered later in this chapter.
The imageType field of the VkImageCreateInfo structure specifies the type of image that you want to create. The image type is essentially the dimensionality of the image and can be one of VK_IMAGE_TYPE_1D, VK_IMAGE_TYPE_2D, or VK_IMAGE_TYPE_3D for a 1D, 2D, or 3D image, respectively.
Images also have a format, which describes how texel data is stored in memory and how it is interpreted by Vulkan. The format of the image is specified by the format field of the VkImageCreateInfo structure and must be one of the image formats represented by a member of the VkFormat enumeration. Vulkan supports a large number of formats—too many to list here. We will use some of the formats in the book examples and explain how they work at that time. For the rest, refer to the Vulkan specification.
The extent of an image is its size in texels. This is specified in the extent field of the VkImageCreateInfo structure. This is an instance of the VkExtent3D structure, which has three members: width, height, and depth. These should be set to the width, height, and depth of the desired image, respectively. For 1D images, height should be set to 1, and for 1D and 2D images, depth should be set to 1. Rather than alias the next-higher dimension as an array count, Vulkan uses an explicit array size, which is set in arrayLayers.
The maximum size of an image that can be created is device-dependent. To determine the largest image size, call vkGetPhysicalDeviceFeatures() and check the maxImageDimension1D, maxImageDimension2D, and maxImageDimension3D fields of the embedded VkPhysicalDeviceLimits structure. maxImageDimension1D contains the maximum supported width for 1D images, maxImageDimension2D the maximum side length for 2D images, and maxImageDimension3D the maximum side length for 3D images. Likewise, the maximum number of layers in an array image is contained in the maxImageArrayLayers field.

If the image is a cube map, then the maximum side length for the cube is stored in maxImageDimensionCube. maxImageDimension1D, maxImageDimension2D, and maxImageDimensionCube are guaranteed to be at least 4,096 texels, and maxImageDimensionCube and maxImageArrayLayers are guaranteed to be at least 256. If the image you want to create is smaller than these dimensions, then there’s no need to check the device features. Further, it’s quite common to find Vulkan implementations that support significantly higher limits. It would be reasonable to make larger image sizes a hard requirement rather than trying to create fallback paths for lower-end devices. The number of mipmap levels to create in the image is specified in mipLevels. Mipmapping is the process of using a set of prefiltered images of successively lower resolution in order to improve image quality when undersampling the image. The images that make up the various mipmap levels are arranged in a pyramid, as shown in Figure 2.1.
Figure 2.1: Mipmap Image Layout
In a mipmapped texture, the base level is the lowest-numbered level (usually level zero) and has the resolution of the texture. Each successive level is half the size of the level above it until halving the size of the image again in one of the dimensions would result in a single texel in that direction. Sampling from mipmapped textures is covered in some detail in Chapter 6, “Shaders and Pipelines.” Likewise, the number of samples in the image is specified in samples. This field is somewhat unlike the others. It must be a member of the VkSampleCountFlagBits enumeration, which is actually defined as bits to be used in a bitfield. However, only power-of-two sample counts are currently defined in Vulkan, which means they’re “1-hot” values, so single-bit enumerant values work just fine. The next few fields describe how the image will be used. First is the tiling mode, specified in the tiling field. This is a member of the VkImageTiling enumeration, which contains only VK_IMAGE_TILING_LINEAR or VK_IMAGE_TILING_OPTIMAL. Linear tiling means that image data is laid out left to right, top to bottom,1 such that if you map the underlying memory and write it with the CPU, it would form a linear image. Meanwhile, optimal tiling is an opaque representation used by Vulkan to lay data out in memory to improve efficiency of the memory subsystem on the device. This is generally what you should choose unless you plan to map and manipulate the image with the CPU. Optimal tiling will likely perform significantly better than linear tiling in most operations, and linear tiling might not be supported at all for some operations or formats, depending on the Vulkan implementation.

1. Really, images don’t have a “top” or a “bottom.” They have a positive direction in their sampling coordinates. By convention, however, we call positive u “down,” making the texels at u = 1.0 the “bottom” of the image.
The usage field is a bitfield describing where the image will be used. This is similar to the usage field in the VkBufferCreateInfo structure. The usage field here is made up of members of the VkImageUsageFlags enumeration, the members of which are as follows:
• VK_IMAGE_USAGE_TRANSFER_SRC_BIT and VK_IMAGE_USAGE_TRANSFER_DST_BIT mean that the image will be the source or destination of transfer commands, respectively. Transfer commands operating on images are covered in Chapter 4, “Moving Data.”
• VK_IMAGE_USAGE_SAMPLED_BIT means that the image can be sampled from in a shader.
• VK_IMAGE_USAGE_STORAGE_BIT means that the image can be used for general-purpose storage, including writes from a shader.
• VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT means that the image can be bound as a color attachment and drawn into using graphics operations. Framebuffers and their attachments are covered in Chapter 7, “Graphics Pipelines.”
• VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT means that the image can be bound as a depth or stencil attachment and used for depth or stencil testing (or both). Depth and stencil operations are covered in Chapter 10, “Fragment Processing.”
• VK_IMAGE_USAGE_TRANSIENT_ATTACHMENT_BIT means that the image can be used as a transient attachment, which is a special kind of image used to store intermediate results of a graphics operation. Transient attachments are covered in Chapter 13, “Multipass Rendering.”
• VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT means that the image can be used as a special input during graphics rendering. Input images differ from regular sampled or storage images in that only fragment shaders can read from them and only at their own pixel location. Input attachments are also covered in detail in Chapter 13, “Multipass Rendering.”
The sharingMode is identical in function to the similarly named field in the VkBufferCreateInfo structure described in “Buffers” earlier in this chapter. If it is set to VK_SHARING_MODE_EXCLUSIVE, the image will be used with only a single queue family at a time. If it is set to VK_SHARING_MODE_CONCURRENT, then the image may be accessed by multiple queues concurrently. Likewise, queueFamilyIndexCount and pQueueFamilyIndices provide similar function and are used when sharingMode is VK_SHARING_MODE_CONCURRENT.
Finally, images have a layout, which specifies in part how it will be used at any given moment. The initialLayout field determines which layout the image will be created in. The available layouts are the members of the VkImageLayout enumeration, which are
• VK_IMAGE_LAYOUT_UNDEFINED: The state of the image is undefined. The image must be moved into one of the other layouts before it can be used almost for anything.
• VK_IMAGE_LAYOUT_GENERAL: This is the “lowest common denominator” layout and is used where no other layout matches the intended use case. Images in VK_IMAGE_LAYOUT_GENERAL can be used almost anywhere in the pipeline.
• VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL: The image is going to be rendered into using a graphics pipeline.

• VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL: The image is going to be used as a depth or stencil buffer as part of a graphics pipeline.
• VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL: The image is going to be used for depth testing but will not be written to by the graphics pipeline. In this special state, the image can also be read from in shaders.
• VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL: The image will be bound for reading by shaders. This layout is typically used when an image is going to be used as a texture.
• VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL: The image is the source of copy operations.
• VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL: The image is the destination of copy operations.
• VK_IMAGE_LAYOUT_PREINITIALIZED: The image contains data placed there by an external actor, such as by mapping the underlying memory and writing into it from the host.
• VK_IMAGE_LAYOUT_PRESENT_SRC_KHR: The image is used as the source for presentation, which is the act of showing it to the user.
Images can be moved from layout to layout, and we will cover the various layouts as we introduce the topics related to them. However, images must initially be created in either the VK_IMAGE_LAYOUT_UNDEFINED or the VK_IMAGE_LAYOUT_PREINITIALIZED layout. VK_IMAGE_LAYOUT_PREINITIALIZED should be used only when you have data in memory that you will bind to the image resource immediately. VK_IMAGE_LAYOUT_UNDEFINED should be used when you plan to move the resource to another layout before use. Images can be moved out of VK_IMAGE_LAYOUT_UNDEFINED layout at little or no cost at any time.
The mechanism for changing the layout of an image is known as a pipeline barrier, or simply a barrier. A barrier not only serves as a means to change the layout of a resource but can also synchronize access to that resource by different stages in the Vulkan pipeline and even by different queues running concurrently on the same device. As such, a pipeline barrier is fairly complex and quite difficult to get right. Pipeline barriers are discussed in some detail in Chapter 4, “Moving Data,” and are further explained in the sections of the book where they are relevant.
Listing 2.4 shows a simple example of creating an image resource.

Listing 2.4: Creating an Image Object

Click here to view code image
VkImage image = VK_NULL_HANDLE; VkResult result = VK_SUCCESS;

static const VkImageCreateInfo imageCreateInfo = {
VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO, nullptr, 0, VK_IMAGE_TYPE_2D, VK_FORMAT_R8G8B8A8_UNORM, { 1024, 1024, 1 },

// sType // pNext // flags // imageType // format // extent

10,

// mipLevels

1,

// arrayLayers

VK_SAMPLE_COUNT_1_BIT,

// samples

VK_IMAGE_TILING_OPTIMAL,

// tiling

VK_IMAGE_USAGE_SAMPLED_BIT,

// usage

VK_SHARING_MODE_EXCLUSIVE,

// sharingMode

0,

// queueFamilyIndexCount

nullptr,

// pQueueFamilyIndices

VK_IMAGE_LAYOUT_UNDEFINED

// initialLayout

};

result = vkCreateImage(device, &imageCreateInfo, nullptr, &image);

The image created by the code in Listing 2.4 is a 1,024 × 1,024 texel 2D image with a single sample, in VK_FORMAT_R8G8B8A8_UNORM format and optimal tiling. The code creates it in the undefined layout, which means that we can move it to another layout later to place data into it. The image is to be used as a texture in one of our shaders, so we set the VK_IMAGE_USAGE_SAMPLED_BIT usage flag. In our simple applications, we use only a single queue, so we set the sharing mode to exclusive.

Linear Images
As discussed earlier, two tiling modes are available for use in image resources: VK_IMAGE_TILING_LINEAR and VK_IMAGE_TILING_OPTIMAL. The VK_IMAGE_TILING_OPTIMAL mode represents an opaque, implementation-defined layout that is intended to improve the efficiency of the memory subsystem of the device for read and write operations on the image. However, VK_IMAGE_TILING_LINEAR is a transparent layout of the data that is intended to be intuitive. Pixels in the image are laid out left to right, top to bottom. Therefore, it’s possible to map the memory used to back the resource to allow the host to read and write to it directly.
In addition to the image’s width, height, depth, and pixel format, a few pieces of information are needed to enable host access to the underlying image data. These are the row pitch of the image, which is the distance in bytes between the start of each row of the image; the array pitch, which is the distance between array layers; and the depth pitch, which is the distance between depth slices. Of course, the array pitch and depth pitch apply only to array or 3D images, respectively, and the row pitch applies only to 2D or 3D images.
An image is normally made up of several subresources. Some formats have more than one aspect, which is a component of the image such as the depth or stencil component in a depth-stencil image. Mipmap levels and array layers are also considered to be separate subresources. The layout of each subresource within an image may be different and therefore has different layout information. This information can be queried by calling vkGetImageSubresourceLayout(), the prototype of which is
Click here to view code image

void vkGetImageSubresourceLayout ( VkDevice VkImage const VkImageSubresource* VkSubresourceLayout*

device, image, pSubresource, pLayout);

The device that owns the image that is being queried is passed in device, and the image being queried is passed in image. A description of the subresource is passed through an instance of the VkImageSubresource structure, a pointer to which is passed in the pSubresource parameter. The definition of VkImageSubresource is
Click here to view code image

typedef struct VkImageSubresource {

VkImageAspectFlags aspectMask;

uint32_t

mipLevel;

uint32_t

arrayLayer;

} VkImageSubresource;

The aspect or aspects of the image that you want to query the layout of is specified in aspectMask. For color images, this should be VK_IMAGE_ASPECT_COLOR_BIT, and for depth, stencil, or depth-stencil images, this should be some combination of VK_IMAGE_ASPECT_DEPTH_BIT, and VK_IMAGE_ASPECT_STENCIL_BIT. The mipmap level for which the parameters are to be returned is specified in mipLevel, and the array layer is specified in arrayLayer. You should normally set arrayLayer to zero, as the parameters of the image aren’t expected to change across layers.
When vkGetImageSubresourceLayout() returns, it will have written the layout parameters of the subresource into the VkSubresourceLayout structure pointed to by pLayout. The definition of VkSubresourceLayout is
Click here to view code image

typedef struct VkSubresourceLayout { VkDeviceSize offset; VkDeviceSize size; VkDeviceSize rowPitch; VkDeviceSize arrayPitch; VkDeviceSize depthPitch;
} VkSubresourceLayout;

The size of the memory region consumed by the requested subresource is returned in size, and the offset within the resource where the subresource begins is returned in offset. The rowPitch, arrayPitch, and depthPitch fields contain the row, array layer, and depth slice pitches, respectively. The unit of these fields is always bytes, regardless of the pixel format of the images. Pixels within a row are always tightly packed. Figure 2.2 illustrates how these parameters represent memory layout of an image. In the figure, the valid image data is represented by the grey grid, and padding around the image is shown as blank space.

Figure 2.2: Memory Layout of LINEAR Tiled Images
Given the memory layout of an image in LINEAR tiling mode, it is possible to trivially compute the memory address for a single texel within the image. Loading image data into a LINEAR tiled image is then simply a case of loading scanlines from the image into memory at the right location. For many texel formats and image dimensions, it is highly likely that the image’s rows are tightly packed in memory—that is, the rowPitch field of the VkSubresourceLayout structure is equal to the subresource’s width. In this case, many image-loading libraries will be able to load the image directly into the mapped memory of the image.

Nonlinear Encoding
You may have noticed that some of the Vulkan image formats include SRGB in their names. This refers to sRGB color encoding, which is a nonlinear encoding that uses a gamma curve approximating that of CRTs. Although CRTs are all but obsolete now, sRGB encoding is still in widespread use for texture and image data.
Because the amount of light energy produced by a CRT is not linear with the amount of electrical energy used to produce the electron beam that excites the phosphor, an inverse mapping must be applied to color signals to make a linear rise in numeric value produce a linear increase in light output. The amount of light output by a CRT is approximately
Lout = Vinγ
The standard value of γ in NTSC television systems (common in North America, parts of South America, and parts of Asia) is 2.2. Meanwhile, the standard value of γ in SECAM and PAL systems (common in Europe, Africa, Australia, and other regions of Asia) is 2.8.
The sRGB curve attempts to compensate for this by applying gamma correction to linear data in memory. The standard sRGB transfer function is not a pure gamma curve but is made up of a short linear section followed by a curved, gamma-corrected section. The function applied to data to go from linear to sRGB space is
Click here to view code image
if (cl >= 1.0) {
cs = 1.0; } else if (cl <= 0.0) {
cs = 0.0; } else if (cl < 0.0031308) {
cs = 12.92 * cl; } else {
cs = 1.055 * pow(cl, 0.41666) - 0.055; }
To go from sRGB space to linear space, the following transform is made:
Click here to view code image
if (cs >= 1.0) {
cl = 1.0; } else if (cs <= 0.0) {
cl = 0.0; }

else if (cs <= 0.04045) {
cl = cs / 12.92; } else {
cl = pow((cs + 0.0555) / 1.055), 2.4) }
In both code snippets, cs is the sRGB color space value, and cl is the linear value. Figure 2.3 shows a side-by-side comparison of a simple γ = 2.2 curve and the standard sRGB transfer function. As you can see in the figure, the curves for sRGB correction (shown on the top) and a simple power curve (shown on the bottom) are almost identical. While Vulkan implementations are expected to implement sRGB using the official definition, if you need to perform the transformation manually in your shaders, you may be able to get away with a simple power function without accumulating too much error.

Figure 2.3: Gamma Curves for sRGB (Top) and Simple Powers (Bottom)
When rendering to an image in sRGB format, linear values produced by your shaders are transformed to sRGB encoding before being written into the image. When reading from an image in sRGB format, texels are transformed from sRGB format back to linear space before being returned to your shader.
Blending and interpolation always occurs in linear space such that data read from a framebuffer is first transformed from sRGB to linear space and then blended with the source data in linear space, and the final result is transformed back to sRGB encoding before being written into the framebuffer.

Rendering in sRGB space provides more precision in darker colors and can result in less banding artifacts and richer colors. However, for best image quality, including high-dynamic-range rendering, it’s best to choose a floating-point color format and render in a linear space, converting to sRGB as late as possible before display.
Compressed Image Formats
Image resources are likely to be the largest consumers of device memory in your application. For this reason, Vulkan provides the capability for images to be compressed. Image compression provides two significant benefits to an application:
• It reduces the total amount of memory consumed by image resources used by the application.
• It reduces the total memory bandwidth consumed while accessing those resources.
All currently defined compressed image formats in Vulkan are what are known as block compressed formats. Texels within an image are compressed in small square or rectangular blocks that can be decompressed independently of all others. All formats are lossy, and the compression ratio is not competitive with formats such as JPEG or even PNG. However, decompression is fast and cheap to implement in hardware, and random access to texels is relatively straightforward.
Support for various compressed image formats is optional, but all Vulkan implementations are required to support at least one family of formats. You can determine which family of compressed formats is supported by checking various fields of the device’s VkPhysicalDeviceFeatures structure as returned from a call to vkGetPhysicalDeviceProperties().
If textureCompressionBC is VK_TRUE, then the device supports the block compressed formats, also known as BC formats. The BC family includes
• BC1: Made up of the VK_FORMAT_BC1_RGB_UNORM_BLOCK, VK_FORMAT_BC1_RGB_SRGB_BLOCK, VK_FORMAT_BC1_RGBA_UNORM_BLOCK, and VK_FORMAT_BC1_RGBA_SRGB_BLOCK formats, BC1 encodes images in blocks of 4 × 4 texels, with each block represented as a 64-bit quantity.
• BC2: Consisting of VK_FORMAT_BC2_UNORM_BLOCK and VK_FORMAT_BC2_SRGB_BLOCK, BC2 encodes images in blocks of 4 × 4 texels, with each block represented as a 128-bit quantity. BC2 images always have an alpha channel. The encoding for the RGB channels is the same as with BC1 RGB formats, and the alpha is stored as 4 bits per texel in a second 64-bit field before the BC1 encoded RGB data.
• BC3: The VK_FORMAT_BC3_UNORM_BLOCK and VK_FORMAT_BC3_SRGB_BLOCK formats make up the BC3 family, again encoding texels in 4 × 4 blocks, with each block consuming 128 bits of storage. The first 64-bit quantity stores compressed alpha values, allowing coherent alpha data to be stored with higher precision than BC2. The second 64-bit quantity stores compressed color data in a similar form to BC1.
• BC4: VK_FORMAT_BC4_UNORM_BLOCK and VK_FORMAT_BC4_SRGB_BLOCK represent single-channel formats, again encoded as 4 × 4 blocks of texels, with each block consuming 64 bits of storage. The encoding of the single-channel data is essentially the same as that of the alpha channel of a BC3 image.
• BC5: Made up of VK_FORMAT_BC5_UNORM_BLOCK and VK_FORMAT_BC5_SRGB_BLOCK, the BC5 family is a two-channel format, with each 4 × 4 block essentially consisting of two BC4 blocks back-to-back.

• BC6: The VK_FORMAT_BC6H_SFLOAT_BLOCK and VK_FORMAT_BC6H_UFLOAT_BLOCK formats are signed and unsigned floating-point compressed formats, respectively. Each 4 × 4 block of RGB texels is stored in 128 bits of data.
• BC7: VK_FORMAT_BC7_UNORM_BLOCK and VK_FORMAT_BC7_SRGB_BLOCK are fourchannel formats with each 4 × 4 block of RGBA texel data stored in a 128-bit component.
If the textureCompressionETC2 member of VkPhysicalDeviceFeatures is VK_TRUE, then the device supports the ETC formats, including ETC2 and EAC. The following formats are included in this family:
• VK_FORMAT_ETC2_R8G8B8_UNORM_BLOCK and VK_FORMAT_ETC2_R8G8B8_SRGB_BLOCK: Unsigned formats where 4 × 4 blocks of RGB texels are packed into 64 bits of compressed data.
• VK_FORMAT_ETC2_R8G8B8A1_UNORM_BLOCK and VK_FORMAT_ETC2_R8G8B8A1_SRGB_BLOCK: Unsigned formats where 4 × 4 blocks of RGB texels plus a one-bit alpha value per texel are packed into 64 bits of compressed data.
• VK_FORMAT_ETC2_R8G8B8A8_UNORM_BLOCK and VK_FORMAT_ETC2_R8G8B8A8_SRGB_BLOCK: Each 4 × 4 block of texels is represented as a 128-bit quantity. Each texel has 4 channels.
• VK_FORMAT_EAC_R11_UNORM_BLOCK and VK_FORMAT_EAC_R11_SNORM_BLOCK: Unsigned and signed single-channel formats with each 4 × 4 block of texels represented as a 64-bit quantity.
• VK_FORMAT_EAC_R11G11_UNORM_BLOCK and VK_FORMAT_EAC_R11G11_SNORM_BLOCK: Unsigned and signed two-channel formats with each 4 × 4 block of texels represented as a 64-bit quantity.
The final family is the ASTC family. If the textureCompressionASTC_LDR member of VkPhysicalDeviceFeatures is VK_TRUE, then the device supports the ASTC formats. You may have noticed that for all of the formats in the BC and ETC families, the block size is fixed at 4 × 4 texels, but depending on format, the texel format and number of bits used to store the compressed data vary.
ASTC is different here in that the number of bits per block is always 128, and all ASTC formats have four channels. However, the block size in texels can vary. The following block sizes are supported: 4 × 4, 5 × 4, 5 × 5, 6 × 5, 6 × 6, 8 × 5, 8 × 6, 8 × 8, 10 × 5, 10 × 6, 10 × 8, 10 × 10, 12 × 10, and 12 × 12.
The format of the token name for ASTC formats is formulated as VK_FORMAT_ASTC_{N}x{M}_{encoding}_BLOCK, where {N} and {M} represent the width and height of the block, and {encoding} is either UNORM or SRGB, depending on whether the data is linear or encoded as sRGB nonlinear. For example, VK_FORMAT_ASTC_8x6_SRGB_BLOCK is an RGBA ASTC compressed format with 8 × 6 blocks and sRGB encoded data.
For all formats including SRGB, only the R, G, and B channels use nonlinear encoding. The A channel is always stored with linear encoding.

Resource Views
Buffers and images are the two primary types of resources supported in Vulkan. In addition to creating these two resource types, you can create views of existing resources in order to partition them, reinterpret their content, or use them for multiple purposes. Views of buffers, which represent a subrange of a buffer object, are known as buffer views, and views of images, which can alias formats or represent a subresource of another image, are known as image views.
Before a view of a buffer or image can be created, you need to bind memory to the parent object.

Buffer Views
A buffer view is used to interpret the data in a buffer with a specific format. Because the raw data in the buffer is then treated as a sequence of texels, this is also known as a texel buffer view. A texel buffer view can be accessed directly in shaders, and Vulkan will automatically convert the texels in the buffer into the format expected by the shader. One example use for this functionality is to directly fetch the properties of vertices in a vertex shader by reading from a texel buffer rather than using a vertex buffer. While this is more restrictive, it does allow random access to the data in the buffer.
To create a buffer view, call vkCreateBufferView(), the prototype of which is
Click here to view code image

VkResult vkCreateBufferView ( VkDevice const VkBufferViewCreateInfo* const VkAllocationCallbacks* VkBufferView*

device, pCreateInfo, pAllocator, pView);

The device that is to create the new view is passed in device. This should be the same device that created the buffer of which you are creating a view. The remaining parameters of the new view are passed through a pointer to an instance of the VkBufferViewCreateInfo structure, the definition of which is
Click here to view code image

typedef struct VkBufferViewCreateInfo {

VkStructureType

sType;

const void*

pNext;

VkBufferViewCreateFlags flags;

VkBuffer

buffer;

VkFormat

format;

VkDeviceSize

offset;

VkDeviceSize

range;

} VkBufferViewCreateInfo;

The sType field of VkBufferViewCreateInfo should be set to VK_STRUCTURE_TYPE_BUFFER_VIEW_CREATE_INFO, and pNext should be set to nullptr. The flags field is reserved and should be set to 0. The parent buffer is specified in buffer. The new view will be a “window” into the parent buffer starting at offset bytes and extending for range bytes. When bound as a texel buffer, the data in the buffer is interpreted as a sequence of texels with the format as specified in format.

The maximum number of texels that can be stored in a texel buffer is determined by inspecting the maxTexelBufferElements field of the device’s VkPhysicalDeviceLimits structure, which can be retrieved by calling vkGetPhysicalDeviceProperties(). If the buffer is to be used as a texel buffer, then range divided by the size of a texel in format must be less than or equal to this limit. maxTexelBufferElements is guaranteed to be at least 65,536, so if the view you’re creating contains fewer texels, there’s no need to query this limit.
The parent buffer must have been created with the VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT or VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT flags in the usage field of the VkBufferCreateInfo used to create the buffer. The specified format must support the VK_FORMAT_FEATURE_UNIFORM_TEXEL_BUFFER_BIT, VK_FORMAT_FEATURE_STORAGE_TEXEL_BUFFER_BIT, or VK_FORMAT_FEATURE_STORAGE_TEXEL_BUFFER_ATOMIC_BIT as reported by vkGetPhysicalDeviceFormatProperties().
On success, vkCreateBufferView() places the handle to the newly created buffer view in the variable pointed to by pView. If pAllocator is not nullptr, then the allocation callbacks specified in the VkAllocationCallbacks structure it points to are used to allocate any host memory required by the new object.

Image Views
In many cases, the image resource cannot be used directly, as more information about it is needed than is included in the resource itself. For example, you cannot use an image resource directly as an attachment to a framebuffer or bind an image into a descriptor set in order to sample from it in a shader. To satisfy these additional requirements, you must create an image view, which is essentially a collection of properties and a reference to a parent image resource.
An image view also allows all or part of an existing image to be seen as a different format. The resulting view of the parent image must have the same dimensions as the parent, although a subset of the parent’s array layers or mip levels may be included in the view. The format of the parent and child images must also be compatible, which usually means that they have the same number of bits per pixel, even if the data formats are completely different and even if there are a different number of channels in the image.
To create a new view of an existing image, call vkCreateImageView(), the prototype of which is
Click here to view code image

VkResult vkCreateImageView ( VkDevice const VkImageViewCreateInfo* const VkAllocationCallbacks* VkImageView*

device, pCreateInfo, pAllocator, pView);

The device that will be used to create the new view and that should own the parent image is specified in device. The remaining parameters used in the creation of the new view are passed through an instance of the VkImageViewCreateInfo structure, a pointer to which is passed in pCreateInfo. The definition of VkImageViewCreateInfo is

Click here to view code image

typedef struct VkImageViewCreateInfo {

VkStructureType

sType;

const void*

pNext;

VkImageViewCreateFlags

flags;

VkImage

image;

VkImageViewType

viewType;

VkFormat

format;

VkComponentMapping

components;

VkImageSubresourceRange subresourceRange;

} VkImageViewCreateInfo;

The sType field of VkImageViewCreateInfo should be set to VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO, and pNext should be set to nullptr. The flags field is reserved for future use and should be set to 0.
The parent image of which to create a new view is specified in image. The type of view to create is specified in viewType. The view type must be compatible with the parent’s image type and is a member of the VkImageViewType enumeration, which is larger than the VkImageType enumeration used in creating the parent image. The image view types are as follows:
• VK_IMAGE_VIEW_TYPE_1D, VK_IMAGE_VIEW_TYPE_2D, and VK_IMAGE_VIEW_TYPE_3D are the “normal” 1D, 2D, and 3D image types.
• VK_IMAGE_VIEW_TYPE_CUBE and VK_IMAGE_VIEW_TYPE_CUBE_ARRAY are cube map and cube map array images.
• VK_IMAGE_VIEW_TYPE_1D_ARRAY and VK_IMAGE_VIEW_TYPE_2D_ARRAY are 1D and 2D array images.
Note that all images are essentially considered array images, even if they only have one layer. It is, however, possible to create nonarray views of parent images that refer to one of the layers of the image.
The format of the new view is specified in format. This must be a format that is compatible with that of the parent image. In general, if two formats have the same number of bits per pixel, then they are considered compatible. If either or both of the formats is a block compressed image format, then one of two things must be true:
• If both images have compressed formats, then the number of bits per block must match between those formats.
• If only one image is compressed and the other is not, then bits per block in the compressed image must be the same as the number of bits per texel in the uncompressed image.
By creating an uncompressed view of a compressed image, you give access to the raw, compressed data, making it possible to do things like write compressed data from a shader into the image or interpret the compressed data directly in your application. Note that while all block-compressed formats encode blocks either as 64-bit or 128-bit quantities, there are no uncompressed, singlechannel 64-bit or 128-bit image formats. To alias a compressed image as an uncompressed format, you need to choose an uncompressed format with the same number of bits per texel and then aggregate the bits from the different image channels within your shader to extract the individual fields from the compressed data.

The component ordering in the view may be different from that in the parent. This allows, for example, an RGBA view of a BGRA format image to be created. This remapping is specified using an instance of VkComponentMapping, the definition of which is simply
Click here to view code image

typedef struct VkComponentMapping { VkComponentSwizzle r; VkComponentSwizzle g; VkComponentSwizzle b; VkComponentSwizzle a;
} VkComponentMapping;

Each member of VkComponentMapping specifies the source of data in the parent image that will be used to fill the resulting texel fetched from the child view. They are members of the VkComponentSwizzle enumeration, the members of which are as follows:
• VK_COMPONENT_SWIZZLE_R, VK_COMPONENT_SWIZZLE_G, VK_COMPONENT_SWIZZLE_B, and VK_COMPONENT_SWIZZLE_A indicate that the source data should be read from the R, G, B, or A channels of the parent image, respectively.
• VK_COMPONENT_SWIZZLE_ZERO and VK_COMPONENT_SWIZZLE_ONE indicate that the data in the child image should be read as zero or one, respectively, regardless of the content of the parent image.
• VK_COMPONENT_SWIZZLE_IDENTITY indicates that the data in the child image should be read from the corresponding channel in the parent image. Note that the numeric value of VK_COMPONENT_SWIZZLE_IDENTITY is zero, so simply setting the entire VkComponentMapping structure to zero will result in an identity mapping between child and parent images.
The child image can be a subset of the parent image. This subset is specified using the embedded VkImageSubresourceRange structure in subresourceRange. The definition of VkImageSubresourceRange is
Click here to view code image

typedef struct VkImageSubresourceRange {

VkImageAspectFlags aspectMask;

uint32_t

baseMipLevel;

uint32_t

levelCount;

uint32_t

baseArrayLayer;

uint32_t

layerCount;

} VkImageSubresourceRange;

The aspectMask field is a bitfield made up from members of the VkImageAspectFlagBits enumeration specifying which aspects of the image are affected by the barrier. Some image types have more than one logical part, even though the data itself might be interleaved or otherwise related. An example of this is depth-stencil images, which have both a depth component and a stencil component. Each of these two components may be viewable as a separate image in its own right, and these subimages are known as aspects. The flags that can be included in aspectMask are
• VK_IMAGE_ASPECT_COLOR_BIT: The color part of an image. There is usually only a color aspect in color images.

• VK_IMAGE_ASPECT_DEPTH_BIT: The depth aspect of a depth-stencil image.
• VK_IMAGE_ASPECT_STENCIL_BIT: The stencil aspect of a depth-stencil image.
• VK_IMAGE_ASPECT_METADATA_BIT: Any additional information associated with the image that might track its state and is used, for example, in various compression techniques.
When you create the new view of the parent image, that view can refer to only one aspect of the parent image. Perhaps the most common use case of this is to create a depth- or stencil-only view of a combined depth-stencil format image.
To create a new image view that corresponds only to a subset of the parent’s mip chain, use the baseMipLevel and levelCount to specify where in the mip chain the view begins and how many mip levels it will contain. If the parent image does not have mipmaps, these fields should be set to zero and one, respectively.
Likewise, to create an image view of a subset of a parent’s array layers, use the baseArrayLayer and layerCount fields to specify the starting layer and number of layers, respectively. Again, if the parent image is not an array image, then baseArrayLayer should be set to zero and layerCount should be set to one.
Image Arrays
The defined image types (VkImageType) include only VK_IMAGE_TYPE_1D, VK_IMAGE_TYPE_2D, or VK_IMAGE_TYPE_3D, which are used to create 1D, 2D, and 3D images, respectively. However, in addition to their sizes in each of the x, y, and z dimensions, all images have a layer count, contained in the arrayLayers field of their VkImageCreateInfo structure.
Images can be aggregated into arrays, and each element of an array image is known as a layer. Array images allow images to be grouped into single objects, and sampling from multiple layers of the same array image is often more performant than sampling from several loose array objects. Because all Vulkan images have a layerCount field, they are all technically array images. However, in practice, we only refer to images with a layerCount greater than 1 as an array image.
When views are created of images, the view is explicitly marked as either an array or a nonarray. A nonarray view implicitly has only one layer whereas an array view has multiple layers. Sampling from a nonarray view may perform better than sampling from a single layer of an array image, simply because the device needs to perform fewer indirections and parameter lookups.
A 1D array texture is conceptually different from a 2D texture, and a 2D array texture is different from a 3D texture. The primary difference is that linear filtering can be performed in the y direction of a 2D texture and in the z direction in a 3D texture, whereas filtering cannot be performed across multiple layers in an array image. Notice that there is no 3D array image view type included in VkImageViewType, and most Vulkan implementations will not allow you to create a 3D image with an arrayLayers field greater than 1.
In addition to image arrays, a cube map is a special type of image that allows groups of six layers of an array image to be interpreted as the sides of a cube. Imagine standing in the center of a cubeshaped room. The room has four walls, a floor, and a ceiling. To your left and right are considered the negative and positive X directions, behind and in front of you are the negative and positive Z directions, and the floor and ceiling are the negative and positive Y directions. These faces are often

notated as the -X, +X, -Y, +Y, -Z, and +Z faces. These are the six faces of a cube map, and a group of six consecutive array layers can be interpreted in that order. A cube map is sampled using a 3D coordinate. This coordinate is interpreted as a vector pointing from the center of the cube map outward, and the point sampled in the cube-map is the point where the vector meets the cube. Again, put yourself back into the cube-map room and imagine you have a laser pointer. As you point the laser in different directions, the spot on the wall or ceiling is the point from which texture data is taken when the cube map is sampled. Figure 2.4 shows this pictorially. As you can see in the figure, the cube map is constructed from a selection of six consecutive elements from the parent texture. To create a cube-map view, first create a 2D array image with at least six faces. The imageType field of the VkImageCreateInfo structure should be set to VK_IMAGE_TYPE_2D and the arrayLayers field should be at least 6. Note that the number of layers in the parent array doesn’t have to be a multiple of 6, but it has to be at least 6.
Figure 2.4: Cube Map Construction The flags field of the parent image’s VkImageCreateInfo structure must have the VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT set, and the image must be square (because the faces of a cube are square). Next, we create a view of the 2D array parent, but rather than creating a normal 2D (array) view of the image, we create a cube-map view. To do this, set the viewType field of the VkImageViewCreateInfo structure used to create the view to VK_IMAGE_VIEW_TYPE_CUBE. In the embedded subresourceRange field, the

baseArrayLayer and layerCount fields are used to determine where in the array the cube map begins. To create a single cube, layerCount should be set to 6.
The first element of the array (at the index specified in the baseArrayLayer field) becomes the X face, and the next five layers become the +X, -Y, +Y, -Z, and +Z faces, in that order.
Cube maps can also form arrays of their own. This is simply a concatenation of an integer multiple of six faces, with each group of six forming a separate cube. To create a cube-map array image, set the viewType field of VkImageViewCreateInfo to VK_IMAGE_VIEW_TYPE_CUBE_ARRAY, and set the layerCount to a multiple of 6. The number of cubes in the array is therefore the layerCount for the array divided by 6. The number of layers in the parent image must be at least as many layers as are referenced by the cube-map view.
When data is placed in a cube map or cube-map array image, it is treated identically to an array image. Each array layer is laid out consecutively, and commands such as vkCmdCopyBufferToImage() (which is covered in Chapter 4, “Moving Data”) can be used to write into the image. The image can be bound as a color attachment and rendered to. Using layered rendering, you can even write to multiple faces of a cube map in a single drawing command.

Destroying Resources
When you are done with buffers, images, and other resources, it is important to destroy them cleanly. Before destroying a resource, you must make sure that it is not in use and that no work is pending that might access it. Once you are certain that this is the case, you can destroy the resource by calling the appropriate destruction function. To destroy a buffer resource, call vkDestroyBuffer(), the prototype of which is
Click here to view code image

void vkDestroyBuffer ( VkDevice VkBuffer const VkAllocationCallbacks*

device, buffer, pAllocator);

The device that owns the buffer object should be specified in device, and the handle to the buffer object should be specified in buffer. If a host memory allocator was used to create the buffer object, a compatible allocator should be specified in pAllocator; otherwise, pAllocator should be set to nullptr.
Note that destroying a buffer object for which other views exist will also invalidate those views. The view objects themselves must still be destroyed explicitly, but it is not legal to access a view of a buffer that has been destroyed. To destroy a buffer view, call vkDestroyBufferView(), the prototype of which is
Click here to view code image

void vkDestroyBufferView ( VkDevice VkBufferView const VkAllocationCallbacks*

device, bufferView, pAllocator);

Again, device is a handle to the device that owns the view, and bufferView is a handle to the view to be destroyed. pAllocator should point to a host memory allocator compatible with that used to create the view or should be set to nullptr if no allocator was used to create the view.
Destruction of images is almost identical to that of buffers. To destroy an image object, call vkDestroyImage(), the prototype of which is
Click here to view code image

void vkDestroyImage ( VkDevice VkImage const VkAllocationCallbacks*

device, image, pAllocator);

device is the device that owns the image to be destroyed, and image is the handle to that image. Again, if a host memory allocator was used to create the original image, then pAllocator should point to one compatible with it; otherwise, pAllocator should be nullptr.
As with buffers, destroying an image invalidates all views of that image. It is not legal to access a view of an image that has already been destroyed. The only thing you can do with such views is to destroy them. Destroying an image view is accomplished by calling vkDestroyImageView(), the prototype of which is
Click here to view code image

void vkDestroyImageView ( VkDevice VkImageView const VkAllocationCallbacks*

device, imageView, pAllocator);

As you might expect, device is the device that owns the view being destroyed, and imageView is the handle to that view. As with all other destruction functions mentioned so far, pAllocator is a pointer to an allocator compatible with the one used to create the view or nullptr if no allocator was used.

Device Memory Management
When the Vulkan device operates on data, the data must be stored in device memory. This is memory that is accessible to the device. In a Vulkan system there are four classes of memory. Some systems may have only a subset of these, and some may only have two. Given a host (the processor upon which your application is running) and a device (the processor that executes your Vulkan commands), there could be separate memory physically attached to each. In addition, some regions of the physical memory attached to each processor might be accessible to the other processor or processors in the system.
In some cases, the visible region of shared memory might be relatively small, and in other cases, there may actually be only one physical piece of memory, which is shared between the host and the device. Figure 2.5 demonstrates the memory map of a host and device with physically separate memories.

Figure 2.5: Host and Device Memory
Any memory that is accessible to the device is known as device memory, even if that memory is physically attached to the host. In this case, it is host local device memory. This is distinct from host memory, which might also be known as system memory, which is regular memory allocated with a function such as malloc or new. Device memory may also be accessible to the host through a mapping.
A typical discrete GPU as found on an add-in card plugged into a PCI-Express slot will have an amount of dedicated memory physically attached to its circuit board. Some part of this memory may be accessible only to the device, and some part of the memory may be accessible to the host through some form of window. In addition, the GPU will have access to some or all of the host’s system memory. All of these pools of memory will appear as a heap to the host, and memory will be mapped into those heaps via the various types of memory.
On the other hand, a typical embedded GPU—such as those found in embedded systems, mobile devices, or even laptop processors—may share memory controller and subsystem with the host processor. In this case, it is likely that access to main system memory is coherent and the device will expose fewer heaps—perhaps only one. This is considered a unified memory architecture.

Allocating Device Memory
A device memory allocation is represented as a VkDeviceMemory object that is created using the vkAllocateMemory() function, the prototype of which is
Click here to view code image

VkResult vkAllocateMemory ( VkDevice const VkMemoryAllocateInfo* const VkAllocationCallbacks* VkDeviceMemory*

device, pAllocateInfo, pAllocator, pMemory);

The device that will use the memory is passed in device. pAllocateInfo describes the new device memory object which, if the allocation is successful, will be placed in the variable pointed to by pMemory. pAllocateInfo points to an instance of the VkMemoryAllocateInfo structure, the definition of which is
Click here to view code image

typedef struct VkMemoryAllocateInfo {

VkStructureType sType;

const void*

pNext;

VkDeviceSize

allocationSize;

uint32_t

memoryTypeIndex;

} VkMemoryAllocateInfo;

This is a simple structure containing only the size and the memory type to be used for the allocation. sType should be set to VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO, and pNext should be set to nullptr unless an extension is in use that requires more information about the allocation. The size of the allocation is passed in allocationSize and is measured in bytes. The memory type, passed in memoryTypeIndex, is an index into the memory type array returned from a call to vkGetPhysicalDeviceMemoryProperties(), as described in “Physical Device Memory” in Chapter 1, “Overview of Vulkan.”
Once you have allocated device memory, it can be used to back resources such as buffers and images. Vulkan may use device memory for other purposes, such as other types of device objects, internal allocations and data structures, scratch storage, and so on. These allocations are managed by the Vulkan driver, as the requirements may vary quite widely between implementations.
When you are done with a memory allocation, you need to free it. To do this, call vkFreeMemory(), the prototype of which is
Click here to view code image

void vkFreeMemory ( VkDevice VkDeviceMemory const VkAllocationCallbacks*

device, memory, pAllocator);

vkFreeMemory() takes the memory object directly in memory. It is your responsibility to ensure that there is no work queued up to a device that might use the memory object before you free it. Vulkan will not track this for you. If a device attempts to access memory after it’s been freed, the results can be unpredictable and can easily crash your application.

Further, access to memory must be externally synchronized. Attempting to free device memory with a call to vkFreeMemory() while another command is executing in another thread will produce undefined behavior and possibly crash your application.
On some platforms, there may be an upper bound to the total number of memory allocations that can exist within a single process. If you try to create more allocations than this limit, allocation could fail. This limit can be determined by calling vkGetPhysicalDeviceProperties() and inspecting the maxMemoryAllocationCount field of the returned VkPhysicalDeviceLimits structure. The limit is guaranteed to be at least 4,096 allocations, though some platforms may report a much higher limit. Although this may seem low, the intention is that you create a small number of large allocations and then suballocate from them to place many resources in the same allocation. There is no upper limit to the total number of resources can be created, memory allowing.
Normally, when you allocate memory from a heap, that memory is permanently assigned to the returned VkDeviceMemory object until that object is destroyed by calling vkFreeMemory(). In some cases, you (or even the Vulkan implementation) may not know exactly how much memory is required for certain operations, or indeed whether any memory is required at all.
In particular, this is often the case for images that are used for intermediate storage of data during rendering. When the image is created, if the VK_IMAGE_USAGE_TRANSIENT_ATTACHMENT_BIT is included in the VkImageCreateInfo structure, then Vulkan knows that the data in the image will live for a short time, and therefore, it’s possible that it may never need to be written out to device memory.
In this case, you can ask Vulkan to be lazy with its allocation of the memory object to defer true allocation until Vulkan can determine that the physical storage for data is really needed. To do this, choose a memory type with the VK_MEMORY_PROPERTY_LAZILY_ALLOCATED_BIT set. Choosing an otherwise-appropriate memory type that does not have this bit set will still work correctly but will always allocate the memory up front, even if it never ends up being used.
If you want to know whether a memory allocation is physically backed and how much backing has actually been allocated for a memory object, call vkGetDeviceMemoryCommitment(), the prototype of which is
Click here to view code image

void vkGetDeviceMemoryCommitment ( VkDevice VkDeviceMemory VkDeviceSize*

device, memory, pCommittedMemoryInBytes);

The device that owns the memory allocation is passed in device and the memory allocation to query is passed in memory. pCommittedMemoryInBytes is a pointer to a variable that will be overwritten with the number of bytes actually allocated for the memory object. That commitment will always come from the heap associated with the memory type used to allocate the memory object.
For memory objects allocated with memory types that don’t include VK_MEMORY_PROPERTY_LAZILY_ALLOCATED_BIT, or if the memory object ended up fully committed, vkGetDeviceMemoryCommitment() will always return the full size of the memory object. The commitment returned from vkGetDeviceMemoryCommitment() is informational at best. In many cases, the information could be out of date, and there’s not much you can do with the information anyway.

Host Access to Device Memory
As discussed earlier in this chapter, device memory is divided into multiple regions. Pure device memory is accessible only to the device. However, there are regions of memory that are accessible to both the host and the device. The host is the processor upon which your application is running, and it is possible to ask Vulkan to give you a pointer to memory allocated from host-accessible regions. This is known as mapping memory.
To map device memory into the host’s address space, the memory object to be mapped must have been allocated from a heap that has the VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT flag set in its heap properties. Assuming that this is the case, mapping the memory to obtain a pointer usable by the host is achieved by calling vkMapMemory(), the prototype of which is
Click here to view code image

VkResult vkMapMemory ( VkDevice VkDeviceMemory VkDeviceSize VkDeviceSize VkMemoryMapFlags void**

device, memory, offset, size, flags, ppData);

The device that owns the memory object to be mapped is passed in device, and the handle to the memory object being mapped is passed in memory. Access to the memory object must be externally synchronized. To map a range of a memory object, specify the starting offset in offset and the size of the region in size. If you want to map the entire memory object, set offset to 0 and size to VK_WHOLE_SIZE. Setting offset to a nonzero value and size to VK_WHOLE_SIZE will map the memory object starting from offset to the end. offset and size are both specified in bytes. You should not attempt to map a region of the memory object that extends beyond its bounds.
The flags parameter is reserved for future use and should be set to zero.
If vkMapMemory() is successful, a pointer to the mapped region is written into the variable pointed to by ppData. This pointer can then be cast to the appropriate type in your application and dereferenced to directly read and write the device memory. Vulkan guarantees that pointers returned from vkMapMemory() are aligned to an integer multiple of the device’s minimum memory mapping alignment when offset is subtracted from them.
This value is reported in the minMemoryMapAlignment field of the VkPhysicalDeviceLimits structure returned from a call to vkGetPhysicalDeviceProperties(). It is guaranteed to be at least 64 bytes but could be any higher power of two. On some CPU architectures, much higher performance can be achieved by using memory load and store instructions that assume aligned addresses. minMemoryMapAlignment will often match a cache line size or the natural alignment of the machine’s widest register, for example, to facilitate this. Some host CPU instructions will fault if passed an unaligned address. Therefore, you can check minMemoryMapAlignment once and decide whether to use optimized functions that assume aligned addressing or fallback functions that can handle unaligned addresses at the expense of performance.
When you’re done with the pointer to the mapped memory range, it can be unmapped by calling vkUnmapMemory(), the prototype of which is

Click here to view code image

void vkUnmapMemory ( VkDevice VkDeviceMemory

device, memory);

The device that owns the memory object is passed in device, and the memory object to be unmapped is passed in memory. As with vkMapMemory(), access to the memory object must be externally synchronized.
It’s not possible to map the same memory object more than once at the same time. That is, you can’t call vkMapMemory() on the same memory object with different memory ranges, whether they overlap or not, without unmapping the memory object in between. The range isn’t needed when unmapping the object because Vulkan knows the range that was mapped.
As soon as the memory object is unmapped, any pointer received from a call to vkMapMemory() is invalid and should not be used. Also, if you map the same range of the same memory object over and over, you shouldn’t assume that the pointer you get back will be the same.
When device memory is mapped into host address space, there are effectively two clients of that memory, which may both perform writes into it. There is likely to be a cache hierarchy on both the host and the device sides of the mapping, and those caches may or may not be coherent. In order to ensure that both the host and the device see a coherent view of data written by the other client, it is necessary to force Vulkan to flush caches that might contain data written by the host but not yet made visible to the device or to invalidate a host cache that might hold stale data that has been overwritten by the device.
Each memory type advertised by the device has a number of properties, one of which might be VK_MEMORY_PROPERTY_HOST_COHERENT_BIT. If this is the case, and a mapping is made from a region with this property set, then Vulkan will take care of coherency between caches. In some cases, the caches are automatically coherent because they are either shared between host and device or have some form of coherency protocol to keep them in sync. In other cases, a Vulkan driver might be able to infer when caches need to be flushed or invalidated and then perform these operations behind the scenes.
If VK_MEMORY_PROPERTY_HOST_COHERENT_BIT is not set in the memory properties of a mapped memory region, then it is your responsibility to explicitly flush or invalidate caches that might be affected by the mapping. To flush host caches that might contain pending writes, call vkFlushMappedMemoryRanges(), the prototype of which is
Click here to view code image

VkResult vkFlushMappedMemoryRanges ( VkDevice uint32_t const VkMappedMemoryRange*

device, memoryRangeCount, pMemoryRanges);

The device that owns the mapped memory objects is specified in device. The number of ranges to flush is specified in memoryRangeCount, and the details of each range are passed in an instance of the VkMappedMemoryRange structure. A pointer to an array of memoryRangeCount of these structures is passed through the pMemoryRanges parameter. The definition of VkMappedMemoryRange is

Click here to view code image

typedef struct VkMappedMemoryRange {

VkStructureType sType;

const void*

pNext;

VkDeviceMemory

memory;

VkDeviceSize

offset;

VkDeviceSize

size;

} VkMappedMemoryRange;

The sType field of VkMappedMemoryRange should be set to VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE, and pNext should be set to nullptr. Each memory range refers to a mapped memory object specified in the memory field and a mapped range within that object, specified by offset and size. You don’t have to flush the entire mapped region of the memory object, so offset and size don’t need to match the parameters used in vkMapMemory(). Also, if the memory object is not mapped, or if offset and size specify a region of the object that isn’t mapped, then the flush command has no effect. To just flush any existing mapping on a memory object, set offset to zero and size to VK_WHOLE_SIZE.
A flush is necessary if the host has written to a mapped memory region and needs the device to see the effect of those writes. However, if the device writes to a mapped memory region and you need the host to see the effect of the device’s writes, you need to invalidate any caches on the host that might now hold stale data. To do this, call vkInvalidateMappedMemoryRanges(), the prototype of which is
Click here to view code image

VkResult vkInvalidateMappedMemoryRanges ( VkDevice uint32_t const VkMappedMemoryRange*

device, memoryRangeCount, pMemoryRanges);

As with vkFlushMappedMemoryRanges(), device is the device that owns the memory objects whose mapped regions are to be invalidated. The number of regions is specified in memoryRangeCount, and a pointer to an array of memoryRangeCount VkMappedMemoryRange structures is passed in pMemoryRanges. The fields of the VkMappedMemoryRange structures are interpreted exactly as they are in vkFlushMappedMemoryRanges(), except that the operation performed is an invalidation rather than a flush.
vkFlushMappedMemoryRanges() and vkInvalidateMappedMemoryRanges() affect only caches and coherency of access by the host and have no effect on the device. Regardless of whether a memory mapping is coherent or not, access by the device to memory that has been mapped must still be synchronized using barriers, which will be discussed later in this chapter.

Binding Memory to Resources
Before a resource such as a buffer or image can be used by Vulkan to store data, memory must be bound to it. Before memory is bound to a resource, you should determine what type of memory and how much of it the resource requires. There is a different function for buffers and for textures. They are vkGetBufferMemoryRequirements() and vkGetImageMemoryRequirements(), and their prototypes are
Click here to view code image

void vkGetBufferMemoryRequirements ( VkDevice VkBuffer VkMemoryRequirements*

device, buffer, pMemoryRequirements);

and
Click here to view code image

void vkGetImageMemoryRequirements ( VkDevice VkImage VkMemoryRequirements*

device, image, pMemoryRequirements);

The only difference between these two functions is that vkGetBufferMemoryRequirements() takes a handle to a buffer object and vkGetImageMemoryRequirements() takes a handle to an image object. Both functions return the memory requirements for the resource in an instance of the VkMemoryRequirements structure, the address of which is passed in the pMemoryRequirements parameter. The definition of VkMemoryRequirements is
Click here to view code image

typedef struct VkMemoryRequirements {

VkDeviceSize size;

VkDeviceSize alignment;

uint32_t

memoryTypeBits;

} VkMemoryRequirements;

The amount of memory needed by the resource is placed in the size field, and the alignment requirements of the object are placed in the alignment field. When you bind memory to the object (which we will get to in a moment), you need to ensure that the offset from the start of the memory object meets the alignment requirements of the resource and that there is sufficient space in the memory object to store the object.
The memoryTypeBits field is populated with all the memory types that the resource can be bound to. One bit is turned on, starting from the least significant bit, for each type that can be used with the resource. If you have no particular requirements for the memory, simply find the lowest-set bit and use its index to choose the memory type, which is then used as the memoryTypeIndex field in the allocation info passed to a call to vkAllocateMemory(). If you do have particular requirements or preferences for the memory—if you want to be able to map the memory or prefer that it be host local, for example—look for a type that includes those bits and is supported by the resource.

Listing 2.5 shows an example of an appropriate algorithm for choosing the memory type for an image resource.
Listing 2.5: Choosing a Memory Type for an Image
Click here to view code image
uint32_t application::chooseHeapFromFlags( const VkMemoryRequirements& memoryRequirements, VkMemoryPropertyFlags requiredFlags, VkMemoryPropertyFlags preferredFlags)
{ VkPhysicalDeviceMemoryProperties deviceMemoryProperties;
vkGetPhysicalDeviceMemoryProperties(m_physicalDevices[0], &deviceMemoryProperties);
uint32_t selectedType = ~0u; uint32_t memoryType;
for (memoryType = 0; memoryType < 32; ++memoryType) {
if (memoryRequirements.memoryTypeBits & (1 << memoryType)) {
const VkMemoryType& type = deviceMemoryProperties.memoryTypes[memoryType];
// If it exactly matches my preferred properties, grab it. if ((type.propertyFlags & preferredFlags) == preferredFlags) {
selectedType = memoryType; break; } } } if (selectedType != ~0u) { for (memoryType = 0; memoryType < 32; ++memoryType) { if (memoryRequirements.memoryTypeBits & (1 << memoryType)) { const VkMemoryType& type =
deviceMemoryProperties.memoryTypes[memoryType];
// If it has all my required properties, it'll do. if ((type.propertyFlags & requiredFlags) == requiredFlags) {
selectedType = memoryType; break; } } }

}

return selectedType; }

The algorithm shown in Listing 2.5 chooses a memory type given the memory requirements for an object, a set of hard requirements, and a set of preferred requirements. First, it iterates through the device’s supported memory types and checks each for the set of preferred flags. If there is a memory type that contains all of the flags that the caller prefers, then it immediately returns that memory type. If none of the device’s memory types exactly matches the preferred flags, then it iterates again, this time returning the first memory type that meets all of the requirements.
Once you have chosen the memory type for the resource, you can bind a piece of a memory object to that resource by calling either vkBindBufferMemory() for buffer objects or vkBindImageMemory() for image objects. Their prototypes are
Click here to view code image

VkResult vkBindBufferMemory ( VkDevice VkBuffer VkDeviceMemory VkDeviceSize

device, buffer, memory, memoryOffset);

and
Click here to view code image

VkResult vkBindImageMemory ( VkDevice VkImage VkDeviceMemory VkDeviceSize

device, image, memory, memoryOffset);

Again, these two functions are identical in declaration except that vkBindBufferMemory() takes a VkBuffer handle and vkBindImageMemory() takes a VkImage handle. In both cases, device must own both the resource and the memory object, whose handle is passed in memory. This is the handle of a memory allocation created through a call to vkAllocateMemory().
Access to buffer and image from vkBindBufferMemory() and vkBindImageMemory(), respectively, must be externally synchronized. Once memory has been bound to a resource object, the memory binding cannot be changed again. If two threads attempt to execute vkBindBufferMemory() or vkBindImageMemory() concurrently, then which thread’s binding takes effect and which one is invalid is subject to a race condition. Even resolving the race condition would not produce a legal command sequence, so this should be avoided.
The memoryOffset parameter specifies where in the memory object the resource will live. The amount of memory consumed by the object is determined from the size of the object’s requirements, as discovered with a call to vkGetBufferMemoryRequirements() or vkGetImageMemoryRequirements().
It is very strongly recommended that rather than simply creating a new memory allocation for each resource, you create a pool of a small number of relatively large memory allocations and place multiple resources in each one at different offsets. It is possible for two resources to overlap in

memory. In general, aliasing data like this is not well defined, but if you can be sure that two resources are not used at the same time, this can be a good way to reduce the memory requirements of your application.
An example of a device memory allocator is included with the book’s source code.

Sparse Resources
Sparse resources are a special type of resource that can be partially backed by memory and can have their memory backing changed after they have been created and even used in the application. A sparse resource must still be bound to memory before it can be used, although that binding can be changed. Additionally, an image or buffer can support sparse residency, which allows parts of the image to not be backed by memory at all.
To create a sparse image, set the VK_IMAGE_CREATE_SPARSE_BINDING_BIT in the flags field of the VkImageCreateInfo structure used to create the image. Likewise, to create a sparse buffer, set the VK_BUFFER_CREATE_SPARSE_BINDING_BIT in the flags field of the VkBufferCreateInfo structure used to create the buffer.
If an image was created with the VK_IMAGE_CREATE_SPARSE_BINDING_BIT bit set, your application should call vkGetImageSparseMemoryRequirements() to determine the additional requirements that the image needs. The prototype of vkGetImageSparseMemoryRequirements() is
Click here to view code image

void vkGetImageSparseMemoryRequirements ( VkDevice VkImage uint32_t* VkSparseImageMemoryRequirements*

device, image, pSparseMemoryRequirementCount, pSparseMemoryRequirements);

The device that owns the image should be passed in device, and the image whose requirements to query should be passed in image. The pSparseMemoryRequirements parameter points to an array of VkSparseImageMemoryRequirements structures that will be filled with the requirements of the image.
If pSparseMemoryRequirements is nullptr, then the initial value of the variable pointed to by pSparseMemoryRequirementCount is ignored and is overwritten with the number of requirements of the image. If pSparseMemoryRequirements is not nullptr, then the initial value of the variable pointed to by pSparseMemoryRequirementCount is the number of elements in the pSparseMemoryRequirements array and is overwritten with the number of requirements actually written to the array.
The definition of VkSparseImageMemoryRequirements is
Click here to view code image

typedef struct VkSparseImageMemoryRequirements {

VkSparseImageFormatProperties formatProperties;

uint32_t

imageMipTailFirstLod;

VkDeviceSize

imageMipTailSize;

VkDeviceSize

imageMipTailOffset;

VkDeviceSize

imageMipTailStride;

} VkSparseImageMemoryRequirements;

The first field of VkSparseImageMemoryRequirements is an instance of the VkSparseImageFormatProperties structure that provides general information about how the image is laid out in memory with respect to binding.
Click here to view code image

typedef struct VkSparseImageFormatProperties {

VkImageAspectFlags

aspectMask;

VkExtent3D

imageGranularity;

VkSparseImageFormatFlags flags;

} VkSparseImageFormatProperties;

The aspectMask field of VkSparseImageFormatProperties is a bitfield indicating the image aspects to which the properties apply. This will generally be all of the aspects in the image. For color images, it will be VK_IMAGE_ASPECT_COLOR_BIT, and for depth, stencil, and depth-stencil images, it will be either or both of VK_IMAGE_ASPECT_DEPTH_BIT and VK_IMAGE_ASPECT_STENCIL_BIT.
When memory is bound to a sparse image, it is bound in blocks rather than to the whole resource at once. Memory has to be bound in implementation-specific sized blocks, and the imageGranularity field of VkSparseImageFormatProperties contains this size.
Finally, the flags field contains some addional flags describing further behavior of the image. The flags that may be included are
• VK_SPARSE_IMAGE_FORMAT_SINGLE_MIPTAIL_BIT: If this bit is set and the image is an array, then the mip tail shares a binding shared by all array layers. If the bit is not set, then each array layer has its own mip tail that can be bound to memory independently of others.
• VK_SPARSE_IMAGE_FORMAT_ALIGNED_MIP_SIZE_BIT: If this bit is set, it is an indicator that the mip tail begins with the first level that is not a multiple of the image’s binding granularity. If the bit is not set, then the tail begins at the first level that is smaller than the image’s binding granularity.
• VK_SPARSE_IMAGE_FORMAT_NONSTANDARD_BLOCK_SIZE_BIT: If this bit is set, then the image’s format does support sparse binding, but not with the standard block sizes. The values reported in imageGranularity are still correct for the image but don’t necessarily match the standard block for the format.
Unless VK_SPARSE_IMAGE_FORMAT_NONSTANDARD_BLOCK_SIZE_BIT is set in flags, then the values in imageGranularity match a set of standard block sizes for the format. The size, in texels, of various formats is shown in Table 2.1.
The remaining fields of VkSparseImageMemoryRequirements describe how the format used by the image behaves in the mip tail. The mip tail is the region of the mipmap chain beginning from the first level that cannot be sparsely bound to memory. This is typically the first level that is smaller than the size of the format’s granularity. As memory must be bound to sparse resources in units of the granularity, the mip tail presents an all-or-nothing binding opportunity. Once any level of the mipmap’s tail is bound to memory, all levels within the tail become bound.

Table 2.1: Sparse Texture Block Sizes

The mip tail begins at the level reported in the imageMipTailFirstLod field of VkSparseImageMemoryRequirements. The size of the tail, in bytes, is contained in imageMipTailSize, and it begins at imageMipTailOffset bytes into the image’s memory binding region. If the image does not have a single mip tail binding for all array layers (as indicated by the presence of VK_SPARSE_IMAGE_FORMAT_SINGLE_MIPTAIL_BIT in the aspectMask field of VkSparseImageFormatProperties), then imageMipTailStride is the distance, in bytes, between the start of the memory binding for each mip tail level.
The properties of a specific format can also be determined by calling vkGetPhysicalDeviceSparseImageFormatProperties(), which, given a specific format, will return a VkSparseImageFormatProperties describing that format’s sparse image requirements without the need to create an image and query it. The prototype of vkGetPhysicalDeviceSparseImageFormatProperties() is
Click here to view code image

void vkGetPhysicalDeviceSparseImageFormatProperties (

VkPhysicalDevice

physicalDevice,

VkFormat

format,

VkImageType

type,

VkSampleCountFlagBits

samples,

VkImageUsageFlags

usage,

VkImageTiling

tiling,

uint32_t*

pPropertyCount,

VkSparseImageFormatProperties*

pProperties);

As you can see, vkGetPhysicalDeviceSparseImageFormatProperties() takes as parameters many of the properties that would be used to construct an image. Sparse image properties are a function of a physical device, a handle to which should be passed in physicalDevice. The format of the image is passed in format, and the type of image (VK_IMAGE_TYPE_1D, VK_IMAGE_TYPE_2D, or VK_IMAGE_TYPE_3D) is passed in type. If multisampling is required, the number of samples (represented as one of the members of the VkSampleCountFlagBits enumeration) is passed in samples.
The intended usage for the image is passed in usage. This should be a bitfield containing the flags specifying how an image with this format will be used. Be aware that sparse images may not be supported at all under certain use cases, so it’s best to set this field conservatively and accurately rather than just turning on every bit and hoping for the best. Finally, the tiling mode to be used for the image is specified in tiling. Again, standard block sizes may be supported only in certain tiling

modes. For example, it’s very unlikely that an implementation would support standard (or even reasonable) block sizes when LINEAR tiling is used.
Just as with vkGetPhysicalDeviceImageFormatProperties(), vkGetPhysicalDeviceSparseImageFormatProperties() can return an array of properties. The pPropertyCount parameter points to a variable that will be overwritten with the number of properties reported for the format. If pProperties is nullptr, then the initial value of the variable pointed to by pPropertyCount is ignored and the total number of properties is written into it. If pProperties is not nullptr, then it should be a pointer to an array of VkSparseImageFormatProperties structures that will receive the properties of the image. In this case, the initial value of the variable pointed to by pPropertyCount is the number of elements in the array, and it is overwritten with the number of items populated in the array.
Because the memory binding used to back sparse images can be changed, even after the image is in use, the update to the binding properties of the image is pipelined along with that work. Unlike vkBindImageMemory() and vkBindBufferMemory(), which are operations likely carried out by the host, memory is bound to a sparse resource using an operation on the queue, allowing the device to execute them. The command to bind memory to a sparse resource is vkQueueBindSparse(), the prototype of which is
Click here to view code image

VkResult vkQueueBindSparse ( VkQueue uint32_t const VkBindSparseInfo* VkFence

queue, bindInfoCount, pBindInfo, fence);

The queue that will execute the binding operation is specified in queue. Several binding operations can be performed by a single call to vkQueueBindSparse(). The number of operations to perform is passed in bindInfoCount, and pBindInfo is a pointer to an array of bindInfoCount VkBindSparseInfo structures, each describing one of the bindings. The definition of VkBindSparseInfo is
Click here to view code image

typedef struct VkBindSparseInfo { VkStructureType const void* uint32_t const VkSemaphore* uint32_t const VkSparseBufferMemoryBindInfo* uint32_t const VkSparseImageOpaqueMemoryBindInfo* uint32_t const VkSparseImageMemoryBindInfo* uint32_t const VkSemaphore*
} VkBindSparseInfo;

sType; pNext; waitSemaphoreCount; pWaitSemaphores; bufferBindCount; pBufferBinds; imageOpaqueBindCount; pImageOpaqueBinds; imageBindCount; pImageBinds; signalSemaphoreCount; pSignalSemaphores;

The act of binding memory to sparse resources is actually pipelined with other work performed by the device. As you read in Chapter 1, “Overview of Vulkan,” work is performed by submitting it to queues. The binding is then performed along with the execution of commands submitted to the same queue. Because vkQueueBindSparse() behaves a lot like a command submission, VkBindSparseInfo contains many fields related to synchronization.
The sType field of VkBindSparseInfo should be set to VK_STRUCTURE_TYPE_BIND_SPARSE_INFO, and pNext should be set to nullptr. As with VkSubmitInfo, each sparse binding operation can optionally wait for one or more semaphores to be signaled before performing the operation and can signal one or more semaphores when it is done. This allows updates to the sparse resource’s bindings to be synchronized with other work performed by the device.
The number of semaphores to wait on is specified in waitSemaphoreCount, and the number of semaphores to signal is specified in signalSemaphoreCount. The pWaitSemaphores field is a pointer to an array of waitSemaphoreCount semaphore handles to wait on, and pSignalSemaphores is a pointer to an array of signalSemaphoreCount semaphores to signal. Semaphores are covered in some detail in Chapter 11, “Synchronization.”
Each binding operation can include updates to buffers and images. The number of buffer binding updates is specified in bufferBindCount and pBufferBinds is a pointer to an array of bufferBindCount VkSparseBufferMemoryBindInfo structures, each describing one of the buffer memory binding operations. The definition of VkSparseBufferMemoryBindInfo is
Click here to view code image

typedef struct VkSparseBufferMemoryBindInfo {

VkBuffer

buffer;

uint32_t

bindCount;

const VkSparseMemoryBind* pBinds;

} VkSparseBufferMemoryBindInfo;

Each instance of VkSparseBufferMemoryBindInfo contains the handle of the buffer to which memory will be bound. A number of regions of memory can be bound to the buffer at different offsets. The number of memory regions is specified in bindCount, and each binding is described by an instance of the VkSparseMemoryBind structure. pBinds is a pointer to an array of bindCount VkSparseMemoryBind structures. The definition of VkSparseMemoryBind is
Click here to view code image

typedef struct VkSparseMemoryBind {

VkDeviceSize

resourceOffset;

VkDeviceSize

size;

VkDeviceMemory

memory;

VkDeviceSize

memoryOffset;

VkSparseMemoryBindFlags } VkSparseMemoryBind;

flags;

The size of the block of memory to bind to the resource is contained in size. The offsets of the block in the resource and in the memory object are contained in resourceOffset and memoryOffset, respectively, and are both expressed in units of bytes. The memory object that is

the source of storage for the binding is specified in memory. When the binding is executed, the block of memory, size bytes long and starting at memoryOffset bytes into the memory object specified in memory, will be bound into the buffer specified in the buffer field of the VkSparseBufferMemoryBindInfo structure.
The flags field contains additional flags that can be used to further control the binding. No flags are used for buffer resources. However, image resources use the same VkSparseMemoryBind structure to affect memory bindings directly to images. This is known as an opaque image memory binding, and the opaque image memory bindings to be performed are also passed through the VkBindSparseInfo structure. The pImageOpaqueBinds member of VkBindSparseInfo points to an array of imageOpaqueBindCount VkSparseImageOpaqueMemoryBindInfo structures defining the opaque memory bindings. The definition of VkSparseImageOpaqueMemoryBindInfo is
Click here to view code image

typedef struct VkSparseImageOpaqueMemoryBindInfo {

VkImage

image;

uint32_t

bindCount;

const VkSparseMemoryBind* pBinds;

} VkSparseImageOpaqueMemoryBindInfo;

Just as with VkSparseBufferMemoryBindInfo, VkSparseImageOpaqueMemoryBindInfo contains a handle to the image to which to bind memory in image and a pointer to an array of VkSparseMemoryBind structures in pBinds, which is bindCount elements long. This is the same structure used for buffer memory bindings. However, when this structure is used for images, you can set the flags field of each VkSparseMemoryBind structure to include the VK_SPARSE_MEMORY_BIND_METADATA_BIT flag in order to explicitly bind memory to the metadata associated with the image.
When memory is bound opaquely to a sparse image, the blocks of memory have no defined correlation with texels in the image. Rather, the backing store of the image is treated as a large, opaque region of memory with no information about how texels are laid out in it provided to the application. However, so long as memory is bound to the entire image when it is used, results will still be well-defined and consistent. This allows sparse images to be backed by multiple, smaller memory objects, potentially easing pool allocation strategies, for example.
To bind memory to an explicit region of an image, you can perform a nonopaque image memory binding by passing one or more VkSparseImageMemoryBindInfo structures through the VkBindSparseInfo structures passed to vkQueueBindSparse(). The definition of VkSparseImageMemoryBindInfo is
Click here to view code image

typedef struct VkSparseImageMemoryBindInfo {

VkImage

image;

uint32_t

bindCount;

const VkSparseImageMemoryBind* pBinds;

} VkSparseImageMemoryBindInfo;

Again, the VkSparseImageMemoryBindInfo structure contains a handle to the image to which to bind memory in image, a count of the number of bindings to perform in bindCount, and a pointer to an array of structures describing the bindings in pBinds. This time, however, pBinds points to an array of bindCount VkSparseImageMemoryBind structures, the definition of which is
Click here to view code image

typedef struct VkSparseImageMemoryBind {

VkImageSubresource

subresource;

VkOffset3D

offset;

VkExtent3D

extent;

VkDeviceMemory

memory;

VkDeviceSize

memoryOffset;

VkSparseMemoryBindFlags flags;

} VkSparseImageMemoryBind;

The VkSparseImageMemoryBind structure contains much more information about how the memory is to be bound to the image resource. For each binding, the image subresource to which the memory is to be bound is specified in subresource, which is an instance of the VkImageSubresource, the definition of which is
Click here to view code image

typedef struct VkImageSubresource {

VkImageAspectFlags aspectMask;

uint32_t

mipLevel;

uint32_t

arrayLayer;

} VkImageSubresource;

The VkImageSubresource allows you to specify the aspect of the image (VK_IMAGE_ASPECT_COLOR_BIT, VK_IMAGE_ASPECT_DEPTH_BIT, or VK_IMAGE_ASPECT_STENCIL_BIT, for example) in aspectMask, the mipmap level to which you want to bind memory in mipLevel, and the array layer where the memory should be bound in arrayLayer. For nonarray images, arrayLayer should be set to zero.
Within the subresource, the offset and extent fields of the VkSparseImageMemoryBind structure define the offset and size of the region of texels to bind the image data to. This must be aligned to the tile-size boundaries, which are either the standard sizes as shown in Table 2.1 or the per-format block size that can be retrieved from vkGetPhysicalDeviceSparseImageFormatProperties().
Again, the memory object from which to bind memory is specified in memory, and the offset within the memory where the backing store resides is specified in memoryOffset. The same flags are available in the flags field of VkSparseImageMemoryBind.

Summary
This chapter introduced you to the different types of resources that are used by Vulkan. It described how the memory used to back them is allocated and then associated with them. It also explained how you can manage the application memory used by Vulkan through the use of a custom allocator. You have seen how to move resources from state to state and how to synchronize acess to them through pipeline barriers. This enables efficient, parallel access to resources both from multiple stages of the Vulkan pipeline and from the host.

Chapter 3. Queues and Commands

What You’ll Learn in This Chapter • What a queue is and how to use it • How to create commands and send them to Vulkan • How to ensure that a device has finished processing your work

Vulkan devices expose multiple queues that perform work. In this chapter, we discuss the various queue types and explain how to submit work to them in the form of command buffers. We also show how to instruct a queue to complete all of the work you’ve sent it.

Device Queues
Each device in Vulkan has one or more queues. The queue is the part of the device that actually performs work. It can be thought of as a subdevice that exposes a subset of the device’s functionality. In some implementations, each queue may even be a physically separate part of the system.
Queues are grouped into one or more queue families, each containing one or more queues. Queues within a single family are essentially identical. Their capabilities are the same, their performance level and access to system resources is the same, and there is no cost (beyond synchronization) of transferring work between them. If a device contains multiple cores that have the same capabilities but differ in performance, access to memory, or some other factor that might mean they can’t operate identically, it may expose them in separate families that otherwise appear identical.
As discussed in Chapter 1, “Overview of Vulkan,” you can query the properties of each of a physical device’s queue families by calling vkGetPhysicalDeviceQueueFamilyProperties(). This function writes the properties of the queue family into an instance of the VkQueueFamilyProperties structure that you hand it.
The number and type of queues that you wish to use must be specified when you create the device. As you saw in Chapter 1, “Overview of Vulkan,” the VkDeviceCreateInfo structure that you pass to vkCreateDevice() contains the queueCreateInfoCount and pQueueCreateInfos members. Chapter 1, “Overview of Vulkan,” glossed over them, but now it’s time to fill them in. The queueCreateInfoCount member contains the number of VkDeviceQueueCreateInfo structures stored in the array pointed to by pQueueCreateInfos. The definition of the VkDeviceQueueCreateInfo structure is
Click here to view code image

typedef struct VkDeviceQueueCreateInfo {

VkStructureType

sType;

const void*

pNext;

VkDeviceQueueCreateFlags flags;

uint32_t

queueFamilyIndex;

uint32_t

queueCount;

const float*

pQueuePriorities;

} VkDeviceQueueCreateInfo;

As with most Vulkan structures, the sType field is the structure type, which in this case should be VK_STRUCTURE_TYPE_QUEUE_CREATE_INFO, and the pNext field is used for extensions and should be set to nullptr when none are used. The flags field contains flags controlling queue construction, but no flag is defined for use in the current version of Vulkan, so this field should be set to zero.
The fields of interest here are queueFamilyIndex and queueCount. The queueFamilyIndex field specifies the family from which you want to allocate queues, and the queueCount field specifies the number of queues to allocate from that family. To allocate queues from multiple families, simply pass an array of more than one VkDeviceQueueCreateInfo structure in the pQueueCreateInfos member of the VkDeviceCreateInfo structure.
The queues are constructed when the device is created. For this reason, we don’t create queues, but obtain them from the device. To do this, call vkGetDeviceQueue():
Click here to view code image

void vkGetDeviceQueue ( VkDevice uint32_t uint32_t VkQueue*

device, queueFamilyIndex, queueIndex, pQueue);

The vkGetDeviceQueue() function takes as arguments the device from which you want to obtain the queue, the family index, and the index of the queue within that family. These are specified in device, queueFamilyIndex, and queueIndex, respectively. The pQueue parameter points to the VkQueue handle that is to be filled with the handle to the queue. queueFamilyIndex and queueIndex must refer to a queue that was initialized when the device was created. If they do, a queue handle is placed into the variable pointed to by pQueue; otherwise, this variable is set to VK_NULL_HANDLE.

Creating Command Buffers
The primary purpose of a queue is to process work on behalf of your application. Work is represented as a sequence of commands that are recorded into command buffers. Your application will create command buffers containing the work it needs to do and submit them to one of the queues for execution. Before you can record any commands, you need to create a command buffer. Command buffers themselves are not created directly, but allocated from pools. To create a pool, call vkCreateCommandPool(), whose prototype is
Click here to view code image

VkResult vkCreateCommandPool ( VkDevice const VkCommandPoolCreateInfo* const VkAllocationCallbacks* VkCommandPool*

device, pCreateInfo, pAllocator, pCommandPool);

As with most Vulkan object creation functions, the first parameter, device, is the handle to the device that will own the new pool object, and a description of the pool is passed via a structure, a pointer to which is placed in pCreateInfo. This structure is an instance of VkCommandPoolCreateInfo, the definition of which is

Click here to view code image

typedef struct VkCommandPoolCreateInfo {

VkStructureType

sType;

const void*

pNext;

VkCommandPoolCreateFlags flags;

uint32_t

queueFamilyIndex;

} VkCommandPoolCreateInfo;

As with most Vulkan structures, the first two fields, sType and pNext, contain the structure type and a pointer to another structure containing more information about the pool to be created. Here, we’ll set sType to VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO and, because we’re not passing any extra information, set pNext to nullptr.
The flags field contains flags that determine the behavior of the pool and the command buffers that are allocated from it. These are members of the VkCommandPoolCreateFlagBits enumeration, and there are currently two flags defined for use here.
• Setting the VK_COMMAND_POOL_CREATE_TRANSIENT_BIT indicates that command buffers taken from the pool will be short-lived and returned to the pool shortly after use. Not setting this bit suggests to Vulkan that you might keep the command buffers around for some time.
• Setting the VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT allows individual command buffers to be reused by resetting them or restarting them. (Don’t worry, we’ll cover that in a moment.) If this bit is not specified, then only the pool itself can be reset, which implicitly recycles all of the command buffers allocated from it.
Each of these bits may add some overhead to the work done by a Vulkan implementation to track the resources or otherwise alter its allocation strategy. For example, setting VK_COMMAND_POOL_CREATE_TRANSIENT_BIT may cause a Vulkan implementation to employ a more advanced allocation strategy for the pool in order to avoid fragmentation as command buffers are frequently allocated and then returned to it. Setting VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT may cause the implementation to track the reset status of each command buffer rather than simply track it at the pool level.
In this case, we’re actually going to set both bits. This gives us the most flexibility, possibly at the expense of some performance in cases where we could have managed command buffers in bulk.
Finally, the queueFamilyIndex field of VkCommandPoolCreateInfo specifies the family of queues to which command buffers allocated from this pool will be submitted. This is necessary because even where two queues on a device have the same capabilities and support the same set of commands, issuing a particular command to one queue may work differently from issuing that same command to another queue.
The pAllocator parameter is used for application-managed host memory allocations, which is covered in Chapter 2, “Memory and Resources.” Assuming successful creation of the command pool, its handle will be written into the variable pointed to by pCommandPool, and vkCreateCommandPool() will return VK_SUCCESS.
Once we have a pool from which to allocate command buffers, we can grab new command buffers by calling vkAllocateCommandBuffers(), which is defined as

Click here to view code image

VkResult vkAllocateCommandBuffers ( VkDevice const VkCommandBufferAllocateInfo* VkCommandBuffer*

device, pAllocateInfo, pCommandBuffers);

The device used to allocate the command buffers is passed in device, and the remaining parameters describing the command buffers to allocate are passed in an instance of the VkCommandBufferAllocateInfo structure, the address of which is passed in pCommandBuffers. The definition of VkCommandBufferAllocateInfo is
Click here to view code image

typedef struct VkCommandBufferAllocateInfo {

VkStructureType

sType;

const void*

pNext;

VkCommandPool

commandPool;

VkCommandBufferLevel level;

uint32_t

commandBufferCount;

} VkCommandBufferAllocateInfo;

The sType field should be set to VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO, and as we’re using only the core feature set here, we set the pNext parameter to nullptr. A handle to the command pool that we created earlier is placed into the commandPool parameter.
The level parameter specifies the level of the command buffers that we want to allocate. It can be set to either VK_COMMAND_BUFFER_LEVEL_PRIMARY or VK_COMMAND_BUFFER_LEVEL_SECONDARY. Vulkan allows primary command buffers to call secondary command buffers. For our first few examples, we will use only primary-level command buffers. We’ll cover secondary-level command buffers later in the book.
Finally, commandBufferCount specifies the number of command buffers that we want to allocate from the pool. Note that we don’t tell Vulkan anything about the length or size of the command buffers we’re creating. The internal data structures representing device commands will generally vary too greatly for any unit of measurement, such as bytes or commands, to make much sense. Vulkan will manage the command buffer memory for you.
If vkAllocateCommandBuffers() is successful, it will return VK_SUCCESS and place the handles to the allocated command buffers in the array pointed to by pCommandBuffers. This array should be big enough to hold all the handles. Of course, if you want to allocate only a single command buffer, you can point this at a regular VkCommandBuffer handle.
To free command buffers, we use the vkFreeCommandBuffers() command, which is declared as
Click here to view code image

void vkFreeCommandBuffers ( VkDevice VkCommandPool uint32_t const VkCommandBuffer*

device, commandPool, commandBufferCount, pCommandBuffers);

The device parameter is the device that owns the pool from which the command buffers were allocated. commandPool is a handle to that pool, commandBufferCount is the number of command buffers to free, and pCommandBuffers is a pointer to an array of commandBufferCount handles to the command buffers to free. Note that freeing a command buffer doesn’t necessarily free all of the resources associated with it but returns them to the pool from which they were allocated.
To free all of the resources used by a command pool and all of the command buffers allocated from it, call vkDestroyCommandPool(), the prototype of which is
Click here to view code image

void vkDestroyCommandPool ( VkDevice VkCommandPool const VkAllocationCallbacks*

device, commandPool, pAllocator;

The device that owns the command pool is passed in the device parameter, and a handle to the command pool to destroy is passed in commandPool. A pointer to a host memory allocation structure compatible with the one used to create the pool is passed in pAllocator. This parameter should be nullptr if the pAllocator parameter to vkCreateCommandPool() was also nullptr.
There is no need to explicitly free all of the command buffers allocated from a pool before destroying the pool. The command buffers allocated from the pool are all freed as a part of destroying the pool and freeing its resources. Care should be taken, however, that no command buffers allocated from the pool are still executing or queued for execution on the device when vkDestroyCommandPool() is called.

Recording Commands
Commands are recorded into command buffers using Vulkan command functions, all of which take a command buffer handle as their first parameter. Access to the command buffer must be externally synchronized, meaning that it is the responsibility of your application to ensure that no two threads simultaneously attempt to record commands into the same command buffer at the same time. However, the following is perfectly acceptable:
• One thread can record commands into multiple command buffers by simply calling command buffer functions on different command buffers in succession.
• Two or more threads can participate in building a single command buffer, so long as the application can guarantee that no two of them are ever executing a command buffer building function concurrently.
One of the key design principles of Vulkan is to enable efficient multithreading. To achieve this, it is important that your application’s threads do not block each other’s execution by, for example, taking a mutex to protect a shared resource. For this reason, it’s best to have one or more command buffers for each thread rather than to try sharing one. Further, as command buffers are allocated from pools, you can go further and create a command pool for each thread, allowing command buffers to be allocated by your worker threads from their respective pools without interacting.

