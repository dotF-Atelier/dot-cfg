Real-Time Rendering
Fourth Edition

Real-Time Rendering
Fourth Edition
Tomas Akenine-M¨oller Eric Haines Naty Hoﬀman Angelo Pesce Michal Iwanicki S´ebastien Hillaire

CRC Press Taylor & Francis Group 6000 Broken Sound Parkway NW, Suite 300 Boca Raton, FL 33487-2742
c 2018 by Taylor & Francis Group, LLC CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed on acid-free paper
International Standard Book Number-13: 978-1-1386-2700-0 (Hardback)
This book contains information obtained from authentic and highly regarded sources. Reasonable eﬀorts have been made to publish reliable data and information, but the author and publisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microﬁlming, and recording, or in any information storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-proﬁt organization that provides licenses and registration for a variety of users. For organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for identiﬁcation and explanation without intent to infringe.
Library of Congress Cataloging-in-Publication Data
Names: M¨oller, Tomas, 1971- author. Title: Real-time rendering / Tomas Akenine-M¨oller, Eric Haines, Naty Hoﬀman, Angelo Pesce, Michal Iwanicki, S´ebastien Hillaire Description: Fourth edition. | Boca Raton : Taylor & Francis, CRC Press, 2018. Identiﬁers: LCCN 2018009546 | ISBN 9781138627000 (hardback : alk. paper) Subjects: LCSH: Computer graphics. | Real-time data processing. | Rendering (Computer graphics) Classiﬁcation: LCC T385 .M635 2018 | DDC 006.6/773--dc23 LC record available at https://lccn.loc.gov/2018009546
Visit the Taylor & Francis Web site at http://www.taylorandfrancis.com
and the CRC Press Web site at http://www.crcpress.com

Dedicated to Eva, Felix, and Elina
T. A-M.
Dedicated to Cathy, Ryan, and Evan
E. H.
Dedicated to Dorit, Karen, and Daniel
N. H.
Dedicated to Fei, Clelia, and Alberto
A. P.
Dedicated to Aneta and Weronika
M. I.
Dedicated to St´ephanie and Svea
S. H.

Contents

Preface

xiii

1 Introduction

1

1.1 Contents Overview . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

1.2 Notation and Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . .

5

2 The Graphics Rendering Pipeline

11

2.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.2 The Application Stage . . . . . . . . . . . . . . . . . . . . . . . . . 13

2.3 Geometry Processing . . . . . . . . . . . . . . . . . . . . . . . . . . 14

2.4 Rasterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2.5 Pixel Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

2.6 Through the Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . 25

3 The Graphics Processing Unit

29

3.1 Data-Parallel Architectures . . . . . . . . . . . . . . . . . . . . . . 30

3.2 GPU Pipeline Overview . . . . . . . . . . . . . . . . . . . . . . . . 34

3.3 The Programmable Shader Stage . . . . . . . . . . . . . . . . . . . 35

3.4 The Evolution of Programmable Shading and APIs . . . . . . . . . 37

3.5 The Vertex Shader . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

3.6 The Tessellation Stage . . . . . . . . . . . . . . . . . . . . . . . . . 44

3.7 The Geometry Shader . . . . . . . . . . . . . . . . . . . . . . . . . 47

3.8 The Pixel Shader . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

3.9 The Merging Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

3.10 The Compute Shader . . . . . . . . . . . . . . . . . . . . . . . . . . 54

4 Transforms

57

4.1 Basic Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

4.2 Special Matrix Transforms and Operations . . . . . . . . . . . . . . 70

4.3 Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76

4.4 Vertex Blending . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

4.5 Morphing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

4.6 Geometry Cache Playback . . . . . . . . . . . . . . . . . . . . . . . 92

4.7 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

vii

viii

Contents

5 Shading Basics

103

5.1 Shading Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

5.2 Light Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106

5.3 Implementing Shading Models . . . . . . . . . . . . . . . . . . . . . 117

5.4 Aliasing and Antialiasing . . . . . . . . . . . . . . . . . . . . . . . . 130

5.5 Transparency, Alpha, and Compositing . . . . . . . . . . . . . . . . 148

5.6 Display Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160

6 Texturing

167

6.1 The Texturing Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . 169

6.2 Image Texturing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176

6.3 Procedural Texturing . . . . . . . . . . . . . . . . . . . . . . . . . . 198

6.4 Texture Animation . . . . . . . . . . . . . . . . . . . . . . . . . . . 200

6.5 Material Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201

6.6 Alpha Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202

6.7 Bump Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208

6.8 Parallax Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214

6.9 Textured Lights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221

7 Shadows

223

7.1 Planar Shadows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225

7.2 Shadows on Curved Surfaces . . . . . . . . . . . . . . . . . . . . . . 229

7.3 Shadow Volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230

7.4 Shadow Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234

7.5 Percentage-Closer Filtering . . . . . . . . . . . . . . . . . . . . . . 247

7.6 Percentage-Closer Soft Shadows . . . . . . . . . . . . . . . . . . . . 250

7.7 Filtered Shadow Maps . . . . . . . . . . . . . . . . . . . . . . . . . 252

7.8 Volumetric Shadow Techniques . . . . . . . . . . . . . . . . . . . . 257

7.9 Irregular Z -Buﬀer Shadows . . . . . . . . . . . . . . . . . . . . . . 259

7.10 Other Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . 262

8 Light and Color

267

8.1 Light Quantities . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267

8.2 Scene to Screen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281

9 Physically Based Shading

293

9.1 Physics of Light . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293

9.2 The Camera . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307

9.3 The BRDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308

9.4 Illumination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315

9.5 Fresnel Reﬂectance . . . . . . . . . . . . . . . . . . . . . . . . . . . 316

9.6 Microgeometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327

9.7 Microfacet Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 331

Contents

ix

9.8 BRDF Models for Surface Reﬂection . . . . . . . . . . . . . . . . . 336 9.9 BRDF Models for Subsurface Scattering . . . . . . . . . . . . . . . 347 9.10 BRDF Models for Cloth . . . . . . . . . . . . . . . . . . . . . . . . 356 9.11 Wave Optics BRDF Models . . . . . . . . . . . . . . . . . . . . . . 359 9.12 Layered Materials . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363 9.13 Blending and Filtering Materials . . . . . . . . . . . . . . . . . . . 365

10 Local Illumination

375

10.1 Area Light Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . 377

10.2 Environment Lighting . . . . . . . . . . . . . . . . . . . . . . . . . 391

10.3 Spherical and Hemispherical Functions . . . . . . . . . . . . . . . . 392

10.4 Environment Mapping . . . . . . . . . . . . . . . . . . . . . . . . . 404

10.5 Specular Image-Based Lighting . . . . . . . . . . . . . . . . . . . . 414

10.6 Irradiance Environment Mapping . . . . . . . . . . . . . . . . . . . 424

10.7 Sources of Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433

11 Global Illumination

437

11.1 The Rendering Equation . . . . . . . . . . . . . . . . . . . . . . . . 437

11.2 General Global Illumination . . . . . . . . . . . . . . . . . . . . . . 441

11.3 Ambient Occlusion . . . . . . . . . . . . . . . . . . . . . . . . . . . 446

11.4 Directional Occlusion . . . . . . . . . . . . . . . . . . . . . . . . . . 465

11.5 Diﬀuse Global Illumination . . . . . . . . . . . . . . . . . . . . . . 472

11.6 Specular Global Illumination . . . . . . . . . . . . . . . . . . . . . 497

11.7 Uniﬁed Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . 509

12 Image-Space Eﬀects

513

12.1 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513

12.2 Reprojection Techniques . . . . . . . . . . . . . . . . . . . . . . . . 522

12.3 Lens Flare and Bloom . . . . . . . . . . . . . . . . . . . . . . . . . 524

12.4 Depth of Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527

12.5 Motion Blur . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536

13 Beyond Polygons

545

13.1 The Rendering Spectrum . . . . . . . . . . . . . . . . . . . . . . . . 545

13.2 Fixed-View Eﬀects . . . . . . . . . . . . . . . . . . . . . . . . . . . 546

13.3 Skyboxes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547

13.4 Light Field Rendering . . . . . . . . . . . . . . . . . . . . . . . . . 549

13.5 Sprites and Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . 550

13.6 Billboarding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551

13.7 Displacement Techniques . . . . . . . . . . . . . . . . . . . . . . . . 564

13.8 Particle Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567

13.9 Point Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572

13.10 Voxels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 578

x

Contents

14 Volumetric and Translucency Rendering

589

14.1 Light Scattering Theory . . . . . . . . . . . . . . . . . . . . . . . . 589

14.2 Specialized Volumetric Rendering . . . . . . . . . . . . . . . . . . . 600

14.3 General Volumetric Rendering . . . . . . . . . . . . . . . . . . . . . 605

14.4 Sky Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613

14.5 Translucent Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . 623

14.6 Subsurface Scattering . . . . . . . . . . . . . . . . . . . . . . . . . . 632

14.7 Hair and Fur . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640

14.8 Uniﬁed Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . 648

15 Non-Photorealistic Rendering

651

15.1 Toon Shading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 652

15.2 Outline Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . 654

15.3 Stroke Surface Stylization . . . . . . . . . . . . . . . . . . . . . . . 669

15.4 Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673

15.5 Text Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675

16 Polygonal Techniques

681

16.1 Sources of Three-Dimensional Data . . . . . . . . . . . . . . . . . . 682

16.2 Tessellation and Triangulation . . . . . . . . . . . . . . . . . . . . . 683

16.3 Consolidation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 690

16.4 Triangle Fans, Strips, and Meshes . . . . . . . . . . . . . . . . . . . 696

16.5 Simpliﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 706

16.6 Compression and Precision . . . . . . . . . . . . . . . . . . . . . . . 712

17 Curves and Curved Surfaces

717

17.1 Parametric Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . 718

17.2 Parametric Curved Surfaces . . . . . . . . . . . . . . . . . . . . . . 734

17.3 Implicit Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 749

17.4 Subdivision Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . 753

17.5 Subdivision Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . 756

17.6 Eﬃcient Tessellation . . . . . . . . . . . . . . . . . . . . . . . . . . 767

18 Pipeline Optimization

783

18.1 Proﬁling and Debugging Tools . . . . . . . . . . . . . . . . . . . . . 784

18.2 Locating the Bottleneck . . . . . . . . . . . . . . . . . . . . . . . . 786

18.3 Performance Measurements . . . . . . . . . . . . . . . . . . . . . . 788

18.4 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 790

18.5 Multiprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 805

Contents

xi

19 Acceleration Algorithms

817

19.1 Spatial Data Structures . . . . . . . . . . . . . . . . . . . . . . . . 818

19.2 Culling Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 830

19.3 Backface Culling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 831

19.4 View Frustum Culling . . . . . . . . . . . . . . . . . . . . . . . . . 835

19.5 Portal Culling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837

19.6 Detail and Small Triangle Culling . . . . . . . . . . . . . . . . . . . 839

19.7 Occlusion Culling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840

19.8 Culling Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 850

19.9 Level of Detail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 852

19.10 Rendering Large Scenes . . . . . . . . . . . . . . . . . . . . . . . . 866

20 Eﬃcient Shading

881

20.1 Deferred Shading . . . . . . . . . . . . . . . . . . . . . . . . . . . . 883

20.2 Decal Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 888

20.3 Tiled Shading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 892

20.4 Clustered Shading . . . . . . . . . . . . . . . . . . . . . . . . . . . 898

20.5 Deferred Texturing . . . . . . . . . . . . . . . . . . . . . . . . . . . 905

20.6 Object- and Texture-Space Shading . . . . . . . . . . . . . . . . . . 908

21 Virtual and Augmented Reality

915

21.1 Equipment and Systems Overview . . . . . . . . . . . . . . . . . . 916

21.2 Physical Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . 919

21.3 APIs and Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . 924

21.4 Rendering Techniques . . . . . . . . . . . . . . . . . . . . . . . . . 932

22 Intersection Test Methods

941

22.1 GPU-Accelerated Picking . . . . . . . . . . . . . . . . . . . . . . . 942

22.2 Deﬁnitions and Tools . . . . . . . . . . . . . . . . . . . . . . . . . . 943

22.3 Bounding Volume Creation . . . . . . . . . . . . . . . . . . . . . . 948

22.4 Geometric Probability . . . . . . . . . . . . . . . . . . . . . . . . . 953

22.5 Rules of Thumb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 954

22.6 Ray/Sphere Intersection . . . . . . . . . . . . . . . . . . . . . . . . 955

22.7 Ray/Box Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . 959

22.8 Ray/Triangle Intersection . . . . . . . . . . . . . . . . . . . . . . . 962

22.9 Ray/Polygon Intersection . . . . . . . . . . . . . . . . . . . . . . . 966

22.10 Plane/Box Intersection . . . . . . . . . . . . . . . . . . . . . . . . . 970

22.11 Triangle/Triangle Intersection . . . . . . . . . . . . . . . . . . . . . 972

22.12 Triangle/Box Intersection . . . . . . . . . . . . . . . . . . . . . . . 974

22.13 Bounding-Volume/Bounding-Volume Intersection . . . . . . . . . . 976

22.14 View Frustum Intersection . . . . . . . . . . . . . . . . . . . . . . . 981

22.15 Line/Line Intersection . . . . . . . . . . . . . . . . . . . . . . . . . 987

22.16 Intersection between Three Planes . . . . . . . . . . . . . . . . . . 990

xii

Contents

23 Graphics Hardware 23.1 Rasterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23.2 Massive Compute and Scheduling . . . . . . . . . . . . . . . . . . . 23.3 Latency and Occupancy . . . . . . . . . . . . . . . . . . . . . . . . 23.4 Memory Architecture and Buses . . . . . . . . . . . . . . . . . . . 23.5 Caching and Compression . . . . . . . . . . . . . . . . . . . . . . . 23.6 Color Buﬀering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23.7 Depth Culling, Testing, and Buﬀering . . . . . . . . . . . . . . . . 23.8 Texturing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23.9 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23.10 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23.11 Ray Tracing Architectures . . . . . . . . . . . . . . . . . . . . . . .

993 993 1002 1004 1006 1007 1009 1014 1017 1019 1024 1039

24 The Future

1041

24.1 Everything Else . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1042

24.2 You . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1046

Bibliography

1051

Index

1155

Preface
“Things have not changed that much in the past eight years,” was our thought entering into this fourth edition. “How hard could it be to update the book?” A year and a half later, and with three more experts recruited, our task is done. We could probably spend another year editing and elaborating, at which time there would be easily a hundred more articles and presentations to fold in. As a data point, we made a Google Doc of references that is more than 170 pages long, with about 20 references and related notes on each page. Some references we cite could and do each take up a full section in some other book. A few of our chapters, such as that on shadows, have entire books dedicated to their subjects. While creating more work for us, this wealth of information is good news for practitioners. We will often point to these primary sources, as they oﬀer much more detail than appropriate here.
This book is about algorithms that create synthetic images fast enough that the viewer can interact with a virtual environment. We have focused on three-dimensional rendering and, to a limited extent, on the mechanics of user interaction. Modeling, animation, and many other areas are important to the process of making a real-time application, but these topics are beyond the scope of this book.
We expect you to have some basic understanding of computer graphics before reading this book, as well as knowledge of computer science and programming. We also focus on algorithms, not APIs. Many texts are available on these other subjects. If some section does lose you, skim on through or look at the references. We believe that the most valuable service we can provide you is a realization of what you yet do not know about—a basic kernel of an idea, a sense of what others have discovered about it, and ways to learn more, if you wish.
We make a point of referencing relevant material as possible, as well as providing a summary of further reading and resources at the end of most chapters. In prior editions we cited nearly everything we felt had relevant information. Here we are more a guidebook than an encyclopedia, as the ﬁeld has far outgrown exhaustive (and exhausting) lists of all possible variations of a given technique. We believe you are better served by describing only a few representative schemes of many, by replacing original sources with newer, broader overviews, and by relying on you, the reader, to pursue more information from the references cited.
Most of these sources are but a mouse click away; see realtimerendering.com for the list of links to references in the bibliography. Even if you have only a passing interest in a topic, consider taking a little time to look at the related references, if for nothing else than to see some of the fantastic images presented. Our website also
xiii

xiv

Preface

contains links to resources, tutorials, demonstration programs, code samples, software libraries, book corrections, and more.
Our true goal and guiding light while writing this book was simple. We wanted to write a book that we wished we had owned when we had started out, a book that both was uniﬁed yet also included details and references not found in introductory texts. We hope that you will ﬁnd this book, our view of the world, of use in your travels.
Acknowledgments for the Fourth Edition We are not experts in everything, by any stretch of the imagination, nor perfect writers. Many, many people’s responses and reviews improved this edition immeasurably, saving us from our own ignorance or inattention. As but one example, when we asked around for advice on what to cover in the area of virtual reality, Johannes Van Waveren (who did not know any of us) instantly responded with a wonderfully detailed outline of topics, which formed the basis for that chapter. These kind acts by computer graphics professionals were some of the great pleasures in writing this book. One person is of particular note: Patrick Cozzi did a yeoman’s job, reviewing every chapter in the book. We are grateful to the many people who helped us along the way with this edition. We could write a sentence or three about everyone who helped us along the way, but this would push us further past our book-breaking page limit.
To all the rest, in our hearts we give our appreciation and thanks to you: Sebastian Aaltonen, Johan Andersson, Magnus Andersson, Ulf Assarsson, Dan Baker, Chad Barb, Rasmus Barringer, Michal Bastien, Louis Bavoil, Michael Beale, Adrian Bentley, Ashwin Bhat, Antoine Bouthors, Wade Brainerd, Waylon Brinck, Ryan Brucks, Eric Bruneton, Valentin de Bruyn, Ben Burbank, Brent Burley, Ignacio Castan˜o, Cem Cebenoyan, Mark Cerny, Matthaeus Chajdas, Danny Chan, Rob Cook, JeanLuc Corenthin, Adrian Courr`eges, Cyril Crassin, Zhihao Cui, Kuba Cupisz, Robert Cupisz, Michal Drobot, Wolfgang Engel, Eugene d’Eon, Matej Drame, Michal Drobot, Alex Evans, Cass Everitt, Kayvon Fatahalian, Adam Finkelstein, Kurt Fleischer, Tim Foley, Tom Forsyth, Guillaume Franc¸ois, Daniel Girardeau-Montaut, Olga Gocmen, Marcin Gollent, Ben Golus, Carlos Gonzalez-Ochoa, Judah Graham, Simon Green, Dirk Gregorius, Larry Gritz, Andrew Hamilton, Earl Hammon, Jr., Jon Harada, Jon Hasselgren, Aaron Hertzmann, Stephen Hill, Rama Hoetzlein, Nicolas Holzschuch, Liwen Hu, John “Spike” Hughes, Ben Humberston, Warren Hunt, Andrew Hurley, John Hutchinson, Milan Ikits, Jon Jansen, Jorge Jimenez, Anton Kaplanyan, Go¨khan Karadayi, Brian Karis, Nicolas Kasyan, Alexander Keller, Brano Kemen, Emmett Kilgariﬀ, Byumjin Kim, Chris King, Joe Michael Kniss, Manuel Kraemer, Anders Wang Kristensen, Christopher Kulla, Edan Kwan, Chris Landreth, David Larsson, Andrew Lauritzen, Aaron Lefohn, Eric Lengyel, David Li, Ulrik Lindahl, Edward Liu, Ignacio Llamas, Dulce Isis Segarra Lo´pez, David Luebke, Patrick Lundell, Miles Macklin, Dzmitry Malyshau, Sam Martin, Morgan McGuire, Brian McIntyre, James McLaren, Mariano Merchante, Arne Meyer, Sergiy Migdalskiy, Kenny Mitchell, Gregory Mitrano, Adam Moravanszky, Jacob Munkberg, Kensaku Nakata, Srinivasa G. Narasimhan, David Neubelt, Fabrice Neyret, Jane Ng, Kasper Høy Nielsen, Matthias

Preface

xv

Nießner, Jim Nilsson, Reza Nourai, Chris Oat, Ola Olsson, Rafael Orozco, Bryan Pardilla, Steve Parker, Ankit Patel, Jasmin Patry, Jan Pechenik, Emil Persson, Marc Petit, Matt Pettineo, Agnieszka Piechnik, Jerome Platteaux, Aras Pranckeviˇcius, Elinor Quittner, Silvia Rasheva, Nathaniel Reed, Philip Rideout, Jon Rocatis, Robert Runesson, Marco Salvi, Nicolas Savva, Andrew Schneider, Michael Schneider, Markus Schuetz, Jeremy Selan, Tarek Sherif, Peter Shirley, Peter Sikachev, Peter-Pike Sloan, Ashley Vaughan Smith, Rys Sommefeldt, Edvard Sørg˚ard, Tiago Sousa, Tomasz Stachowiak, Nick Stam, Lee Stemkoski, Jonathan Stone, Kier Storey, Jacob Str¨om, Filip Strugar, Pierre Terdiman, Aaron Thibault, Nicolas Thibieroz, Robert Toth, Thatcher Ulrich, Mauricio Vives, Alex Vlachos, Evan Wallace, Ian Webster, Nick Whiting, Brandon Whitley, Mattias Widmark, Graham Wihlidal, Michael Wimmer, Daniel Wright, Bart Wron´ski, Chris Wyman, Ke Xu, Cem Yuksel, and Egor Yusov. We thank you for your time and eﬀort, selﬂessly oﬀered and gratefully received.
Finally, we want to thank the people at Taylor & Francis for all their eﬀorts, in particular Rick Adams, for getting us going and guiding us along the way, Jessica Vega and Michele Dimont, for their eﬃcient editorial work, and Charlotte Byrnes, for her superb copyediting.
Tomas Akenine-M¨oller Eric Haines
Naty Hoﬀman Angelo Pesce Michal Iwanicki S´ebastien Hillaire February 2018

Acknowledgments for the Third Edition
Special thanks go out to a number of people who went out of their way to provide us with help. First, our graphics architecture case studies would not have been anywhere as good without the extensive and generous cooperation we received from the companies making the hardware. Many thanks to Edvard Sørgard, Borgar Ljosland, Dave Shreiner, and Jørn Nystad at ARM for providing details about their Mali 200 architecture. Thanks also to Michael Dougherty at Microsoft, who provided extremely valuable help with the Xbox 360 section. Masaaki Oka at Sony Computer Entertainment provided his own technical review of the PLAYSTATION R 3 system case study, while also serving as the liaison with the Cell Broadband EngineTM and RSX R developers for their reviews.
In answering a seemingly endless stream of questions, fact-checking numerous passages, and providing many screenshots, Natalya Tatarchuk of ATI/AMD went well beyond the call of duty in helping us out. In addition to responding to our usual requests for information and clariﬁcation, Wolfgang Engel was extremely helpful in providing us with articles from the upcoming ShaderX6 book and copies of the diﬃcult-to-

xvi

Preface

obtain ShaderX2 books [427, 428], now available online for free. Ignacio Castan˜o at NVIDIA provided us with valuable support and contacts, going so far as to rework a refractory demo so we could get just the right screenshot.
The chapter reviewers provided an invaluable service to us. They suggested numerous improvements and provided additional insights, helping us immeasurably. In alphabetical order they are: Michael Ashikhmin, Dan Baker, Willem de Boer, Ben Diamand, Ben Discoe, Amir Ebrahimi, Christer Ericson, Michael Gleicher, Manny Ko, Wallace Lages, Thomas Larsson, Gr´egory Massal, Ville Miettinen, Mike Ramsey, Scott Schaefer, Vincent Scheib, Peter Shirley, K.R. Subramanian, Mauricio Vives, and Hector Yee.
We also had a number of reviewers help us on speciﬁc sections. Our thanks go out to Matt Bronder, Christine DeNezza, Frank Fox, Jon Hasselgren, Pete Isensee, Andrew Lauritzen, Morgan McGuire, Jacob Munkberg, Manuel M. Oliveira, Aurelio Reis, Peter-Pike Sloan, Jim Tilander, and Scott Whitman.
We particularly thank Rex Crowle, Kareem Ettouney, and Francis Pang from Media Molecule for their considerable help in providing fantastic imagery and layout concepts for the cover design.
Many people helped us out in other ways, such as answering questions and providing screenshots. Many gave signiﬁcant amounts of time and eﬀort, for which we thank you. Listed alphabetically: Paulo Abreu, Timo Aila, Johan Andersson, Andreas Bærentzen, Louis Bavoil, Jim Blinn, Jaime Borasi, Per Christensen, Patrick Conran, Rob Cook, Erwin Coumans, Leo Cubbin, Richard Daniels, Mark DeLoura, Tony DeRose, Andreas Dietrich, Michael Dougherty, Bryan Dudash, Alex Evans, Cass Everitt, Randy Fernando, Jim Ferwerda, Chris Ford, Tom Forsyth, Sam Glassenberg, Robin Green, Ned Greene, Larry Gritz, Joakim Grundwall, Mark Harris, Ted Himlan, Jack Hoxley, John “Spike” Hughes, Ladislav Kavan, Alicia Kim, Gary King, Chris Lambert, Jeﬀ Lander, Daniel Leaver, Eric Lengyel, Jennifer Liu, Brandon Lloyd, Charles Loop, David Luebke, Jonathan Ma¨ım, Jason Mitchell, Martin Mittring, Nathan Monteleone, Gabe Newell, Hubert Nguyen, Petri Nordlund, Mike Pan, Ivan Pedersen, Matt Pharr, Fabio Policarpo, Aras Pranckeviˇcius, Siobhan Reddy, Dirk Reiners, Christof Rezk-Salama, Eric Risser, Marcus Roth, Holly Rushmeier, Elan Ruskin, Marco Salvi, Daniel Scherzer, Kyle Shubel, Philipp Slusallek, Torbjo¨rn S¨oderman, Tim Sweeney, Ben Trumbore, Michal Valient, Mark Valledor, Carsten Wenzel, Steve Westin, Chris Wyman, Cem Yuksel, Billy Zelsnack, Fan Zhang, and Renaldas Zioma.
We also thank many others who responded to our queries on public forums such as GD Algorithms. Readers who took the time to send us corrections have also been a great help. It is this supportive attitude that is one of the pleasures of working in this ﬁeld.
As we have come to expect, the cheerful competence of the people at A K Peters made the publishing part of the process much easier. For this wonderful support, we thank you all.

Preface

xvii

On a personal note, Tomas would like to thank his son Felix and daughter Elina for making him understand (again) just how fun it can be to play computer games (on the Wii), instead of just looking at the graphics, and needless to say, his beautiful wife Eva. . .
Eric would also like to thank his sons Ryan and Evan for their tireless eﬀorts in ﬁnding cool game demos and screenshots, and his wife Cathy for helping him survive it all.
Naty would like to thank his daughter Karen and son Daniel for their forbearance when writing took precedence over piggyback rides, and his wife Dorit for her constant encouragement and support.
Tomas Akenine-M¨oller Eric Haines
Naty Hoﬀman March 2008
Acknowledgments for the Second Edition One of the most agreeable aspects of writing this second edition has been working with people and receiving their help. Despite their own pressing deadlines and concerns, many people gave us signiﬁcant amounts of their time to improve this book. We would particularly like to thank the major reviewers. They are, listed alphabetically: Michael Abrash, Ian Ashdown, Ulf Assarsson, Chris Brennan, S´ebastien Domin´e, David Eberly, Cass Everitt, Tommy Fortes, Evan Hart, Greg James, Jan Kautz, Alexander Keller, Mark Kilgard, Adam Lake, Paul Lalonde, Thomas Larsson, Dean Macri, Carl Marshall, Jason L. Mitchell, Kasper Høy Nielsen, Jon Paul Schelter, Jacob Stro¨m, Nick Triantos, Joe Warren, Michael Wimmer, and Peter Wonka. Of these, we wish to single out Cass Everitt at NVIDIA and Jason L. Mitchell at ATI Technologies for spending large amounts of time and eﬀort in getting us the resources we needed. Our thanks also go out to Wolfgang Engel for freely sharing the contents of his upcoming book, ShaderX [426], so that we could make this edition as current as possible.
From discussing their work with us, to providing images or other resources, to writing reviews of sections of the book, many others helped in creating this edition. They all have our gratitude. These people include: Jason Ang, Haim Barad, Jules Bloomenthal, Jonathan Blow, Chas. Boyd, John Brooks, Cem Cebenoyan, Per Christensen, Hamilton Chu, Michael Cohen, Daniel Cohen-Or, Matt Craighead, Paul Debevec, Joe Demers, Walt Donovan, Howard Dortch, Mark Duchaineau, Phil Dutr´e, Dave Eberle, Gerald Farin, Simon Fenney, Randy Fernando, Jim Ferwerda, Nickson Fong, Tom Forsyth, Piero Foscari, Laura Fryer, Markus Giegl, Peter Glaskowsky, Andrew Glassner, Amy Gooch, Bruce Gooch, Simon Green, Ned Greene, Larry Gritz, Joakim Grundwall, Juan Guardado, Pat Hanrahan, Mark Harris, Michael Herf, Carsten Hess, Rich Hilmer, Kenneth Hoﬀ III, Naty Hoﬀman, Nick Holliman, Hugues Hoppe, Heather Horne, Tom Hubina, Richard Huddy, Adam James, Kaveh Kardan, Paul Keller, David

xviii

Preface

Kirk, Alex Klimovitski, Jason Knipe, Jeﬀ Lander, Marc Levoy, J.P. Lewis, Ming Lin, Adrian Lopez, Michael McCool, Doug McNabb, Stan Melax, Ville Miettinen, Kenny Mitchell, Steve Morein, Henry Moreton, Jerris Mungai, Jim Napier, George Ngo, Hubert Nguyen, Tito Paga´n, J¨org Peters, Tom Porter, Emil Praun, Kekoa Proudfoot, Bernd Raabe, Ravi Ramamoorthi, Ashutosh Rege, Szymon Rusinkiewicz, Chris Seitz, Carlo S´equin, Jonathan Shade, Brian Smits, John Spitzer, Wolfgang Straßer, Wolfgang Stu¨rzlinger, Philip Taylor, Pierre Terdiman, Nicolas Thibieroz, Jack Tumblin, Fredrik Ulfves, Thatcher Ulrich, Steve Upstill, Alex Vlachos, Ingo Wald, Ben Watson, Steve Westin, Dan Wexler, Matthias Wloka, Peter Woytiuk, David Wu, Garrett Young, Borut Zalik, Harold Zatz, Hansong Zhang, and Denis Zorin. We also wish to thank the journal ACM Transactions on Graphics for providing a mirror website for this book.
Alice and Klaus Peters, our production manager Ariel Jaﬀee, our editor Heather Holcombe, our copyeditor Michelle M. Richards, and the rest of the staﬀ at A K Peters have done a wonderful job making this book the best possible. Our thanks to all of you.
Finally, and most importantly, our deepest thanks go to our families for giving us the huge amounts of quiet time we have needed to complete this edition. Honestly, we never thought it would take this long!
Tomas Akenine-M¨oller Eric Haines May 2002
Acknowledgments for the First Edition Many people helped in making this book. Some of the greatest contributions were made by those who reviewed parts of it. The reviewers willingly gave the beneﬁt of their expertise, helping to signiﬁcantly improve both content and style. We wish to thank (in alphabetical order) Thomas Barregren, Michael Cohen, Walt Donovan, Angus Dorbie, Michael Garland, Stefan Gottschalk, Ned Greene, Ming C. Lin, Jason L. Mitchell, Liang Peng, Keith Rule, Ken Shoemake, John Stone, Phil Taylor, Ben Trumbore, Jorrit Tyberghein, and Nick Wilt. We cannot thank you enough.
Many other people contributed their time and labor to this project. Some let us use images, others provided models, still others pointed out important resources or connected us with people who could help. In addition to the people listed above, we wish to acknowledge the help of Tony Barkans, Daniel Baum, Nelson Beebe, Curtis Beeson, Tor Berg, David Blythe, Chas. Boyd, Don Brittain, Ian Bullard, Javier Castellar, Satyan Coorg, Jason Della Rocca, Paul Diefenbach, Alyssa Donovan, Dave Eberly, Kells Elmquist, Stuart Feldman, Fred Fisher, Tom Forsyth, Marty Franz, Thomas Funkhouser, Andrew Glassner, Bruce Gooch, Larry Gritz, Robert Grzeszczuk, Paul Haeberli, Evan Hart, Paul Heckbert, Chris Hecker, Joachim Helenklaken, Hugues Hoppe, John Jack, Mark Kilgard, David Kirk, James Klosowski, Subodh Kumar, Andr´e LaMothe, Jeﬀ Lander, Jens Larsson, Jed Lengyel, Fredrik Liliegren, David Luebke, Thomas Lundqvist, Tom McReynolds, Stan Melax, Don Mitchell, Andr´e M¨oller,

Preface

xix

Steve Molnar, Scott R. Nelson, Hubert Nguyen, Doug Rogers, Holly Rushmeier, Gernot Schauﬂer, Jonas Skeppstedt, Stephen Spencer, Per Stenstr¨om, Jacob Str¨om, Filippo Tampieri, Gary Tarolli, Ken Turkowski, Turner Whitted, Agata and Andrzej Wojaczek, Andrew Woo, Steve Worley, Brian Yen, Hans-Philip Zachau, Gabriel Zachmann, and Al Zimmerman. We also wish to thank the journal ACM Transactions on Graphics for providing a stable website for this book.
Alice and Klaus Peters and the staﬀ at AK Peters, particularly Carolyn Artin and Sarah Gillis, have been instrumental in making this book a reality. To all of you, thanks.
Finally, our deepest thanks go to our families and friends for providing support throughout this incredible, sometimes grueling, often exhilarating process.
Tomas M¨oller Eric Haines March 1999

Chapter 1
Introduction
Real-time rendering is concerned with rapidly making images on the computer. It is the most highly interactive area of computer graphics. An image appears on the screen, the viewer acts or reacts, and this feedback aﬀects what is generated next. This cycle of reaction and rendering happens at a rapid enough rate that the viewer does not see individual images, but rather becomes immersed in a dynamic process.
The rate at which images are displayed is measured in frames per second (FPS) or Hertz (Hz). At one frame per second, there is little sense of interactivity; the user is painfully aware of the arrival of each new image. At around 6 FPS, a sense of interactivity starts to grow. Video games aim for 30, 60, 72, or higher FPS; at these speeds the user focuses on action and reaction.
Movie projectors show frames at 24 FPS but use a shutter system to display each frame two to four times to avoid ﬂicker. This refresh rate is separate from the display rate and is expressed in Hertz (Hz). A shutter that illuminates the frame three times has a 72 Hz refresh rate. LCD monitors also separate refresh rate from display rate.
Watching images appear on a screen at 24 FPS might be acceptable, but a higher rate is important for minimizing response time. As little as 15 milliseconds of temporal delay can slow and interfere with interaction [1849]. As an example, head-mounted displays for virtual reality often require 90 FPS to minimize latency.
There is more to real-time rendering than interactivity. If speed was the only criterion, any application that rapidly responded to user commands and drew anything on the screen would qualify. Rendering in real time normally means producing threedimensional images.
Interactivity and some sense of connection to three-dimensional space are suﬃcient conditions for real-time rendering, but a third element has become a part of its deﬁnition: graphics acceleration hardware. Many consider the introduction of the 3Dfx Voodoo 1 card in 1996 the real beginning of consumer-level three-dimensional graphics [408]. With the rapid advances in this market, every computer, tablet, and mobile phone now comes with a graphics processor built in. Some excellent examples of the results of real-time rendering made possible by hardware acceleration are shown in Figures 1.1 and 1.2.
1

2

1. Introduction

Figure 1.1. A shot from Forza Motorsport 7. (Image courtesy of Turn 10 Studios, Microsoft.)
Figure 1.2. The city of Beauclair rendered in The Witcher 3. (CD PROJEKT R , The Witcher R are registered trademarks of CD PROJEKT Capital Group. The Witcher game c CD PROJEKT S.A. Developed by CD PROJEKT S.A. All rights reserved. The Witcher game is based on the prose of Andrzej Sapkowski. All other copyrights and trademarks are the property of their respective owners.)
Advances in graphics hardware have fueled an explosion of research in the ﬁeld of interactive computer graphics. We will focus on providing methods to increase speed and improve image quality, while also describing the features and limitations of acceleration algorithms and graphics APIs. We will not be able to cover every topic in depth, so our goal is to present key concepts and terminology, explain the most robust and practical algorithms in the ﬁeld, and provide pointers to the best places to go for more information. We hope our attempts to provide you with tools for understanding this ﬁeld prove to be worth the time and eﬀort you spend with our book.

1.1. Contents Overview

3

1.1 Contents Overview
What follows is a brief overview of the chapters ahead.
Chapter 2, The Graphics Rendering Pipeline. The heart of real-time rendering is the set of steps that takes a scene description and converts it into something we can see.
Chapter 3, The Graphics Processing Unit. The modern GPU implements the stages of the rendering pipeline using a combination of ﬁxed-function and programmable units.
Chapter 4, Transforms. Transforms are the basic tools for manipulating the position, orientation, size, and shape of objects and the location and view of the camera.
Chapter 5, Shading Basics. Discussion begins on the deﬁnition of materials and lights and their use in achieving the desired surface appearance, whether realistic or stylized. Other appearance-related topics are introduced, such as providing higher image quality through the use of antialiasing, transparency, and gamma correction.
Chapter 6, Texturing. One of the most powerful tools for real-time rendering is the ability to rapidly access and display images on surfaces. This process is called texturing, and there are a wide variety of methods for applying it.
Chapter 7, Shadows. Adding shadows to a scene increases both realism and comprehension. The more popular algorithms for computing shadows rapidly are presented.
Chapter 8, Light and Color. Before we perform physically based rendering, we ﬁrst need to understand how to quantify light and color. And after our physical rendering process is done, we need to transform the resulting quantities into values for the display, accounting for the properties of the screen and viewing environment. Both topics are covered in this chapter.
Chapter 9, Physically Based Shading. We build an understanding of physically based shading models from the ground up. The chapter starts with the underlying physical phenomena, covers models for a variety of rendered materials, and ends with methods for blending materials together and ﬁltering them to avoid aliasing and preserve surface appearance.
Chapter 10, Local Illumination. Algorithms for portraying more elaborate light sources are explored. Surface shading takes into account that light is emitted by physical objects, which have characteristic shapes.
Chapter 11, Global Illumination. Algorithms that simulate multiple interactions between the light and the scene further increase the realism of an image. We discuss ambient and directional occlusion and methods for rendering global illumination effects on diﬀuse and specular surfaces, as well as some promising uniﬁed approaches.
Chapter 12, Image-Space Eﬀects. Graphics hardware is adept at performing image processing at rapid speeds. Image ﬁltering and reprojection techniques are discussed

4

1. Introduction

ﬁrst, then we survey several popular post-processing eﬀects: lens ﬂares, motion blur, and depth of ﬁeld.
Chapter 13, Beyond Polygons. Triangles are not always the fastest or most realistic way to describe objects. Alternate representations based on using images, point clouds, voxels, and other sets of samples each have their advantages.
Chapter 14, Volumetric and Translucency Rendering. The focus here is the theory and practice of volumetric material representations and their interactions with light sources. The simulated phenomena range from large-scale atmospheric eﬀects down to light scattering within thin hair ﬁbers.
Chapter 15, Non-Photorealistic Rendering. Attempting to make a scene look realistic is only one way of rendering it. Other styles, such as cartoon shading and watercolor eﬀects, are surveyed. Line and text generation techniques are also discussed.
Chapter 16, Polygonal Techniques. Geometric data comes from a wide range of sources, and sometimes requires modiﬁcation to be rendered rapidly and well. The many facets of polygonal data representation and compression are presented.
Chapter 17, Curves and Curved Surfaces. More complex surface representations oﬀer advantages such as being able to trade oﬀ between quality and rendering speed, more compact representation, and smooth surface generation.
Chapter 18, Pipeline Optimization. Once an application is running and uses eﬃcient algorithms, it can be made even faster using various optimization techniques. Finding the bottleneck and deciding what to do about it is the theme here. Multiprocessing is also discussed.
Chapter 19, Acceleration Algorithms. After you make it go, make it go fast. Various forms of culling and level of detail rendering are covered.
Chapter 20, Eﬃcient Shading. A large number of lights in a scene can slow performance considerably. Fully shading surface fragments before they are known to be visible is another source of wasted cycles. We explore a wide range of approaches to tackle these and other forms of ineﬃciency while shading.
Chapter 21, Virtual and Augmented Reality. These ﬁelds have particular challenges and techniques for eﬃciently producing realistic images at rapid and consistent rates.
Chapter 22, Intersection Test Methods. Intersection testing is important for rendering, user interaction, and collision detection. In-depth coverage is provided here for a wide range of the most eﬃcient algorithms for common geometric intersection tests.
Chapter 23, Graphics Hardware. The focus here is on components such as color depth, framebuﬀers, and basic architecture types. A case study of representative GPUs is provided.
Chapter 24, The Future. Take a guess (we do).

1.2. Notation and Deﬁnitions

5

Due to space constraints, we have made a chapter about Collision Detection free for download at realtimerendering.com, along with appendices on linear algebra and trigonometry.

1.2 Notation and Deﬁnitions
First, we shall explain the mathematical notation used in this book. For a more thorough explanation of many of the terms used in this section, and throughout this book, get our linear algebra appendix at realtimerendering.com.
1.2.1 Mathematical Notation
Table 1.1 summarizes most of the mathematical notation we will use. Some of the concepts will be described at some length here.
Note that there are some exceptions to the rules in the table, primarily shading equations using notation that is extremely well established in the literature, e.g., L for radiance, E for irradiance, and σs for scattering coeﬃcient.
The angles and the scalars are taken from R, i.e., they are real numbers. Vectors and points are denoted by bold lowercase letters, and the components are accessed as
 vx  v =  vy  ,
vz
that is, in column vector format, which is commonly used in the computer graphics world. At some places in the text we use (vx, vy, vz) instead of the formally more correct (vx vy vz)T , since the former is easier to read.

Type angle scalar vector or point matrix plane
triangle line segment geometric entity

Notation lowercase Greek lowercase italic lowercase bold capital bold π: a vector and a scalar △ 3 points two points capital italic

Examples αi, φ, ρ, η, γ242, θ a, b, t, uk, v, wij a,u, vs h(ρ), hz T(t), X, Rx(ρ) π : n · x + d = 0, π1 : n1 · x + d1 = 0 △v0v1v2, △cba uv, aibj AOBB , T , BAABB

Table 1.1. Summary of the notation used in this book.

6

1. Introduction

Using homogeneous notation, a coordinate is represented by four values v =

(vx vy vz vw)T , where a vector is v = (vx vy vz 0)T and a point is v = (vx vy vz 1)T . Sometimes we use only three-element vectors and points, but we

try to avoid any ambiguity as to which type is being used. For matrix manipulations,

it is extremely advantageous to have the same notation for vectors as for points. For

more information, see Chapter 4 on transforms. In some algorithms, it will be conve-

nient to use numeric indices instead of x, y, and z, for example v = (v0 v1 v2)T . All

these rules for vectors and points also hold for two-element vectors; in that case, we

simply skip the last component of a three-element vector.

The matrix deserves a bit more explanation. The common sizes that will be used

are 2 × 2, 3 × 3, and 4 × 4. We will review the manner of accessing a 3 × 3 matrix

M, and it is simple to extend this process to the other sizes. The (scalar) elements of

M are denoted mij, 0 ≤ (i, j) ≤ 2, where i denotes the row and j the column, as in

Equation 1.1:

 m00 m01 m02 

M =  m10 m11 m12  .

(1.1)

m20 m21 m22

The following notation, shown in Equation 1.2 for a 3 × 3 matrix, is used to isolate vectors from the matrix M: m,j represents the jth column vector and mi, represents the ith row vector (in column vector form). As with vectors and points, indexing the column vectors can also be done with x, y, z, and sometimes w, if that is more convenient:

 mT0, 

M=

m,0 m,1 m,2

=

mx my mz

=

 



mT1,

 

.



mT2,

(1.2)

A plane is denoted π : n · x + d = 0 and contains its mathematical formula, the plane normal n and the scalar d. The normal is a vector describing what direction the plane faces. More generally (e.g., for curved surfaces), a normal describes this direction for a particular point on the surface. For a plane the same normal happens to apply to all its points. π is the common mathematical notation for a plane. The plane π is said to divide the space into a positive half-space, where n · x + d > 0, and a negative half-space, where n · x + d < 0. All other points are said to lie in the plane.
A triangle can be deﬁned by three points v0, v1, and v2 and is denoted by △v0v1v2.
Table 1.2 presents some additional mathematical operators and their notation. The dot, cross, determinant, and length operators are explained in our downloadable linear algebra appendix at realtimerendering.com. The transpose operator turns a column vector into a row vector and vice versa. Thus a column vector can be written in compressed form in a block of text as v = (vx vy vz)T . Operator 4, introduced in Graphics Gems IV [735], is a unary operator on a two-dimensional vector. Letting

1.2. Notation and Deﬁnitions

7

Operator Description

1:

·

dot product

2:

×

cross product

3:

vT

transpose of the vector v

4:

⊥

the unary, perp dot product operator

5:

|·|

determinant of a matrix

6:

|·|

absolute value of a scalar

7:

·

length (or norm) of argument

8:

x+

clamping x to 0

9:

x+

clamping x between 0 and 1

10:

n!

factorial

n

11:

binomial coeﬃcients

k

Table 1.2. Notation for some mathematical operators.
this operator work on a vector v = (vx vy)T gives a vector that is perpendicular to v, i.e., v⊥ = (−vy vx)T . We use |a| to denote the absolute value of the scalar a, while |A| means the determinant of the matrix A. Sometimes, we also use |A| = |a b c| = det(a, b, c), where a, b, and c are column vectors of the matrix A.
Operators 8 and 9 are clamping operators, commonly used in shading calculations. Operator 8 clamps negative values to 0:

x+ = x, if x > 0, 0, otherwise,

(1.3)

and operator 9 clamps values between 0 and 1:

 1, x+ = x, 0,

if x ≥ 1, if 0 < x < 1, otherwise.

(1.4)

The tenth operator, factorial, is deﬁned as shown below, and note that 0! = 1:

n! = n(n − 1)(n − 2) · · · 3 · 2 · 1.

(1.5)

The eleventh operator, the binomial factor, is deﬁned as shown in Equation 1.6:

n k

=

n! k!(n −

k)!

.

(1.6)

8

1. Introduction

Function Description 1: atan2(y, x) two-value arctangent 2: log(n) natural logarithm of n
Table 1.3. Notation for some specialized mathematical functions.

Further on, we call the common planes x = 0, y = 0, and z = 0 the coordinate

planes or axis-aligned planes. The axes ex = (1 0 0)T , ey = (0 1 0)T , and ez = (0 0 1)T are called main axes or main directions and individually called the

x-axis, y-axis, and z-axis. This set of axes is often called the standard basis. Unless

otherwise noted, we will use orthonormal bases (consisting of mutually perpendicular

unit vectors).

The notation for a range that includes both a and b, and all numbers in between,

is [a, b]. If we want all number between a and b, but not a and b themselves, then we

write (a, b). Combinations of these can also be made, e.g., [a, b) means all numbers

between a and b including a but not b.

The C-math function atan2(y,x) is often used in this text, and so deserves some

attention. It is an extension of the mathematical function arctan(x). The main

diﬀerences

between

them

are

that

−

π 2

<

arctan(x)

<

π 2

,

that

0

≤

atan2(y, x)

<

2π,

and that an extra argument has been added to the latter function. A common use for

arctan is to compute arctan(y/x), but when x = 0, division by zero results. The extra

argument for atan2(y,x) avoids this.

In this volume the notation log(n) always means the natural logarithm, loge(n), not the base-10 logarithm, log10(n).
We use a right-hand coordinate system since this is the standard system for three-

dimensional geometry in the ﬁeld of computer graphics.

Colors are represented by a three-element vector, such as (red, green, blue), where

each element has the range [0, 1].

1.2.2 Geometrical Deﬁnitions
The basic rendering primitives (also called drawing primitives) used by almost all graphics hardware are points, lines, and triangles.1
Throughout this book, we will refer to a collection of geometric entities as either a model or an object. A scene is a collection of models comprising everything that is included in the environment to be rendered. A scene can also include material descriptions, lighting, and viewing speciﬁcations.
Examples of objects are a car, a building, and even a line. In practice, an object often consists of a set of drawing primitives, but this may not always be the case; an object may have a higher kind of geometrical representation, such as B´ezier curves or
1The only exceptions we know of are Pixel-Planes [502], which could draw spheres, and the NVIDIA NV1 chip, which could draw ellipsoids.

1.2. Notation and Deﬁnitions

9

surfaces, or subdivision surfaces. Also, objects can consist of other objects, e.g., a car object includes four door objects, four wheel objects, and so on.
1.2.3 Shading
Following well-established computer graphics usage, in this book terms derived from “shading,” “shader,” and related words are used to refer to two distinct but related concepts: computer-generated visual appearance (e.g., “shading model,” “shading equation,” “toon shading”) or a programmable component of a rendering system (e.g., “vertex shader,” “shading language”). In both cases, the intended meaning should be clear from the context.
Further Reading and Resources
The most important resource we can refer you to is the website for this book: realtimerendering.com. It contains links to the latest information and websites relevant to each chapter. The ﬁeld of real-time rendering is changing with real-time speed. In the book we have attempted to focus on concepts that are fundamental and techniques that are unlikely to go out of style. On the website we have the opportunity to present information that is relevant to today’s software developer, and we have the ability to keep it up-to-date.

Chapter 2 The Graphics Rendering Pipeline
“A chain is no stronger than its weakest link.” —Anonymous
This chapter presents the core component of real-time graphics, namely the graphics rendering pipeline, also known simply as “the pipeline.” The main function of the pipeline is to generate, or render, a two-dimensional image, given a virtual camera, three-dimensional objects, light sources, and more. The rendering pipeline is thus the underlying tool for real-time rendering. The process of using the pipeline is depicted in Figure 2.1. The locations and shapes of the objects in the image are determined by their geometry, the characteristics of the environment, and the placement of the camera in that environment. The appearance of the objects is aﬀected by material properties, light sources, textures (images applied to surfaces), and shading equations.
Figure 2.1. In the left image, a virtual camera is located at the tip of the pyramid (where four lines converge). Only the primitives inside the view volume are rendered. For an image that is rendered in perspective (as is the case here), the view volume is a frustum (plural: frusta), i.e., a truncated pyramid with a rectangular base. The right image shows what the camera “sees.” Note that the red donut shape in the left image is not in the rendering to the right because it is located outside the view frustum. Also, the twisted blue prism in the left image is clipped against the top plane of the frustum.
11

12

2. The Graphics Rendering Pipeline

We will explain the diﬀerent stages of the rendering pipeline, with a focus on function rather than implementation. Relevant details for applying these stages will be covered in later chapters.

2.1 Architecture
In the physical world, the pipeline concept manifests itself in many diﬀerent forms, from factory assembly lines to fast food kitchens. It also applies to graphics rendering. A pipeline consists of several stages [715], each of which performs part of a larger task.
The pipeline stages execute in parallel, with each stage dependent upon the result of the previous stage. Ideally, a nonpipelined system that is then divided into n pipelined stages could give a speedup of a factor of n. This increase in performance is the main reason to use pipelining. For example, a large number of sandwiches can be prepared quickly by a series of people—one preparing the bread, another adding meat, another adding toppings. Each passes the result to the next person in line and immediately starts work on the next sandwich. If each person takes twenty seconds to perform their task, a maximum rate of one sandwich every twenty seconds, three a minute, is possible. The pipeline stages execute in parallel, but they are stalled until the slowest stage has ﬁnished its task. For example, say the meat addition stage becomes more involved, taking thirty seconds. Now the best rate that can be achieved is two sandwiches a minute. For this particular pipeline, the meat stage is the bottleneck, since it determines the speed of the entire production. The toppings stage is said to be starved (and the customer, too) during the time it waits for the meat stage to be done.
This kind of pipeline construction is also found in the context of real-time computer graphics. A coarse division of the real-time rendering pipeline into four main stages—application, geometry processing, rasterization, and pixel processing—is shown in Figure 2.2. This structure is the core—the engine of the rendering pipeline—which is used in real-time computer graphics applications and is thus an essential base for

Application

Geometry Processing

Rasterization

Pixel Processing

Figure 2.2. The basic construction of the rendering pipeline, consisting of four stages: application, geometry processing, rasterization, and pixel processing. Each of these stages may be a pipeline in itself, as illustrated below the geometry processing stage, or a stage may be (partly) parallelized, as shown below the pixel processing stage. In this illustration, the application stage is a single process, but this stage could also be pipelined or parallelized. Note that rasterization ﬁnds the pixels inside a primitive, e.g., a triangle.

2.2. The Application Stage

13

discussion in subsequent chapters. Each of these stages is usually a pipeline in itself, which means that it consists of several substages. We diﬀerentiate between the functional stages shown here and the structure of their implementation. A functional stage has a certain task to perform but does not specify the way that task is executed in the pipeline. A given implementation may combine two functional stages into one unit or execute using programmable cores, while it divides another, more time-consuming, functional stage into several hardware units.
The rendering speed may be expressed in frames per second (FPS), that is, the number of images rendered per second. It can also be represented using Hertz (Hz), which is simply the notation for 1/seconds, i.e., the frequency of update. It is also common to just state the time, in milliseconds (ms), that it takes to render an image. The time to generate an image usually varies, depending on the complexity of the computations performed during each frame. Frames per second is used to express either the rate for a particular frame, or the average performance over some duration of use. Hertz is used for hardware, such as a display, which is set to a ﬁxed rate.
As the name implies, the application stage is driven by the application and is therefore typically implemented in software running on general-purpose CPUs. These CPUs commonly include multiple cores that are capable of processing multiple threads of execution in parallel. This enables the CPUs to eﬃciently run the large variety of tasks that are the responsibility of the application stage. Some of the tasks traditionally performed on the CPU include collision detection, global acceleration algorithms, animation, physics simulation, and many others, depending on the type of application. The next main stage is geometry processing, which deals with transforms, projections, and all other types of geometry handling. This stage computes what is to be drawn, how it should be drawn, and where it should be drawn. The geometry stage is typically performed on a graphics processing unit (GPU) that contains many programmable cores as well as ﬁxed-operation hardware. The rasterization stage typically takes as input three vertices, forming a triangle, and ﬁnds all pixels that are considered inside that triangle, then forwards these to the next stage. Finally, the pixel processing stage executes a program per pixel to determine its color and may perform depth testing to see whether it is visible or not. It may also perform per-pixel operations such as blending the newly computed color with a previous color. The rasterization and pixel processing stages are also processed entirely on the GPU. All these stages and their internal pipelines will be discussed in the next four sections. More details on how the GPU processes these stages are given in Chapter 3.

2.2 The Application Stage
The developer has full control over what happens in the application stage, since it usually executes on the CPU. Therefore, the developer can entirely determine the implementation and can later modify it in order to improve performance. Changes here can also aﬀect the performance of subsequent stages. For example, an application

14

2. The Graphics Rendering Pipeline

stage algorithm or setting could decrease the number of triangles to be rendered. All this said, some application work can be performed by the GPU, using a separate
mode called a compute shader. This mode treats the GPU as a highly parallel general processor, ignoring its special functionality meant speciﬁcally for rendering graphics.
At the end of the application stage, the geometry to be rendered is fed to the geometry processing stage. These are the rendering primitives, i.e., points, lines, and triangles, that might eventually end up on the screen (or whatever output device is being used). This is the most important task of the application stage.
A consequence of the software-based implementation of this stage is that it is not divided into substages, as are the geometry processing, rasterization, and pixel processing stages.1 However, to increase performance, this stage is often executed in parallel on several processor cores. In CPU design, this is called a superscalar construction, since it is able to execute several processes at the same time in the same stage. Section 18.5 presents various methods for using multiple processor cores.
One process commonly implemented in this stage is collision detection. After a collision is detected between two objects, a response may be generated and sent back to the colliding objects, as well as to a force feedback device. The application stage is also the place to take care of input from other sources, such as the keyboard, the mouse, or a head-mounted display. Depending on this input, several diﬀerent kinds of actions may be taken. Acceleration algorithms, such as particular culling algorithms (Chapter 19), are also implemented here, along with whatever else the rest of the pipeline cannot handle.

2.3 Geometry Processing
The geometry processing stage on the GPU is responsible for most of the per-triangle and per-vertex operations. This stage is further divided into the following functional stages: vertex shading, projection, clipping, and screen mapping (Figure 2.3).

Vertex Shading

Projection

Clipping

Screen Mapping

Figure 2.3. The geometry processing stage divided into a pipeline of functional stages.

1Since a CPU itself is pipelined on a much smaller scale, you could say that the application stage is further subdivided into several pipeline stages, but this is not relevant here.

2.3. Geometry Processing

15

2.3.1 Vertex Shading
There are two main tasks of vertex shading, namely, to compute the position for a vertex and to evaluate whatever the programmer may like to have as vertex output data, such as a normal and texture coordinates. Traditionally much of the shade of an object was computed by applying lights to each vertex’s location and normal and storing only the resulting color at the vertex. These colors were then interpolated across the triangle. For this reason, this programmable vertex processing unit was named the vertex shader [1049]. With the advent of the modern GPU, along with some or all of the shading taking place per pixel, this vertex shading stage is more general and may not evaluate any shading equations at all, depending on the programmer’s intent. The vertex shader is now a more general unit dedicated to setting up the data associated with each vertex. As an example, the vertex shader can animate an object using the methods in Sections 4.4 and 4.5.
We start by describing how the vertex position is computed, a set of coordinates that is always required. On its way to the screen, a model is transformed into several diﬀerent spaces or coordinate systems. Originally, a model resides in its own model space, which simply means that it has not been transformed at all. Each model can be associated with a model transform so that it can be positioned and oriented. It is possible to have several model transforms associated with a single model. This allows several copies (called instances) of the same model to have diﬀerent locations, orientations, and sizes in the same scene, without requiring replication of the basic geometry.
It is the vertices and the normals of the model that are transformed by the model transform. The coordinates of an object are called model coordinates, and after the model transform has been applied to these coordinates, the model is said to be located in world coordinates or in world space. The world space is unique, and after the models have been transformed with their respective model transforms, all models exist in this same space.
As mentioned previously, only the models that the camera (or observer) sees are rendered. The camera has a location in world space and a direction, which are used to place and aim the camera. To facilitate projection and clipping, the camera and all the models are transformed with the view transform. The purpose of the view transform is to place the camera at the origin and aim it, to make it look in the direction of the negative z-axis, with the y-axis pointing upward and the x-axis pointing to the right. We use the −z-axis convention; some texts prefer looking down the +z-axis. The diﬀerence is mostly semantic, as transform between one and the other is simple. The actual position and direction after the view transform has been applied are dependent on the underlying application programming interface (API). The space thus delineated is called camera space, or more commonly, view space or eye space. An example of the way in which the view transform aﬀects the camera and the models is shown in Figure 2.4. Both the model transform and the view transform may be implemented as 4×4 matrices, which is the topic of Chapter 4. However, it is important to realize that

16
x

2. The Graphics Rendering Pipeline

y world space

xc zc
view space

Figure 2.4. In the left illustration, a top-down view shows the camera located and oriented as the user wants it to be, in a world where the +z-axis is up. The view transform reorients the world so that the camera is at the origin, looking along its negative z-axis, with the camera’s +y-axis up, as shown on the right. This is done to make the clipping and projection operations simpler and faster. The light blue area is the view volume. Here, perspective viewing is assumed, since the view volume is a frustum. Similar techniques apply to any kind of projection.

the position and normal of a vertex can be computed in whatever way the programmer prefers.
Next, we describe the second type of output from vertex shading. To produce a realistic scene, it is not suﬃcient to render the shape and position of objects, but their appearance must be modeled as well. This description includes each object’s material, as well as the eﬀect of any light sources shining on the object. Materials and lights can be modeled in any number of ways, from simple colors to elaborate representations of physical descriptions.
This operation of determining the eﬀect of a light on a material is known as shading. It involves computing a shading equation at various points on the object. Typically, some of these computations are performed during geometry processing on a model’s vertices, and others may be performed during per-pixel processing. A variety of material data can be stored at each vertex, such as the point’s location, a normal, a color, or any other numerical information that is needed to evaluate the shading equation. Vertex shading results (which can be colors, vectors, texture coordinates, along with any other kind of shading data) are then sent to the rasterization and pixel processing stages to be interpolated and used to compute the shading of the surface.
Vertex shading in the form of the GPU vertex shader is discussed in more depth throughout this book and most speciﬁcally in Chapters 3 and 5.
As part of vertex shading, rendering systems perform projection and then clipping, which transforms the view volume into a unit cube with its extreme points at (−1, −1, −1) and (1, 1, 1). Diﬀerent ranges deﬁning the same volume can and are used, for example, 0 ≤ z ≤ 1. The unit cube is called the canonical view volume. Projection is done ﬁrst, and on the GPU it is done by the vertex shader. There are two commonly used projection methods, namely orthographic (also called parallel )

2.3. Geometry Processing

17

Figure 2.5. On the left is an orthographic, or parallel, projection; on the right is a perspective projection.
and perspective projection. See Figure 2.5. In truth, orthographic is just one type of parallel projection. Several others ﬁnd use, particularly in the ﬁeld of architecture, such as oblique and axonometric projections. The old arcade game Zaxxon is named from the latter.
Note that projection is expressed as a matrix (Section 4.7) and so it may sometimes be concatenated with the rest of the geometry transform.
The view volume of orthographic viewing is normally a rectangular box, and the orthographic projection transforms this view volume into the unit cube. The main characteristic of orthographic projection is that parallel lines remain parallel after the transform. This transformation is a combination of a translation and a scaling.
The perspective projection is a bit more complex. In this type of projection, the farther away an object lies from the camera, the smaller it appears after projection. In addition, parallel lines may converge at the horizon. The perspective transform thus mimics the way we perceive objects’ size. Geometrically, the view volume, called a frustum, is a truncated pyramid with rectangular base. The frustum is transformed

18

2. The Graphics Rendering Pipeline

into the unit cube as well. Both orthographic and perspective transforms can be constructed with 4 × 4 matrices (Chapter 4), and after either transform, the models are said to be in clip coordinates. These are in fact homogeneous coordinates, discussed in Chapter 4, and so this occurs before division by w. The GPU’s vertex shader must always output coordinates of this type in order for the next functional stage, clipping, to work correctly.
Although these matrices transform one volume into another, they are called projections because after display, the z-coordinate is not stored in the image generated but is stored in a z-buﬀer, described in Section 2.5. In this way, the models are projected from three to two dimensions.
2.3.2 Optional Vertex Processing
Every pipeline has the vertex processing just described. Once this processing is done, there are a few optional stages that can take place on the GPU, in this order: tessellation, geometry shading, and stream output. Their use depends both on the capabilities of the hardware—not all GPUs have them—and the desires of the programmer. They are independent of each other, and in general they are not commonly used. More will be said about each in Chapter 3.
The ﬁrst optional stage is tessellation. Imagine you have a bouncing ball object. If you represent it with a single set of triangles, you can run into problems with quality or performance. Your ball may look good from 5 meters away, but up close the individual triangles, especially along the silhouette, become visible. If you make the ball with more triangles to improve quality, you may waste considerable processing time and memory when the ball is far away and covers only a few pixels on the screen. With tessellation, a curved surface can be generated with an appropriate number of triangles.
We have talked a bit about triangles, but up to this point in the pipeline we have just processed vertices. These could be used to represent points, lines, triangles, or other objects. Vertices can be used to describe a curved surface, such as a ball. Such surfaces can be speciﬁed by a set of patches, and each patch is made of a set of vertices. The tessellation stage consists of a series of stages itself—hull shader, tessellator, and domain shader—that converts these sets of patch vertices into (normally) larger sets of vertices that are then used to make new sets of triangles. The camera for the scene can be used to determine how many triangles are generated: many when the patch is close, few when it is far away.
The next optional stage is the geometry shader. This shader predates the tessellation shader and so is more commonly found on GPUs. It is like the tessellation shader in that it takes in primitives of various sorts and can produce new vertices. It is a much simpler stage in that this creation is limited in scope and the types of output primitives are much more limited. Geometry shaders have several uses, with one of the most popular being particle generation. Imagine simulating a ﬁreworks explosion.

2.3. Geometry Processing

19

Each ﬁreball could be represented by a point, a single vertex. The geometry shader can take each point and turn it into a square (made of two triangles) that faces the viewer and covers several pixels, so providing a more convincing primitive for us to shade.
The last optional stage is called stream output. This stage lets us use the GPU as a geometry engine. Instead of sending our processed vertices down the rest of the pipeline to be rendered to the screen, at this point we can optionally output these to an array for further processing. These data can be used by the CPU, or the GPU itself, in a later pass. This stage is typically used for particle simulations, such as our ﬁreworks example.
These three stages are performed in this order—tessellation, geometry shading, and stream output—and each is optional. Regardless of which (if any) options are used, if we continue down the pipeline we have a set of vertices with homogeneous coordinates that will be checked for whether the camera views them.
2.3.3 Clipping
Only the primitives wholly or partially inside the view volume need to be passed on to the rasterization stage (and the subsequent pixel processing stage), which then draws them on the screen. A primitive that lies fully inside the view volume will be passed on to the next stage as is. Primitives entirely outside the view volume are not passed on further, since they are not rendered. It is the primitives that are partially inside the view volume that require clipping. For example, a line that has one vertex outside and one inside the view volume should be clipped against the view volume, so that the vertex that is outside is replaced by a new vertex that is located at the intersection between the line and the view volume. The use of a projection matrix means that the transformed primitives are clipped against the unit cube. The advantage of performing the view transformation and projection before clipping is that it makes the clipping problem consistent; primitives are always clipped against the unit cube.
The clipping process is depicted in Figure 2.6. In addition to the six clipping planes of the view volume, the user can deﬁne additional clipping planes to visibly chop objects. An image showing this type of visualization, called sectioning, is shown in Figure 19.1 on page 818.
The clipping step uses the 4-value homogeneous coordinates produced by projection to perform clipping. Values do not normally interpolate linearly across a triangle in perspective space. The fourth coordinate is needed so that data are properly interpolated and clipped when a perspective projection is used. Finally, perspective division is performed, which places the resulting triangles’ positions into three-dimensional normalized device coordinates. As mentioned earlier, this view volume ranges from (−1, −1, −1) to (1, 1, 1). The last step in the geometry stage is to convert from this space to window coordinates.

20

2. The Graphics Rendering Pipeline

Figure 2.6. After the projection transform, only the primitives inside the unit cube (which correspond to primitives inside the view frustum) are needed for continued processing. Therefore, the primitives outside the unit cube are discarded, and primitives fully inside are kept. Primitives intersecting with the unit cube are clipped against the unit cube, and thus new vertices are generated and old ones are discarded.
2.3.4 Screen Mapping
Only the (clipped) primitives inside the view volume are passed on to the screen mapping stage, and the coordinates are still three-dimensional when entering this stage. The x- and y-coordinates of each primitive are transformed to form screen coordinates. Screen coordinates together with the z-coordinates are also called window coordinates. Assume that the scene should be rendered into a window with the minimum corner at (x1, y1) and the maximum corner at (x2, y2), where x1 < x2 and y1 < y2. Then the screen mapping is a translation followed by a scaling operation. The new x- and ycoordinates are said to be screen coordinates. The z-coordinate ([−1, +1] for OpenGL and [0, 1] for DirectX) is also mapped to [z1, z2], with z1 = 0 and z2 = 1 as the default values. These can be changed with the API, however. The window coordinates along with this remapped z-value are passed on to the rasterizer stage. The screen mapping process is depicted in Figure 2.7.
Figure 2.7. The primitives lie in the unit cube after the projection transform, and the screen mapping procedure takes care of ﬁnding the coordinates on the screen.

2.4. Rasterization

21

Next, we describe how integer and ﬂoating point values relate to pixels (and texture coordinates). Given a horizontal array of pixels and using Cartesian coordinates, the left edge of the leftmost pixel is 0.0 in ﬂoating point coordinates. OpenGL has always used this scheme, and DirectX 10 and its successors use it. The center of this pixel is at 0.5. So, a range of pixels [0, 9] cover a span from [0.0, 10.0). The conversions are simply

d = floor(c), c = d + 0.5,

(2.1) (2.2)

where d is the discrete (integer) index of the pixel and c is the continuous (ﬂoating point) value within the pixel.
While all APIs have pixel location values that increase going from left to right, the location of zero for the top and bottom edges is inconsistent in some cases between OpenGL and DirectX.2 OpenGL favors the Cartesian system throughout, treating the lower left corner as the lowest-valued element, while DirectX sometimes deﬁnes the upper left corner as this element, depending on the context. There is a logic to each, and no right answer exists where they diﬀer. As an example, (0, 0) is located at the lower left corner of an image in OpenGL, while it is upper left for DirectX. This diﬀerence is important to take into account when moving from one API to the other.

2.4 Rasterization
Given the transformed and projected vertices with their associated shading data (all from geometry processing), the goal of the next stage is to ﬁnd all pixels—short for picture elements—that are inside the primitive, e.g., a triangle, being rendered. We call this process rasterization, and it is split up into two functional substages: triangle setup (also called primitive assembly) and triangle traversal. These are shown to the left in Figure 2.8. Note that these can handle points and lines as well, but since triangles are most common, the substages have “triangle” in their names. Rasterization, also called scan conversion, is thus the conversion from two-dimensional vertices in screen space—each with a z-value (depth value) and various shading information associated with each vertex—into pixels on the screen. Rasterization can also be thought of as a synchronization point between geometry processing and pixel processing, since it is here that triangles are formed from three vertices and eventually sent down to pixel processing.
Whether the triangle is considered to overlap the pixel depends on how you have set up the GPU’s pipeline. For example, you may use point sampling to determine
2“Direct3D” is the three-dimensional graphics API component of DirectX. DirectX includes other API elements, such an input and audio control. Rather than diﬀerentiate between writing “DirectX” when specifying a particular release and “Direct3D” when discussing this particular API, we follow common usage by writing “DirectX” throughout.

22

2. The Graphics Rendering Pipeline

Triangle Setup

Triangle Traversal

Pixel Shading

Merging

} }

Rasterization

Pixel Processing

Figure 2.8. Left: rasterization split into two functional stages, called triangle setup and triangle traversal. Right: pixel processing split into two functional stages, namely, pixel processing and merging.

“insideness.” The simplest case uses a single point sample in the center of each pixel, and so if that center point is inside the triangle then the corresponding pixel is considered inside the triangle as well. You may also use more than one sample per pixel using supersampling or multisampling antialiasing techniques (Section 5.4.2). Yet another way is to use conservative rasterization, where the deﬁnition is that a pixel is “inside” the triangle if at least part of the pixel overlaps with the triangle (Section 23.1.2).
2.4.1 Triangle Setup
In this stage the diﬀerentials, edge equations, and other data for the triangle are computed. These data may be used for triangle traversal (Section 2.4.2), as well as for interpolation of the various shading data produced by the geometry stage. Fixedfunction hardware is used for this task.
2.4.2 Triangle Traversal
Here is where each pixel that has its center (or a sample) covered by the triangle is checked and a fragment generated for the part of the pixel that overlaps the triangle. More elaborate sampling methods can be found in Section 5.4. Finding which samples or pixels are inside a triangle is often called triangle traversal. Each triangle fragment’s properties are generated using data interpolated among the three triangle vertices (Chapter 5). These properties include the fragment’s depth, as well as any shading data from the geometry stage. McCormack et al. [1162] oﬀer more information on triangle traversal. It is also here that perspective-correct interpolation over the triangles is performed [694] (Section 23.1.1). All pixels or samples that are inside a primitive are then sent to the pixel processing stage, described next.
2.5 Pixel Processing
At this point, all the pixels that are considered inside a triangle or other primitive have been found as a consequence of the combination of all the previous stages. The

2.5. Pixel Processing

23

pixel processing stage is divided into pixel shading and merging, shown to the right in Figure 2.8. Pixel processing is the stage where per-pixel or per-sample computations and operations are performed on pixels or samples that are inside a primitive.

2.5.1 Pixel Shading
Any per-pixel shading computations are performed here, using the interpolated shading data as input. The end result is one or more colors to be passed on to the next stage. Unlike the triangle setup and traversal stages, which are usually performed by dedicated, hardwired silicon, the pixel shading stage is executed by programmable GPU cores. To that end, the programmer supplies a program for the pixel shader (or fragment shader, as it is known in OpenGL), which can contain any desired computations. A large variety of techniques can be employed here, one of the most important of which is texturing. Texturing is treated in more detail in Chapter 6. Simply put, texturing an object means “gluing” one or more images onto that object, for a variety of purposes. A simple example of this process is depicted in Figure 2.9. The image may be one-, two-, or three-dimensional, with two-dimensional images being the most common. At its simplest, the end product is a color value for each fragment, and these are passed on to the next substage.

Figure 2.9. A dragon model without textures is shown in the upper left. The pieces in the image texture are “glued” onto the dragon, and the result is shown in the lower left.

24

2. The Graphics Rendering Pipeline

2.5.2 Merging
The information for each pixel is stored in the color buﬀer, which is a rectangular array of colors (a red, a green, and a blue component for each color). It is the responsibility of the merging stage to combine the fragment color produced by the pixel shading stage with the color currently stored in the buﬀer. This stage is also called ROP, standing for “raster operations (pipeline)” or “render output unit,” depending on who you ask. Unlike the shading stage, the GPU subunit that performs this stage is typically not fully programmable. However, it is highly conﬁgurable, enabling various eﬀects.
This stage is also responsible for resolving visibility. This means that when the whole scene has been rendered, the color buﬀer should contain the colors of the primitives in the scene that are visible from the point of view of the camera. For most or even all graphics hardware, this is done with the z-buﬀer (also called depth buﬀer) algorithm [238]. A z-buﬀer is the same size and shape as the color buﬀer, and for each pixel it stores the z-value to the currently closest primitive. This means that when a primitive is being rendered to a certain pixel, the z-value on that primitive at that pixel is being computed and compared to the contents of the z-buﬀer at the same pixel. If the new z-value is smaller than the z-value in the z-buﬀer, then the primitive that is being rendered is closer to the camera than the primitive that was previously closest to the camera at that pixel. Therefore, the z-value and the color of that pixel are updated with the z-value and color from the primitive that is being drawn. If the computed z-value is greater than the z-value in the z-buﬀer, then the color buﬀer and the z-buﬀer are left untouched. The z-buﬀer algorithm is simple, has O(n) convergence (where n is the number of primitives being rendered), and works for any drawing primitive for which a z-value can be computed for each (relevant) pixel. Also note that this algorithm allows most primitives to be rendered in any order, which is another reason for its popularity. However, the z-buﬀer stores only a single depth at each point on the screen, so it cannot be used for partially transparent primitives. These must be rendered after all opaque primitives, and in back-to-front order, or using a separate order-independent algorithm (Section 5.5). Transparency is one of the major weaknesses of the basic z-buﬀer.
We have mentioned that the color buﬀer is used to store colors and that the z-buﬀer stores z-values for each pixel. However, there are other channels and buﬀers that can be used to ﬁlter and capture fragment information. The alpha channel is associated with the color buﬀer and stores a related opacity value for each pixel (Section 5.5). In older APIs, the alpha channel was also used to discard pixels selectively via the alpha test feature. Nowadays a discard operation can be inserted into the pixel shader program and any type of computation can be used to trigger a discard. This type of test can be used to ensure that fully transparent fragments do not aﬀect the z-buﬀer (Section 6.6).
The stencil buﬀer is an oﬀscreen buﬀer used to record the locations of the rendered primitive. It typically contains 8 bits per pixel. Primitives can be rendered into the stencil buﬀer using various functions, and the buﬀer’s contents can then be used to

2.6. Through the Pipeline

25

control rendering into the color buﬀer and z-buﬀer. As an example, assume that a ﬁlled circle has been drawn into the stencil buﬀer. This can be combined with an operator that allows rendering of subsequent primitives into the color buﬀer only where the circle is present. The stencil buﬀer can be a powerful tool for generating some special eﬀects. All these functions at the end of the pipeline are called raster operations (ROP) or blend operations. It is possible to mix the color currently in the color buﬀer with the color of the pixel being processed inside a triangle. This can enable eﬀects such as transparency or the accumulation of color samples. As mentioned, blending is typically conﬁgurable using the API and not fully programmable. However, some APIs have support for raster order views, also called pixel shader ordering, which enable programmable blending capabilities.
The framebuﬀer generally consists of all the buﬀers on a system. When the primitives have reached and passed the rasterizer stage, those that are visible from the point of view of the camera are displayed on screen. The screen displays the contents of the color buﬀer. To avoid allowing the human viewer to see the primitives as they are being rasterized and sent to the screen, double buﬀering is used. This means that the rendering of a scene takes place oﬀ screen, in a back buﬀer. Once the scene has been rendered in the back buﬀer, the contents of the back buﬀer are swapped with the contents of the front buﬀer that was previously displayed on the screen. The swapping often occurs during vertical retrace, a time when it is safe to do so. For more information on diﬀerent buﬀers and buﬀering methods, see Sections 5.4.2, 23.6, and 23.7.

2.6 Through the Pipeline
Points, lines, and triangles are the rendering primitives from which a model or an object is built. Imagine that the application is an interactive computer aided design (CAD) application, and that the user is examining a design for a waﬄe maker. Here we will follow this model through the entire graphics rendering pipeline, consisting of the four major stages: application, geometry, rasterization, and pixel processing. The scene is rendered with perspective into a window on the screen. In this simple example, the waﬄe maker model includes both lines (to show the edges of parts) and triangles (to show the surfaces). The waﬄe maker has a lid that can be opened. Some of the triangles are textured by a two-dimensional image with the manufacturer’s logo. For this example, surface shading is computed completely in the geometry stage, except for application of the texture, which occurs in the rasterization stage.
Application CAD applications allow the user to select and move parts of the model. For example, the user might select the lid and then move the mouse to open it. The application stage must translate the mouse move to a corresponding rotation matrix, then see to

26

2. The Graphics Rendering Pipeline

it that this matrix is properly applied to the lid when it is rendered. Another example: An animation is played that moves the camera along a predeﬁned path to show the waﬄe maker from diﬀerent views. The camera parameters, such as position and view direction, must then be updated by the application, dependent upon time. For each frame to be rendered, the application stage feeds the camera position, lighting, and primitives of the model to the next major stage in the pipeline—the geometry stage.
Geometry Processing For perspective viewing, we assume here that the application has supplied a projection matrix. Also, for each object, the application has computed a matrix that describes both the view transform and the location and orientation of the object in itself. In our example, the waﬄe maker’s base would have one matrix, the lid another. In the geometry stage the vertices and normals of the object are transformed with this matrix, putting the object into view space. Then shading or other calculations at the vertices may be computed, using material and light source properties. Projection is then performed using a separate user-supplied projection matrix, transforming the object into a unit cube’s space that represents what the eye sees. All primitives outside the cube are discarded. All primitives intersecting this unit cube are clipped against the cube in order to obtain a set of primitives that lies entirely inside the unit cube. The vertices then are mapped into the window on the screen. After all these pertriangle and per-vertex operations have been performed, the resulting data are passed on to the rasterization stage.
Rasterization All the primitives that survive clipping in the previous stage are then rasterized, which means that all pixels that are inside a primitive are found and sent further down the pipeline to pixel processing.
Pixel Processing The goal here is to compute the color of each pixel of each visible primitive. Those triangles that have been associated with any textures (images) are rendered with these images applied to them as desired. Visibility is resolved via the z-buﬀer algorithm, along with optional discard and stencil tests. Each object is processed in turn, and the ﬁnal image is then displayed on the screen.
Conclusion
This pipeline resulted from decades of API and graphics hardware evolution targeted to real-time rendering applications. It is important to note that this is not the only possible rendering pipeline; oﬄine rendering pipelines have undergone diﬀerent evolutionary paths. Rendering for ﬁlm production was often done with micropolygon pipelines [289, 1734], but ray tracing and path tracing have taken over lately. These

2.6. Through the Pipeline

27

techniques, covered in Section 11.2.2, may also be used in architectural and design previsualization.
For many years, the only way for application developers to use the process described here was through a ﬁxed-function pipeline deﬁned by the graphics API in use. The ﬁxed-function pipeline is so named because the graphics hardware that implements it consists of elements that cannot be programmed in a ﬂexible way. The last example of a major ﬁxed-function machine is Nintendo’s Wii, introduced in 2006. Programmable GPUs, on the other hand, make it possible to determine exactly what operations are applied in various sub-stages throughout the pipeline. For the fourth edition of the book, we assume that all development is done using programmable GPUs.
Further Reading and Resources
Blinn’s book A Trip Down the Graphics Pipeline [165] is an older book about writing a software renderer from scratch. It is a good resource for learning about some of the subtleties of implementing a rendering pipeline, explaining key algorithms such as clipping and perspective interpolation. The venerable (yet frequently updated) OpenGL Programming Guide (a.k.a. the “Red Book”) [885] provides a thorough description of the graphics pipeline and algorithms related to its use. Our book’s website, realtimerendering.com, gives links to a variety of pipeline diagrams, rendering engine implementations, and more.

Chapter 3
The Graphics Processing Unit
“The display is the computer.” —Jen-Hsun Huang
Historically, graphics acceleration started with interpolating colors on each pixel scanline overlapping a triangle and then displaying these values. Including the ability to access image data allowed textures to be applied to surfaces. Adding hardware for interpolating and testing z-depths provided built-in visibility checking. Because of their frequent use, such processes were committed to dedicated hardware to increase performance. More parts of the rendering pipeline, and much more functionality for each, were added in successive generations. Dedicated graphics hardware’s only computational advantage over the CPU is speed, but speed is critical.
Over the past two decades, graphics hardware has undergone an incredible transformation. The ﬁrst consumer graphics chip to include hardware vertex processing (NVIDIA’s GeForce256) shipped in 1999. NVIDIA coined the term graphics processing unit (GPU) to diﬀerentiate the GeForce 256 from the previously available rasterizationonly chips, and it stuck. During the next few years, the GPU evolved from conﬁgurable implementations of a complex ﬁxed-function pipeline to highly programmable blank slates where developers could implement their own algorithms. Programmable shaders of various kinds are the primary means by which the GPU is controlled. For eﬃciency, some parts of the pipeline remain conﬁgurable, not programmable, but the trend is toward programmability and ﬂexibility [175].
GPUs gain their great speed from a focus on a narrow set of highly parallelizable tasks. They have custom silicon dedicated to implementing the z-buﬀer, to rapidly accessing texture images and other buﬀers, and to ﬁnding which pixels are covered by a triangle, for example. How these elements perform their functions is covered in Chapter 23. More important to know early on is how the GPU achieves parallelism for its programmable shaders.
29

30

3. The Graphics Processing Unit

Section 3.3 explains how shaders function. For now, what you need to know is that a shader core is a small processor that does some relatively isolated task, such as transforming a vertex from its location in the world to a screen coordinate, or computing the color of a pixel covered by a triangle. With thousands or millions of triangles being sent to the screen each frame, every second there can be billions of shader invocations, that is, separate instances where shader programs are run.
To begin with, latency is a concern that all processors face. Accessing data takes some amount of time. A basic way to think about latency is that the farther away the information is from the processor, the longer the wait. Section 23.3 covers latency in more detail. Information stored in memory chips will take longer to access than that in local registers. Section 18.4.1 discusses memory access in more depth. The key point is that waiting for data to be retrieved means the processor is stalled, which reduces performance.

3.1 Data-Parallel Architectures
Various strategies are used by diﬀerent processor architectures to avoid stalls. A CPU is optimized to handle a wide variety of data structures and large code bases. CPUs can have multiple processors, but each runs code in a mostly serial fashion, limited SIMD vector processing being the minor exception. To minimize the eﬀect of latency, much of a CPU’s chip consists of fast local caches, memory that is ﬁlled with data likely to be needed next. CPUs also avoid stalls by using clever techniques such as branch prediction, instruction reordering, register renaming, and cache prefetching [715]
GPUs take a diﬀerent approach. Much of a GPU’s chip area is dedicated to a large set of processors, called shader cores, often numbering in the thousands. The GPU is a stream processor, in which ordered sets of similar data are processed in turn. Because of this similarity—a set of vertices or pixels, for example—the GPU can process these data in a massively parallel fashion. One other important element is that these invocations are as independent as possible, such that they have no need for information from neighboring invocations and do not share writable memory locations. This rule is sometimes broken to allow new and useful functionality, but such exceptions come at a price of potential delays, as one processor may wait on another processor to ﬁnish its work.
The GPU is optimized for throughput, deﬁned as the maximum rate at which data can be processed. However, this rapid processing has a cost. With less chip area dedicated to cache memory and control logic, latency for each shader core is generally considerably higher than what a CPU processor encounters [462].
Say a mesh is rasterized and two thousand pixels have fragments to be processed; a pixel shader program is to be invoked two thousand times. Imagine there is only a single shader processor, the world’s weakest GPU. It starts to execute the shader program for the ﬁrst fragment of the two thousand. The shader processor performs a few arithmetic operations on values in registers. Registers are local and quick to

3.1. Data-Parallel Architectures

31

access, so no stall occurs. The shader processor then comes to an instruction such as a texture access; e.g., for a given surface location the program needs to know the pixel color of the image applied to the mesh. A texture is an entirely separate resource, not a part of the pixel program’s local memory, and texture access can be somewhat involved. A memory fetch can take hundreds to thousands of clock cycles, during which time the GPU processor is doing nothing. At this point the shader processor would stall, waiting for the texture’s color value to be returned.
To make this terrible GPU into something considerably better, give each fragment a little storage space for its local registers. Now, instead of stalling on a texture fetch, the shader processor is allowed to switch and execute another fragment, number two of two thousand. This switch is extremely fast, nothing in the ﬁrst or second fragment is aﬀected other than noting which instruction was executing on the ﬁrst. Now the second fragment is executed. Same as with the ﬁrst, a few arithmetic functions are performed, then a texture fetch is again encountered. The shader core now switches to another fragment, number three. Eventually all two thousand fragments are processed in this way. At this point the shader processor returns to fragment number one. By this time the texture color has been fetched and is available for use, so the shader program can then continue executing. The processor proceeds in the same fashion until another instruction that is known to stall execution is encountered, or the program completes. A single fragment will take longer to execute than if the shader processor stayed focused on it, but overall execution time for the fragments as a whole is dramatically reduced.
In this architecture, latency is hidden by having the GPU stay busy by switching to another fragment. GPUs take this design a step further by separating the instruction execution logic from the data. Called single instruction, multiple data (SIMD), this arrangement executes the same command in lock-step on a ﬁxed number of shader programs. The advantage of SIMD is that considerably less silicon (and power) needs to be dedicated to processing data and switching, compared to using an individual logic and dispatch unit to run each program. Translating our two-thousandfragment example into modern GPU terms, each pixel shader invocation for a fragment is called a thread. This type of thread is unlike a CPU thread. It consists of a bit of memory for the input values to the shader, along with any register space needed for the shader’s execution. Threads that use the same shader program are bundled into groups, called warps by NVIDIA and wavefronts by AMD. A warp/wavefront is scheduled for execution by some number GPU shader cores, anywhere from 8 to 64, using SIMD-processing. Each thread is mapped to a SIMD lane.
Say we have two thousand threads to be executed. Warps on NVIDIA GPUs contain 32 threads. This yields 2000/32 = 62.5 warps, which means that 63 warps are allocated, one warp being half empty. A warp’s execution is similar to our single GPU processor example. The shader program is executed in lock-step on all 32 processors. When a memory fetch is encountered, all threads encounter it at the same time, because the same instruction is executed for all. The fetch signals that this warp of threads will stall, all waiting for their (diﬀerent) results. Instead of stalling, the

32

3. The Graphics Processing Unit

warp is swapped out for a diﬀerent warp of 32 threads, which is then executed by the 32 cores. This swapping is just as fast as with our single processor system, as no data within each thread is touched when a warp is swapped in or out. Each thread has its own registers, and each warp keeps track of which instruction it is executing. Swapping in a new warp is just a matter of pointing the set of cores at a diﬀerent set of threads to execute; there is no other overhead. Warps execute or swap out until all are completed. See Figure 3.1.
In our simple example the latency of a memory fetch for a texture can cause a warp to swap out. In reality warps could be swapped out for shorter delays, since the cost of swapping is so low. There are several other techniques used to optimize execution [945], but warp-swapping is the major latency-hiding mechanism used by all GPUs. Several factors are involved in how eﬃciently this process works. For example, if there are few threads, then few warps can be created, making latency hiding problematic.
The shader program’s structure is an important characteristic that inﬂuences eﬃciency. A major factor is the amount of register use for each thread. In our example we assume that two thousand threads can all be resident on the GPU at one time. The more registers needed by the shader program associated with each thread, the fewer threads, and thus the fewer warps, can be resident in the GPU. A shortage of warps can mean that a stall cannot be mitigated by swapping. Warps that are resident are said to be “in ﬂight,” and this number is called the occupancy. High occupancy means that there are many warps available for processing, so that idle processors are less likely. Low occupancy will often lead to poor performance. The frequency of memory fetches also aﬀects how much latency hiding is needed. Lauritzen [993] outlines how occupancy is aﬀected by the number of registers and the shared memory that a shader uses. Wronski [1911, 1914] discusses how the ideal occupancy rate can vary depending on the type of operations a shader performs.
Another factor aﬀecting overall eﬃciency is dynamic branching, caused by “if” statements and loops. Say an “if” statement is encountered in a shader program. If all the threads evaluate and take the same branch, the warp can continue without any concern about the other branch. However, if some threads, or even one thread, take the alternate path, then the warp must execute both branches, throwing away the results not needed by each particular thread [530, 945]. This problem is called thread divergence, where a few threads may need to execute a loop iteration or perform an “if” path that the other threads in the warp do not, leaving them idle during this time.
All GPUs implement these architectural ideas, resulting in systems with strict limitations but massive amounts of compute power per watt. Understanding how this system operates will help you as a programmer make more eﬃcient use of the power it provides. In the sections that follow we discuss how the GPU implements the rendering pipeline, how programmable shaders operate, and the evolution and function of each GPU stage.

3.1. Data-Parallel Architectures

33

time

program: mad mul txr cmp emt

fragment/thread

warps

shader processors
mad mad mad mad

mad mad mad mad

mul mul mul mul mul mul mul mul

txr txr txr txr txr txr txr txr

mad mad mad mad stall & swap

mad mad mad mad

mul mul mul mul

mul mul mul mul

txr txr txr txr

txr txr txr txr

mad mad mad mad

stall & swap

mad mad mad mad

mul mul mul mul

mul mul mul mul

txr txr txr txr

txr txr txr txr

cmp cmp cmp cmp cmp cmp cmp cmp

stall & swap

emt emt emt emt emt emt emt emt

cmp cmp cmp cmp finish & swap cmp cmp cmp cmp

emt emt emt emt cmp cmp cmp cmp

emt emt emt emt
finish & swap cmp cmp cmp cmp

emt emt emt emt

emt emt emt emt

Figure 3.1. Simpliﬁed shader execution example. A triangle’s fragments, called threads, are gathered into warps. Each warp is shown as four threads but have 32 threads in reality. The shader program to be executed is ﬁve instructions long. The set of four GPU shader processors executes these instructions for the ﬁrst warp until a stall condition is detected on the “txr” command, which needs time to fetch its data. The second warp is swapped in and the shader program’s ﬁrst three instructions are applied to it, until a stall is again detected. After the third warp is swapped in and stalls, execution continues by swapping in the ﬁrst warp and continuing execution. If its “txr” command’s data are not yet returned at this point, execution truly stalls until these data are available. Each warp ﬁnishes in turn.

34

3. The Graphics Processing Unit

Vertex Shader

Tessellation

Geometry Shader

Clipping

Screen Mapping

Triangle Setup & Traversal

Pixel Shader

Merger

Figure 3.2. GPU implementation of the rendering pipeline. The stages are color coded according to the degree of user control over their operation. Green stages are fully programmable. Dashed lines show optional stages. Yellow stages are conﬁgurable but not programmable, e.g., various blend modes can be set for the merge stage. Blue stages are completely ﬁxed in their function.

3.2 GPU Pipeline Overview
The GPU implements the conceptual geometry processing, rasterization, and pixel processing pipeline stages described in Chapter 2. These are divided into several hardware stages with varying degrees of conﬁgurability or programmability. Figure 3.2 shows the various stages color coded according to how programmable or conﬁgurable they are. Note that these physical stages are split up somewhat diﬀerently than the functional stages presented in Chapter 2.
We describe here the logical model of the GPU, the one that is exposed to you as a programmer by an API. As Chapters 18 and 23 discuss, the implementation of this logical pipeline, the physical model, is up to the hardware vendor. A stage that is ﬁxed-function in the logical model may be executed on the GPU by adding commands to an adjacent programmable stage. A single program in the pipeline may be split into elements executed by separate sub-units, or be executed by a separate pass entirely. The logical model can help you reason about what aﬀects performance, but it should not be mistaken for the way the GPU actually implements the pipeline.
The vertex shader is a fully programmable stage that is used to implement the geometry processing stage. The geometry shader is a fully programmable stage that operates on the vertices of a primitive (point, line, or triangle). It can be used to perform per-primitive shading operations, to destroy primitives, or to create new ones. The tessellation stage and geometry shader are both optional, and not all GPUs support them, especially on mobile devices.
The clipping, triangle setup, and triangle traversal stages are implemented by ﬁxed-function hardware. Screen mapping is aﬀected by window and viewport settings, internally forming a simple scale and repositioning. The pixel shader stage is fully programmable. Although the merger stage is not programmable, it is highly conﬁgurable and can be set to perform a wide variety of operations. It implements the “merging” functional stage, in charge of modifying the color, z-buﬀer, blend, stencil, and any other output-related buﬀers. The pixel shader execution together with the merger stage form the conceptual pixel processing stage presented in Chapter 2.
Over time, the GPU pipeline has evolved away from hard-coded operation and toward increasing ﬂexibility and control. The introduction of programmable shader stages was the most important step in this evolution. The next section describes the features common to the various programmable stages.

3.3. The Programmable Shader Stage

35

3.3 The Programmable Shader Stage
Modern shader programs use a uniﬁed shader design. This means that the vertex, pixel, geometry, and tessellation-related shaders share a common programming model. Internally they have the same instruction set architecture (ISA). A processor that implements this model is called a common-shader core in DirectX, and a GPU with such cores is said to have a uniﬁed shader architecture. The idea behind this type of architecture is that shader processors are usable in a variety of roles, and the GPU can allocate these as it sees ﬁt. For example, a set of meshes with tiny triangles will need more vertex shader processing than large squares each made of two triangles. A GPU with separate pools of vertex and pixel shader cores means that the ideal work distribution to keep all the cores busy is rigidly predetermined. With uniﬁed shader cores, the GPU can decide how to balance this load.
Describing the entire shader programming model is well beyond the scope of this book, and there are many documents, books, and websites that already do so. Shaders are programmed using C-like shading languages such as DirectX’s High-Level Shading Language (HLSL) and the OpenGL Shading Language (GLSL). DirectX’s HLSL can be compiled to virtual machine bytecode, also called the intermediate language (IL or DXIL), to provide hardware independence. An intermediate representation can also allow shader programs to be compiled and stored oﬄine. This intermediate language is converted to the ISA of the speciﬁc GPU by the driver. Console programming usually avoids the intermediate language step, since there is then only one ISA for the system.
The basic data types are 32-bit single-precision ﬂoating point scalars and vectors, though vectors are only part of the shader code and are not supported in hardware as outlined above. On modern GPUs 32-bit integers and 64-bit ﬂoats are also supported natively. Floating point vectors typically contain data such as positions (xyzw), normals, matrix rows, colors (rgba), or texture coordinates (uvwq). Integers are most often used to represent counters, indices, or bitmasks. Aggregate data types such as structures, arrays, and matrices are also supported.
A draw call invokes the graphics API to draw a group of primitives, so causing the graphics pipeline to execute and run its shaders. Each programmable shader stage has two types of inputs: uniform inputs, with values that remain constant throughout a draw call (but can be changed between draw calls), and varying inputs, data that come from the triangle’s vertices or from rasterization. For example, a pixel shader may provide the color of a light source as a uniform value, and the triangle surface’s location changes per pixel and so is varying. A texture is a special kind of uniform input that once was always a color image applied to a surface, but that now can be thought of as any large array of data.
The underlying virtual machine provides special registers for the diﬀerent types of inputs and outputs. The number of available constant registers for uniforms is much larger than those registers available for varying inputs or outputs. This happens because the varying inputs and outputs need to be stored separately for each vertex

36
16 / 16 / 32 registers
Varying Input Registers

4096 registers
Temporary Registers

3. The Graphics Processing Unit

Shader Virtual Machine

16 / 32 / 8 registers
Output Registers

Constant Registers
16 buffers of 4096 registers

Textures
128 arrays of 512 textures

Figure 3.3. Uniﬁed virtual machine architecture and register layout, under Shader Model 4.0. The maximum available number is indicated next to each resource. Three numbers separated by slashes refer to the limits for vertex, geometry, and pixel shaders (from left to right).

or pixel, so there is a natural limit as to how many are needed. The uniform inputs are stored once and reused across all the vertices or pixels in the draw call. The virtual machine also has general-purpose temporary registers, which are used for scratch space. All types of registers can be array-indexed using integer values in temporary registers. The inputs and outputs of the shader virtual machine can be seen in Figure 3.3.
Operations that are common in graphics computations are eﬃciently executed on modern GPUs. Shading languages expose the most common of these operations (such as additions and multiplications) via operators such as * and +. The rest are exposed through intrinsic functions, e.g., atan(), sqrt(), log(), and many others, optimized for the GPU. Functions also exist for more complex operations, such as vector normalization and reﬂection, the cross product, and matrix transpose and determinant computations.
The term ﬂow control refers to the use of branching instructions to change the ﬂow of code execution. Instructions related to ﬂow control are used to implement high-level language constructs such as “if” and “case” statements, as well as various types of loops. Shaders support two types of ﬂow control. Static ﬂow control branches are based on the values of uniform inputs. This means that the ﬂow of the code is constant over the draw call. The primary beneﬁt of static ﬂow control is to allow the same shader to be used in a variety of diﬀerent situations (e.g., a varying numbers of lights). There is no thread divergence, since all invocations take the same code path. Dynamic ﬂow control is based on the values of varying inputs, meaning that each

3.4. The Evolution of Programmable Shading and APIs

37

fragment can execute the code diﬀerently. This is much more powerful than static ﬂow control but can cost performance, especially if the code ﬂow changes erratically between shader invocations.

3.4 The Evolution of Programmable Shading and APIs
The idea of a framework for programmable shading dates back to 1984 with Cook’s shade trees [287]. A simple shader and its corresponding shade tree are shown in Figure 3.4. The RenderMan Shading Language [63, 1804] was developed from this idea in the late 1980s. It is still used today for ﬁlm production rendering, along with other evolving speciﬁcations, such as the Open Shading Language (OSL) project [608].
Consumer-level graphics hardware was ﬁrst successfully introduced by 3dfx Interactive on October 1, 1996. See Figure 3.5 for a timeline from this year. Their Voodoo graphics card’s ability to render the game Quake with high quality and performance led to its quick adoption. This hardware implemented a ﬁxed-function pipeline throughout. Before GPUs supported programmable shaders natively, there were several attempts to implement programmable shading operations in real time via multiple rendering passes. The Quake III: Arena scripting language was the ﬁrst widespread

final color

float ka=0.5, ks=0.5; float roughness=0.1;

float intensity;

color copper=(0.8,0.3,0.1);

*

intensity = ka*ambient() +

ks*specular(normal,view,roughness);

copper

final_color = intensity*copper;
+

color

*

*

weight of ambient component

ambient

weight of specular component

specular function
normal view surface roughness

Figure 3.4. Shade tree for a simple copper shader, and its corresponding shader language program. (After Cook [287].)

38

3. The Graphics Processing Unit

OpenGL 1.1

OpenGL ES 1.0

1996

PLAYSTATION 2 Xbox

OpenGL ES 2.0

Wii,

PS 3

iPad

2006

OpenGL ES 3.0

Vulkan

Metal

2018

SM 3.0

Mantle

3dfx Voodoo

DirectX 8.0

Xbox 360

DX 11.0 SM 5.0

start PS 4,

Rift, Vive

GeForce256 DX 9.0 DX 10.0

Xbox One WebGL 2

“GPU”

SM 2.0

SM 4.0

WebGL

DX 12.0

Figure 3.5. A timeline of some API and graphics hardware releases.

commercial success in this area in 1999. As mentioned at the beginning of the chapter, NVIDIA’s GeForce256 was the ﬁrst hardware to be called a GPU, but it was not programmable. However, it was conﬁgurable.
In early 2001, NVIDIA’s GeForce 3 was the ﬁrst GPU to support programmable vertex shaders [1049], exposed through DirectX 8.0 and extensions to OpenGL. These shaders were programmed in an assembly-like language that was converted by the drivers into microcode on the ﬂy. Pixel shaders were also included in DirectX 8.0, but pixel shaders fell short of actual programmability—the limited “programs” supported were converted into texture blending states by the driver, which in turn wired together hardware “register combiners.” These “programs” were not only limited in length (12 instructions or less) but also lacked important functionality. Dependent texture reads and ﬂoating point data were identiﬁed by Peercy et al. [1363] as crucial to true programmability, from their study of RenderMan.
Shaders at this time did not allow for ﬂow control (branching), so conditionals had to be emulated by computing both terms and selecting or interpolating between the results. DirectX deﬁned the concept of a Shader Model (SM) to distinguish hardware with diﬀerent shader capabilities. The year 2002 saw the release of DirectX 9.0 including Shader Model 2.0, which featured truly programmable vertex and pixel shaders. Similar functionality was also exposed under OpenGL using various extensions. Support for arbitrary dependent texture reads and storage of 16-bit ﬂoating point values was added, ﬁnally completing the set of requirements identiﬁed by Peercy et al. Limits on shader resources such as instructions, textures, and registers were increased, so shaders became capable of more complex eﬀects. Support for ﬂow control was also added. The growing length and complexity of shaders made the assembly programming model increasingly cumbersome. Fortunately, DirectX 9.0

3.4. The Evolution of Programmable Shading and APIs

39

also included HLSL. This shading language was developed by Microsoft in collaboration with NVIDIA. Around the same time, the OpenGL ARB (Architecture Review Board) released GLSL, a fairly similar language for OpenGL [885]. These languages were heavily inﬂuenced by the syntax and design philosophy of the C programming language and included elements from the RenderMan Shading Language.
Shader Model 3.0 was introduced in 2004 and added dynamic ﬂow control, making shaders considerably more powerful. It also turned optional features into requirements, further increased resource limits and added limited support for texture reads in vertex shaders. When a new generation of game consoles was introduced in late 2005 (Microsoft’s Xbox 360) and late 2006 (Sony Computer Entertainment’s PLAYSTATION 3 system), they were equipped with Shader Model 3.0–level GPUs. Nintendo’s Wii console was one of the last notable ﬁxed-function GPUs, which initially shipped in late 2006. The purely ﬁxed-function pipeline is long gone at this point. Shader languages have evolved to a point where a variety of tools are used to create and manage them. A screenshot of one such tool, using Cook’s shade tree concept, is shown in Figure 3.6.
The next large step in programmability also came near the end of 2006. Shader Model 4.0, included in DirectX 10.0 [175], introduced several major features, such as the geometry shader and stream output. Shader Model 4.0 included a uniform

Figure 3.6. A visual shader graph system for shader design. Various operations are encapsulated in function boxes, selectable on the left. When selected, each function box has adjustable parameters, shown on the right. Inputs and outputs for each function box are linked to each other to form the ﬁnal result, shown in the lower right of the center frame. (Screenshot from “mental mill,” mental images inc.)

40

3. The Graphics Processing Unit

programming model for all shaders (vertex, pixel, and geometry), the uniﬁed shader design described earlier. Resource limits were further increased, and support for integer data types (including bitwise operations) was added. The introduction of GLSL 3.30 in OpenGL 3.3 provided a similar shader model.
In 2009 DirectX 11 and Shader Model 5.0 were released, adding the tessellation stage shaders and the compute shader, also called DirectCompute. The release also focused on supporting CPU multiprocessing more eﬀectively, a topic discussed in Section 18.5. OpenGL added tessellation in version 4.0 and compute shaders in 4.3. DirectX and OpenGL evolve diﬀerently. Both set a certain level of hardware support needed for a particular version release. Microsoft controls the DirectX API and so works directly with independent hardware vendors (IHVs) such as AMD, NVIDIA, and Intel, as well as game developers and computer-aided design software ﬁrms, to determine what features to expose. OpenGL is developed by a consortium of hardware and software vendors, managed by the nonproﬁt Khronos Group. Because of the number of companies involved, the API features often appear in a release of OpenGL some time after their introduction in DirectX. However, OpenGL allows extensions, vendor-speciﬁc or more general, that allow the latest GPU functions to be used before oﬃcial support in a release.
The next signiﬁcant change in APIs was led by AMD’s introduction of the Mantle API in 2013. Developed in partnership with video game developer DICE, the idea of Mantle was to strip out much of the graphics driver’s overhead and give this control directly to the developer. Alongside this refactoring was further support for eﬀective CPU multiprocessing. This new class of APIs focuses on vastly reducing the time the CPU spends in the driver, along with more eﬃcient CPU multiprocessor support (Chapter 18). The ideas pioneered in Mantle were picked up by Microsoft and released as DirectX 12 in 2015. Note that DirectX 12 is not focused on exposing new GPU functionality—DirectX 11.3 exposed the same hardware features. Both APIs can be used to send graphics to virtual reality systems such as the Oculus Rift and HTC Vive. However, DirectX 12 is a radical redesign of the API, one that better maps to modern GPU architectures. Low-overhead drivers are useful for applications where the CPU driver cost is causing a bottleneck, or where using more CPU processors for graphics could beneﬁt performance [946]. Porting from earlier APIs can be diﬃcult, and a naive implementation can result in lower performance [249, 699, 1438].
Apple released its own low-overhead API called Metal in 2014. Metal was ﬁrst available on mobile devices such as the iPhone 5S and iPad Air, with newer Macintoshes given access a year later through OS X El Capitan. Beyond eﬃciency, reducing CPU usage saves power, an important factor on mobile devices. This API has its own shading language, meant for both graphics and GPU compute programs.
AMD donated its Mantle work to the Khronos Group, which released its own new API in early 2016, called Vulkan. As with OpenGL, Vulkan works on multiple operating systems. Vulkan uses a new high-level intermediate language called SPIRV, which is used for both shader representation and for general GPU computing. Precompiled shaders are portable and so can be used on any GPU supporting the

3.4. The Evolution of Programmable Shading and APIs

41

capabilities needed [885]. Vulkan can also be used for non-graphical GPU computation, as it does not need a display window [946]. One notable diﬀerence of Vulkan from other low-overhead drivers is that it is meant to work with a wide range of systems, from workstations to mobile devices.
On mobile devices the norm has been to use OpenGL ES. “ES” stands for Embedded Systems, as this API was developed with mobile devices in mind. Standard OpenGL at the time was rather bulky and slow in some of its call structures, as well as requiring support for rarely used functionality. Released in 2003, OpenGL ES 1.0 was a stripped-down version of OpenGL 1.3, describing a ﬁxed-function pipeline. While releases of DirectX are timed with those of graphics hardware that support them, developing graphics support for mobile devices did not proceed in the same fashion. For example, the ﬁrst iPad, released in 2010, implemented OpenGL ES 1.1. In 2007 the OpenGL ES 2.0 speciﬁcation was released, providing programmable shading. It was based on OpenGL 2.0, but without the ﬁxed-function component, and so was not backward-compatible with OpenGL ES 1.1. OpenGL ES 3.0 was released in 2012, providing functionality such as multiple render targets, texture compression, transform feedback, instancing, and a much wider range of texture formats and modes, as well as shader language improvements. OpenGL ES 3.1 adds compute shaders, and 3.2 adds geometry and tessellation shaders, among other features. Chapter 23 discusses mobile device architectures in more detail.
An oﬀshoot of OpenGL ES is the browser-based API WebGL, called through JavaScript. Released in 2011, the ﬁrst version of this API is usable on most mobile devices, as it is equivalent to OpenGL ES 2.0 in functionality. As with OpenGL, extensions give access to more advanced GPU features. WebGL 2 assumes OpenGL ES 3.0 support.
WebGL is particularly well suited for experimenting with features or use in the classroom:
• It is cross-platform, working on all personal computers and almost all mobile devices.
• Driver approval is handled by the browsers. Even if one browser does not support a particular GPU or extension, often another browser does.
• Code is interpreted, not compiled, and only a text editor is needed for development.
• A debugger is built in to most browsers, and code running at any website can be examined.
• Programs can be deployed by uploading them to a website or Github, for example.
Higher-level scene-graph and eﬀects libraries such as three.js [218] give easy access to code for a variety of more involved eﬀects such as shadow algorithms, post-processing eﬀects, physically based shading, and deferred rendering.

42

3. The Graphics Processing Unit

3.5 The Vertex Shader
The vertex shader is the ﬁrst stage in the functional pipeline shown in Figure 3.2. While this is the ﬁrst stage directly under programmer control, it is worth noting that some data manipulation happens before this stage. In what DirectX calls the input assembler [175, 530, 1208], several streams of data can be woven together to form the sets of vertices and primitives sent down the pipeline. For example, an object could be represented by one array of positions and one array of colors. The input assembler would create this object’s triangles (or lines or points) by creating vertices with positions and colors. A second object could use the same array of positions (along with a diﬀerent model transform matrix) and a diﬀerent array of colors for its representation. Data representation is discussed in detail in Section 16.4.5. There is also support in the input assembler to perform instancing. This allows an object to be drawn several times with some varying data per instance, all with a single draw call. The use of instancing is covered in Section 18.4.2.
A triangle mesh is represented by a set of vertices, each associated with a speciﬁc position on the model surface. Besides position, there are other optional properties associated with each vertex, such as a color or texture coordinates. Surface normals are deﬁned at mesh vertices as well, which may seem like an odd choice. Mathematically, each triangle has a well-deﬁned surface normal, and it may seem to make more sense to use the triangle’s normal directly for shading. However, when rendering, triangle meshes are often used to represent an underlying curved surface, and vertex normals are used to represent the orientation of this surface, rather than that of the triangle mesh itself. Section 16.3.4 will discuss methods to compute vertex normals. Figure 3.7 shows side views of two triangle meshes that represent curved surfaces, one smooth and one with a sharp crease.
The vertex shader is the ﬁrst stage to process the triangle mesh. The data describing what triangles are formed is unavailable to the vertex shader. As its name implies, it deals exclusively with the incoming vertices. The vertex shader provides a way

Figure 3.7. Side views of triangle meshes (in black, with vertex normals) representing curved surfaces (in red). On the left smoothed vertex normals are used to represent a smooth surface. On the right the middle vertex has been duplicated and given two normals, representing a crease.

3.5. The Vertex Shader

43

to modify, create, or ignore values associated with each triangle’s vertex, such as its color, normal, texture coordinates, and position. Normally the vertex shader program transforms vertices from model space to homogeneous clip space (Section 4.7). At a minimum, a vertex shader must always output this location.
A vertex shader is much the same as the uniﬁed shader described earlier. Every vertex passed in is processed by the vertex shader program, which then outputs a number of values that are interpolated across a triangle or line. The vertex shader can neither create nor destroy vertices, and results generated by one vertex cannot be passed on to another vertex. Since each vertex is treated independently, any number of shader processors on the GPU can be applied in parallel to the incoming stream of vertices.
Input assembly is usually presented as a process that happens before the vertex shader is executed. This is an example where the physical model often diﬀers from the logical. Physically, the fetching of data to create a vertex might happen in the vertex shader and the driver will quietly prepend every shader with the appropriate instructions, invisible to the programmer.
Chapters that follow explain several vertex shader eﬀects, such as vertex blending for animating joints, and silhouette rendering. Other uses for the vertex shader include:
• Object generation, by creating a mesh only once and having it be deformed by the vertex shader.
• Animating character’s bodies and faces using skinning and morphing techniques.
• Procedural deformations, such as the movement of ﬂags, cloth, or water [802, 943].
• Particle creation, by sending degenerate (no area) meshes down the pipeline and having these be given an area as needed.
• Lens distortion, heat haze, water ripples, page curls, and other eﬀects, by using the entire framebuﬀer’s contents as a texture on a screen-aligned mesh undergoing procedural deformation.
• Applying terrain height ﬁelds by using vertex texture fetch [40, 1227].
Some deformations done using a vertex shader are shown in Figure 3.8. The output of the vertex shader can be consumed in several diﬀerent ways. The
usual path is for each instance’s primitives, e.g., triangles, to then be generated and rasterized, and the individual pixel fragments produced to be sent to the pixel shader program for continued processing. On some GPUs the data can also be sent to the tessellation stage or the geometry shader or be stored in memory. These optional stages are discussed in the following sections.

44

3. The Graphics Processing Unit

Figure 3.8. On the left, a normal teapot. A simple shear operation performed by a vertex shader program produces the middle image. On the right, a noise function creates a ﬁeld that distorts the model. (Images produced by FX Composer 2, courtesy of NVIDIA Corporation.)
3.6 The Tessellation Stage
The tessellation stage allows us to render curved surfaces. The GPU’s task is to take each surface description and turn it into a representative set of triangles. This stage is an optional GPU feature that ﬁrst became available in (and is required by) DirectX 11. It is also supported in OpenGL 4.0 and OpenGL ES 3.2.
There are several advantages to using the tessellation stage. The curved surface description is often more compact than providing the corresponding triangles themselves. Beyond memory savings, this feature can keep the bus between CPU and GPU from becoming the bottleneck for an animated character or object whose shape is changing each frame. The surfaces can be rendered eﬃciently by having an appropriate number of triangles generated for the given view. For example, if a ball is far from the camera, only a few triangles are needed. Up close, it may look best represented with thousands of triangles. This ability to control the level of detail can also allow an application to control its performance, e.g., using a lower-quality mesh on weaker GPUs in order to maintain frame rate. Models normally represented by ﬂat surfaces can be converted to ﬁne meshes of triangles and then warped as desired [1493], or they can be tessellated in order to perform expensive shading computations less frequently [225].
The tessellation stage always consists of three elements. Using DirectX’s terminology, these are the hull shader, tessellator, and domain shader. In OpenGL the hull shader is the tessellation control shader and the domain shader the tessellation evaluation shader, which are a bit more descriptive, though verbose. The ﬁxed-function tessellator is called the primitive generator in OpenGL, and as will be seen, that is indeed what it does.
How to specify and tessellate curves and surfaces is discussed at length in Chapter 17. Here we give a brief summary of each tessellation stage’s purpose. To begin, the input to the hull shader is a special patch primitive. This consists of several control points deﬁning a subdivision surface, B´ezier patch, or other type of curved element. The hull shader has two functions. First, it tells the tessellator how many triangles should be generated, and in what conﬁguration. Second, it performs processing on each of the control points. Also, optionally, the hull shader can modify the incoming

3.6. The Tessellation Stage

45

input patch

tessellator
TFs & type

generated points

hull shader
control points, TFs, & constants

transformed patch

domain shader
output mesh

Figure 3.9. The tessellation stage. The hull shader takes in a patch deﬁned by control points. It sends the tessellation factors (TFs) and type to the ﬁxed-function tessellator. The control point set is transformed as desired by the hull shader and sent on to the domain shader, along with TFs and related patch constants. The tessellator creates the set of vertices along with their barycentric coordinates. These are then processed by the domain shader, producing the triangle mesh (control points shown for reference).

patch description, adding or removing control points as desired. The hull shader outputs its set of control points, along with the tessellation control data, to the domain shader. See Figure 3.9.
The tessellator is a ﬁxed-function stage in the pipeline, only used with tessellation shaders. It has the task of adding several new vertices for the domain shader to process. The hull shader sends the tessellator information about what type of tessellation surface is desired: triangle, quadrilateral, or isoline. Isolines are sets of line strips, sometimes used for hair rendering [1954]. The other important values sent by the hull shader are the tessellation factors (tessellation levels in OpenGL). These are of two types: inner and outer edge. The two inner factors determine how much tessellation occurs inside the triangle or quadrilateral. The outer factors determine how much each exterior edge is split (Section 17.6). An example of increasing tessellation factors is shown in Figure 3.10. By allowing separate controls, we can have adjacent curved surfaces’ edges match in tessellation, regardless of how the interiors are tessellated. Matching edges avoids cracks or other shading artifacts where patches meet. The vertices are assigned barycentric coordinates (Section 22.8), which are values that specify a relative location for each point on the desired surface.
The hull shader always outputs a patch, a set of control point locations. However, it can signal that a patch is to be discarded by sending the tessellator an outer tessellation level of zero or less (or not-a-number, NaN). Otherwise, the tessellator generates a mesh and sends it to the domain shader. The control points for the curved surface from the hull shader are used by each invocation of the domain shader to compute the

46

3. The Graphics Processing Unit

Figure 3.10. The eﬀect of varying the tessellation factors. The Utah teapot is made of 32 patches. Inner and outer tessellation factors, from left to right, are 1, 2, 4, and 8. (Images generated by demo from Rideout and Van Gelder [1493].)
output values for each vertex. The domain shader has a data ﬂow pattern like that of a vertex shader, with each input vertex from the tessellator being processed and generating a corresponding output vertex. The triangles formed are then passed on down the pipeline.
While this system sounds complex, it is structured this way for eﬃciency, and each shader can be fairly simple. The patch passed into a hull shader will often undergo little or no modiﬁcation. This shader may also use the patch’s estimated distance or screen size to compute tessellation factors on the ﬂy, as for terrain rendering [466]. Alternately, the hull shader may simply pass on a ﬁxed set of values for all patches that the application computes and provides. The tessellator performs an involved but ﬁxed-function process of generating the vertices, giving them positions, and specifying what triangles or lines they form. This data ampliﬁcation step is performed outside of a shader for computational eﬃciency [530]. The domain shader takes the barycentric coordinates generated for each point and uses these in the patch’s evaluation equation to generate the position, normal, texture coordinates, and other vertex information desired. See Figure 3.11 for an example.

Figure 3.11. On the left is the underlying mesh of about 6000 triangles. On the right, each triangle is tessellated and displaced using PN triangle subdivision. (Images from NVIDIA SDK 11 [1301] samples, courtesy of NVIDIA Corporation, model from Metro 2033 by 4A Games.)

3.7. The Geometry Shader

47

Figure 3.12. Geometry shader input for a geometry shader program is of some single type: point, line segment, triangle. The two rightmost primitives include vertices adjacent to the line and triangle objects. More elaborate patch types are possible.
3.7 The Geometry Shader
The geometry shader can turn primitives into other primitives, something the tessellation stage cannot do. For example, a triangle mesh could be transformed to a wireframe view by having each triangle create line edges. Alternately, the lines could be replaced by quadrilaterals facing the viewer, so making a wireframe rendering with thicker edges [1492]. The geometry shader was added to the hardware-accelerated graphics pipeline with the release of DirectX 10, in late 2006. It is located after the tessellation shader in the pipeline, and its use is optional. While a required part of Shader Model 4.0, it is not used in earlier shader models. OpenGL 3.2 and OpenGL ES 3.2 support this type of shader as well.
The input to the geometry shader is a single object and its associated vertices. The object typically consists of triangles in a strip, a line segment, or simply a point. Extended primitives can be deﬁned and processed by the geometry shader. In particular, three additional vertices outside of a triangle can be passed in, and the two adjacent vertices on a polyline can be used. See Figure 3.12. With DirectX 11 and Shader Model 5.0, you can pass in more elaborate patches, with up to 32 control points. That said, the tessellation stage is more eﬃcient for patch generation [175].
The geometry shader processes this primitive and outputs zero or more vertices, which are treated as points, polylines, or strips of triangles. Note that no output at all can be generated by the geometry shader. In this way, a mesh can be selectively modiﬁed by editing vertices, adding new primitives, and removing others.
The geometry shader is designed for modifying incoming data or making a limited number of copies. For example, one use is to generate six transformed copies of data to simultaneously render the six faces of a cube map; see Section 10.4.3. It can also be used to eﬃciently create cascaded shadow maps for high-quality shadow generation. Other algorithms that take advantage of the geometry shader include creating variablesized particles from point data, extruding ﬁns along silhouettes for fur rendering, and ﬁnding object edges for shadow algorithms. See Figure 3.13 for more examples. These and other uses are discussed throughout the rest of the book.
DirectX 11 added the ability for the geometry shader to use instancing, where the geometry shader can be run a set number of times on any given primitive [530, 1971]. In

48

3. The Graphics Processing Unit

Figure 3.13. Some uses of the geometry shader (GS). On the left, metaball isosurface tessellation is performed on the ﬂy using the GS. In the middle, fractal subdivision of line segments is done using the GS and stream out, and billboards are generated by the GS for display of the lightning. On the right, cloth simulation is performed by using the vertex and geometry shader with stream out. (Images from NVIDIA SDK 10 [1300] samples, courtesy of NVIDIA Corporation.)
OpenGL 4.0 this is speciﬁed with an invocation count. The geometry shader can also output up to four streams. One stream can be sent on down the rendering pipeline for further processing. All these streams can optionally be sent to stream output render targets.
The geometry shader is guaranteed to output results from primitives in the same order that they are input. This aﬀects performance, because if several shader cores run in parallel, results must be saved and ordered. This and other factors work against the geometry shader being used to replicate or create a large amount of geometry in a single call [175, 530].
After a draw call is issued, there are only three places in the pipeline where work can be created on the GPU: rasterization, the tessellation stage, and the geometry shader. Of these, the geometry shader’s behavior is the least predictable when considering resources and memory needed, since it is fully programmable. In practice the geometry shader usually sees little use, as it does not map well to the GPU’s strengths. On some mobile devices it is implemented in software, so its use is actively discouraged there [69].
3.7.1 Stream Output
The standard use of the GPU’s pipeline is to send data through the vertex shader, then rasterize the resulting triangles and process these in the pixel shader. It used to be that the data always passed through the pipeline and intermediate results could not be accessed. The idea of stream output was introduced in Shader Model 4.0. After vertices are processed by the vertex shader (and, optionally, the tessellation and geometry shaders), these can be output in a stream, i.e., an ordered array, in addition to being sent on to the rasterization stage. Rasterization could, in fact, be turned oﬀ entirely and the pipeline then used purely as a non-graphical stream processor. Data

3.8. The Pixel Shader

49

processed in this way can be sent back through the pipeline, thus allowing iterative processing. This type of operation can be useful for simulating ﬂowing water or other particle eﬀects, as discussed in Section 13.8. It could also be used to skin a model and then have these vertices available for reuse (Section 4.4).
Stream output returns data only in the form of ﬂoating point numbers, so it can have a noticeable memory cost. Stream output works on primitives, not directly on vertices. If meshes are sent down the pipeline, each triangle generates its own set of three output vertices. Any vertex sharing in the original mesh is lost. For this reason a more typical use is to send just the vertices through the pipeline as a point set primitive. In OpenGL the stream output stage is called transform feedback, since the focus of much of its use is transforming vertices and returning them for further processing. Primitives are guaranteed to be sent to the stream output target in the order that they were input, meaning the vertex order will be maintained [530].

3.8 The Pixel Shader
After the vertex, tessellation, and geometry shaders perform their operations, the primitive is clipped and set up for rasterization, as explained in the previous chapter. This section of the pipeline is relatively ﬁxed in its processing steps, i.e., not programmable but somewhat conﬁgurable. Each triangle is traversed to determine which pixels it covers. The rasterizer may also roughly calculate how much the triangle covers each pixel’s cell area (Section 5.4.2). This piece of a triangle partially or fully overlapping the pixel is called a fragment.
The values at the triangle’s vertices, including the z-value used in the z-buﬀer, are interpolated across the triangle’s surface for each pixel. These values are passed to the pixel shader, which then processes the fragment. In OpenGL the pixel shader is known as the fragment shader, which is perhaps a better name. We use “pixel shader” throughout this book for consistency. Point and line primitives sent down the pipeline also create fragments for the pixels covered.
The type of interpolation performed across the triangle is speciﬁed by the pixel shader program. Normally we use perspective-correct interpolation, so that the worldspace distances between pixel surface locations increase as an object recedes in the distance. An example is rendering railroad tracks extending to the horizon. Railroad ties are more closely spaced where the rails are farther away, as more distance is traveled for each successive pixel approaching the horizon. Other interpolation options are available, such as screen-space interpolation, where perspective projection is not taken into account. DirectX 11 gives further control over when and how interpolation is performed [530].
In programming terms, the vertex shader program’s outputs, interpolated across the triangle (or line), eﬀectively become the pixel shader program’s inputs. As the GPU has evolved, other inputs have been exposed. For example, the screen position of the fragment is available to the pixel shader in Shader Model 3.0 and beyond. Also,

50

3. The Graphics Processing Unit

Figure 3.14. User-deﬁned clipping planes. On the left, a single horizontal clipping plane slices the object. In the middle, the nested spheres are clipped by three planes. On the right, the spheres’ surfaces are clipped only if they are outside all three clip planes. (From the three.js examples webgl clipping and webgl clipping intersection [218].)
which side of a triangle is visible is an input ﬂag. This knowledge is important for rendering a diﬀerent material on the front versus back of each triangle in a single pass.
With inputs in hand, typically the pixel shader computes and outputs a fragment’s color. It can also possibly produce an opacity value and optionally modify its z-depth. During merging, these values are used to modify what is stored at the pixel. The depth value generated in the rasterization stage can also be modiﬁed by the pixel shader. The stencil buﬀer value is usually not modiﬁable, but rather it is passed through to the merge stage. DirectX 11.3 allows the shader to change this value. Operations such as fog computation and alpha testing have moved from being merge operations to being pixel shader computations in SM 4.0 [175].
A pixel shader also has the unique ability to discard an incoming fragment, i.e., generate no output. One example of how fragment discard can be used is shown in Figure 3.14. Clip plane functionality used to be a conﬁgurable element in the ﬁxedfunction pipeline and was later speciﬁed in the vertex shader. With fragment discard available, this functionality could then be implemented in any way desired in the pixel shader, such as deciding whether clipping volumes should be AND’ed or OR’ed together.
Initially the pixel shader could output to only the merging stage, for eventual display. The number of instructions a pixel shader can execute has grown considerably over time. This increase gave rise to the idea of multiple render targets (MRT). Instead of sending results of a pixel shader’s program to just the color and z-buﬀer, multiple sets of values could be generated for each fragment and saved to diﬀerent buﬀers, each called a render target. Render targets generally have the same x- and y-dimensions; some APIs allow diﬀerent sizes, but the rendered area will be the smallest of these. Some architectures require render targets to each have the same bit depth, and possibly even identical data formats. Depending on the GPU, the number of render targets available is four or eight.

3.8. The Pixel Shader

51

Even with these limitations, MRT functionality is a powerful aid in performing rendering algorithms more eﬃciently. A single rendering pass could generate a color image in one target, object identiﬁers in another, and world-space distances in a third. This ability has also given rise to a diﬀerent type of rendering pipeline, called deferred shading, where visibility and shading are done in separate passes. The ﬁrst pass stores data about an object’s location and material at each pixel. Successive passes can then eﬃciently apply illumination and other eﬀects. This class of rendering methods is described in Section 20.1.
The pixel shader’s limitation is that it can normally write to a render target at only the fragment location handed to it, and cannot read current results from neighboring pixels. That is, when a pixel shader program executes, it cannot send its output directly to neighboring pixels, nor can it access others’ recent changes. Rather, it computes results that aﬀect only its own pixel. However, this limitation is not as severe as it sounds. An output image created in one pass can have any of its data accessed by a pixel shader in a later pass. Neighboring pixels can be processed using image processing techniques, described in Section 12.1.
There are exceptions to the rule that a pixel shader cannot know or aﬀect neighboring pixels’ results. One is that the pixel shader can immediately access information for adjacent fragments (albeit indirectly) during the computation of gradient or derivative information. The pixel shader is provided with the amounts by which any interpolated value changes per pixel along the x and y screen axes. Such values are useful for various computations and texture addressing. These gradients are particularly important for operations such as texture ﬁltering (Section 6.2.2), where we want to know how much of an image covers a pixel. All modern GPUs implement this feature by processing fragments in groups of 2 × 2, called a quad. When the pixel shader requests a gradient value, the diﬀerence between adjacent fragments is returned. See Figure 3.15. A uniﬁed core has this capability to access neighboring data—kept in diﬀerent threads on the same warp—and so can compute gradients for use in the pixel shader. One consequence of this implementation is that gradient information cannot be accessed in parts of the shader aﬀected by dynamic ﬂow control, i.e., an “if” statement or loop with a variable number of iterations. All the fragments in a group must be processed using the same set of instructions so that all four pixels’ results are meaningful for computing gradients. This is a fundamental limitation that exists even in oﬄine rendering systems [64].
DirectX 11 introduced a buﬀer type that allows write access to any location, the unordered access view (UAV). Originally for only pixel and compute shaders, access to UAVs was extended to all shaders in DirectX 11.1 [146]. OpenGL 4.3 calls this a shader storage buﬀer object (SSBO). Both names are descriptive in their own way. Pixel shaders are run in parallel, in an arbitrary order, and this storage buﬀer is shared among them.
Often some mechanism is needed to avoid a data race condition (a.k.a. a data hazard), where both shader programs are “racing” to inﬂuence the same value, possibly

52

3. The Graphics Processing Unit

19

8

dv/dy = 19 – 17 = 2

17

6

dv/dx = 6 – 17 = –11

Figure 3.15. On the left, a triangle is rasterized into quads, sets of 2 × 2 pixels. The gradient computations for the pixel marked with a black dot is then shown on the right. The value for v is shown for each of the four pixel locations in the quad. Note how three of the pixels are not covered by the triangle, yet they are still processed by the GPU so that the gradients can be found. The gradients in the x and y screen directions are computed for the lower left pixel by using its two quad neighbors.

leading to arbitrary results. As an example, an error could occur if two invocations of a pixel shader tried to, say, add to the same retrieved value at about the same time. Both would retrieve the original value, both would modify it locally, but then whichever invocation wrote its result last would wipe out the contribution of the other invocation—only one addition would occur. GPUs avoid this problem by having dedicated atomic units that the shader can access [530]. However, atomics mean that some shaders may stall as they wait to access a memory location undergoing read/modify/write by another shader.
While atomics avoid data hazards, many algorithms require a speciﬁc order of execution. For example, you may want to draw a more distant transparent blue triangle before overlaying it with a red transparent triangle, blending the red atop the blue. It is possible for a pixel to have two pixel shader invocations for a pixel, one for each triangle, executing in such a way that the red triangle’s shader completes before the blue’s. In the standard pipeline, the fragment results are sorted in the merger stage before being processed. Rasterizer order views (ROVs) were introduced in DirectX 11.3 to enforce an order of execution. These are like UAVs; they can be read and written by shaders in the same fashion. The key diﬀerence is that ROVs guarantee that the data are accessed in the proper order. This increases the usefulness of these shader-accessible buﬀers considerably [327, 328]. For example, ROVs make it possible for the pixel shader to write its own blending methods, since it can directly access and write to any location in the ROV, and thus no merging stage is needed [176]. The price is that, if an out-of-order access is detected, a pixel shader invocation may stall until triangles drawn earlier are processed.

3.9. The Merging Stage

53

3.9 The Merging Stage
As discussed in Section 2.5.2, the merging stage is where the depths and colors of the individual fragments (generated in the pixel shader) are combined with the framebuﬀer. DirectX calls this stage the output merger; OpenGL refers to it as per-sample operations. On most traditional pipeline diagrams (including our own), this stage is where stencil-buﬀer and z-buﬀer operations occur. If the fragment is visible, another operation that takes place in this stage is color blending. For opaque surfaces there is no real blending involved, as the fragment’s color simply replaces the previously stored color. Actual blending of the fragment and stored color is commonly used for transparency and compositing operations (Section 5.5).
Imagine that a fragment generated by rasterization is run through the pixel shader and then is found to be hidden by some previously rendered fragment when the zbuﬀer is applied. All the processing done in the pixel shader was then unnecessary. To avoid this waste, many GPUs perform some merge testing before the pixel shader is executed [530]. The fragment’s z-depth (and whatever else is in use, such as the stencil buﬀer or scissoring) is used for testing visibility. The fragment is culled if hidden. This functionality is called early-z [1220, 1542]. The pixel shader has the ability to change the z-depth of the fragment or to discard the fragment entirely. If either type of operation is found to exist in a pixel shader program, early-z then generally cannot be used and is turned oﬀ, usually making the pipeline less eﬃcient. DirectX 11 and OpenGL 4.2 allow the pixel shader to force early-z testing to be on, though with a number of limitations [530]. See Section 23.7 for more about early-z and other z-buﬀer optimizations. Using early-z eﬀectively can have a large eﬀect on performance, which is discussed in detail in Section 18.4.5.
The merging stage occupies the middle ground between ﬁxed-function stages, such as triangle setup, and the fully programmable shader stages. Although it is not programmable, its operation is highly conﬁgurable. Color blending in particular can be set up to perform a large number of diﬀerent operations. The most common are combinations of multiplication, addition, and subtraction involving the color and alpha values, but other operations are possible, such as minimum and maximum, as well as bitwise logic operations. DirectX 10 added the capability to blend two colors from the pixel shader with the framebuﬀer color. This capability is called dual source-color blending and cannot be used in conjunction with multiple render targets. MRT does otherwise support blending, and DirectX 10.1 introduced the capability to perform diﬀerent blend operations on each separate buﬀer.
As mentioned at the end of the previous section, DirectX 11.3 provided a way to make blending programmable through ROVs, though at a price in performance. ROVs and the merging stage both guarantee draw order, a.k.a. output invariance. Regardless of the order in which pixel shader results are generated, it is an API requirement that results are sorted and sent to the merging stage in the order in which they are input, object by object and triangle by triangle.

54

3. The Graphics Processing Unit

3.10 The Compute Shader
The GPU can be used for more than implementing the traditional graphics pipeline. There are many non-graphical uses in ﬁelds as varied as computing the estimated value of stock options and training neural nets for deep learning. Using hardware in this way is called GPU computing. Platforms such as CUDA and OpenCL are used to control the GPU as a massive parallel processor, with no real need or access to graphics-speciﬁc functionality. These frameworks often use languages such as C or C++ with extensions, along with libraries made for the GPU.
Introduced in DirectX 11, the compute shader is a form of GPU computing, in that it is a shader that is not locked into a location in the graphics pipeline. It is closely tied to the process of rendering in that it is invoked by the graphics API. It is used alongside vertex, pixel, and other shaders. It draws upon the same pool of uniﬁed shader processors as those used in the pipeline. It is a shader like the others, in that it has some set of input data and can access buﬀers (such as textures) for input and output. Warps and threads are more visible in a compute shader. For example, each invocation gets a thread index that it can access. There is also the concept of a thread group, which consists of 1 to 1024 threads in DirectX 11. These thread groups are speciﬁed by x-, y-, and z-coordinates, mostly for simplicity of use in shader code. Each thread group has a small amount of memory that is shared among threads. In DirectX 11, this amounts to 32 kB. Compute shaders are executed by thread group, so that all threads in the group are guaranteed to run concurrently [1971].
One important advantage of compute shaders is that they can access data generated on the GPU. Sending data from the GPU to the CPU incurs a delay, so performance can be improved if processing and results can be kept resident on the GPU [1403]. Post-processing, where a rendered image is modiﬁed in some way, is a common use of compute shaders. The shared memory means that intermediate results from sampling image pixels can be shared with neighboring threads. Using a compute shader to determine the distribution or average luminance of an image, for example, has been found to run twice as fast as performing this operation on a pixel shader [530].
Compute shaders are also useful for particle systems, mesh processing such as facial animation [134], culling [1883, 1884], image ﬁltering [1102, 1710], improving depth precision [991], shadows [865], depth of ﬁeld [764], and any other tasks where a set of GPU processors can be brought to bear. Wihlidal [1884] discusses how compute shaders can be more eﬃcient than tessellation hull shaders. See Figure 3.16 for other uses.
This ends our review of the GPU’s implementation of the rendering pipeline. There are many ways in which the GPUs functions can be used and combined to perform various rendering-related processes. Relevant theory and algorithms tuned to take advantage of these capabilities are the central subjects of this book. Our focus now moves on to transforms and shading.

3.10. The Compute Shader

55

Figure 3.16. Compute shader examples. On the left, a compute shader is used to simulate hair aﬀected by wind, with the hair itself rendered using the tessellation stage. In the middle, a compute shader performs a rapid blur operation. On the right, ocean waves are simulated. (Images from NVIDIA SDK 11 [1301] samples, courtesy of NVIDIA Corporation.)
Further Reading and Resources
Giesen’s tour of the graphics pipeline [530] discusses many facets of the GPU at length, explaining why elements work the way they do. The course by Fatahalian and Bryant [462] discusses GPU parallelism in a series of detailed lecture slide sets. While focused on GPU computing using CUDA, the introductory part of Kirk and Hwa’s book [903] discusses the evolution and design philosophy for the GPU.
To learn the formal aspects of shader programming takes some work. Books such as the OpenGL Superbible [1606] and OpenGL Programming Guide [885] include material on shader programming. The older book OpenGL Shading Language [1512] does not cover more recent shader stages, such as the geometry and tessellation shaders, but does focus speciﬁcally on shader-related algorithms. See this book’s website, realtimerendering.com, for recent and recommended books.

Chapter 4 Transforms

“What if angry vectors veer Round your sleeping head, and form. There’s never need to fear Violence of the poor world’s abstract storm.” —Robert Penn Warren

A transform is an operation that takes entities such as points, vectors, or colors and converts them in some way. For the computer graphics practitioner, it is extremely important to master transforms. With them, you can position, reshape, and animate objects, lights, and cameras. You can also ensure that all computations are carried out in the same coordinate system, and project objects onto a plane in diﬀerent ways. These are only a few of the operations that can be performed with transforms, but they are suﬃcient to demonstrate the importance of the transform’s role in real-time graphics, or, for that matter, in any kind of computer graphics.
A linear transform is one that preserves vector addition and scalar multiplication. Speciﬁcally,

f (x) + f (y) = f (x + y), kf (x) = f (kx).

(4.1) (4.2)

As an example, f (x) = 5x is a transform that takes a vector and multiplies each element by ﬁve. To prove that this is linear, the two conditions (Equations 4.1 and 4.2) need to be fulﬁlled. The ﬁrst condition holds since any two vectors multiplied by ﬁve and then added will be the same as adding the vectors and then multiplying. The scalar multiplication condition (Equation 4.2) is clearly fulﬁlled. This function is called a scaling transform, as it changes the scale (size) of an object. The rotation transform is another linear transform that rotates a vector about the origin. Scaling and rotation transforms, in fact all linear transforms for three-element vectors, can be represented using a 3 × 3 matrix.
However, this size of matrix is usually not large enough. A function for a threeelement vector x such as f (x) = x + (7, 3, 2) is not linear. Performing this function on two separate vectors will add each value of (7, 3, 2) twice to form the result. Adding a ﬁxed vector to another vector performs a translation, e.g., it moves all locations by

57

58

4. Transforms

the same amount. This is a useful type of transform, and we would like to combine various transforms, e.g., scale an object to be half as large, then move it to a diﬀerent location. Keeping functions in the simple forms used so far makes it diﬃcult to easily combine them.
Combining linear transforms and translations can be done using an aﬃne transform, typically stored as a 4 × 4 matrix. An aﬃne transform is one that performs a linear transform and then a translation. To represent four-element vectors we use homogeneous notation, denoting points and directions in the same way (using bold lowercase letters). A direction vector is represented as v = (vx vy vz 0)T and a point as v = (vx vy vz 1)T . Throughout the chapter, we will make extensive use of the terminology and operations explained in the downloadable linear algebra appendix, found on realtimerendering.com.
All translation, rotation, scaling, reﬂection, and shearing matrices are aﬃne. The main characteristic of an aﬃne matrix is that it preserves the parallelism of lines, but not necessarily lengths and angles. An aﬃne transform may also be any sequence of concatenations of individual aﬃne transforms.
This chapter will begin with the most essential, basic aﬃne transforms. This section can be seen as a “reference manual” for simple transforms. More specialized matrices are then described, followed by a discussion and description of quaternions, a powerful transform tool. Then follows vertex blending and morphing, which are two simple but eﬀective ways of expressing animations of meshes. Finally, projection matrices are described. Most of these transforms, their notations, functions, and properties are summarized in Table 4.1, where an orthogonal matrix is one whose inverse is the transpose.
Transforms are a basic tool for manipulating geometry. Most graphics application programming interfaces let the user set arbitrary matrices, and sometimes a library may be used with matrix operations that implement many of the transforms discussed in this chapter. However, it is still worthwhile to understand the real matrices and their interaction behind the function calls. Knowing what the matrix does after such a function call is a start, but understanding the properties of the matrix itself will take you further. For example, such an understanding enables you to discern when you are dealing with an orthogonal matrix, whose inverse is its transpose, making for faster matrix inversions. Knowledge like this can lead to accelerated code.
4.1 Basic Transforms
This section describes the most basic transforms, such as translation, rotation, scaling, shearing, transform concatenation, the rigid-body transform, normal transform (which is not so normal), and computation of inverses. For the experienced reader, this can be used as a reference manual for simple transforms, and for the novice, it can serve as an introduction to the subject. This material is necessary background for the rest of this chapter and for other chapters in this book. We start with the simplest of transforms—the translation.

4.1. Basic Transforms

59

Notation T(t) Rx(ρ)
R S(s) Hij (s)
E(h, p, r)
Po(s) Pp(s) slerp(qˆ, ˆr, t)

Name translation matrix rotation matrix
rotation matrix scaling matrix shear matrix
Euler transform
orthographic projection perspective projection slerp transform

Characteristics
Moves a point. Aﬃne. Rotates ρ radians around the x-axis. Similar notation for the y- and z-axes. Orthogonal & aﬃne. Any rotation matrix. Orthogonal & aﬃne. Scales along all x-, y-, and z-axes according to s. Aﬃne. Shears component i by a factor s, with respect to component j. i, j ∈ {x, y, z}. Aﬃne. Orientation matrix given by the Euler angles head (yaw), pitch, roll. Orthogonal & aﬃne. Parallel projects onto some plane or to a volume. Aﬃne. Projects with perspective onto a plane or to a volume. Creates an interpolated quaternion with respect to the quaternions qˆ and ˆr, and the parameter t.

Table 4.1. Summary of most of the transforms discussed in this chapter.

4.1.1 Translation

A change from one location to another is represented by a translation matrix, T.

This matrix translates an entity by a vector t = (tx, ty, tz). T is given below by

Equation 4.3:

 1 0 0 tx 



T(t)

=

T(tx,

ty

,

tz )

=

 

0 0

1 0

0 1

ty tz

 . 

(4.3)

000 1

An example of the eﬀect of the translation transform is shown in Figure 4.1. It is
easily shown that the multiplication of a point p = (px, py, pz, 1) with T(t) yields a new point p′ = (px + tx, py + ty, pz + tz, 1), which is clearly a translation. Notice that a vector v = (vx, vy, vz, 0) is left unaﬀected by a multiplication by T, because a direction vector cannot be translated. In contrast, both points and vectors are aﬀected by the rest of the aﬃne transforms. The inverse of a translation matrix is T−1(t) = T(−t),
that is, the vector t is negated.

60

4. Transforms

Figure 4.1. The square on the left is transformed with a translation matrix T(5, 2, 0), whereby the square is moved 5 distance units to the right and 2 upward.

We should mention at this point that another valid notational scheme sometimes seen in computer graphics uses matrices with translation vectors in the bottom row. For example, DirectX uses this form. In this scheme, the order of matrices would be reversed, i.e., the order of application would read from left to right. Vectors and matrices in this notation are said to be in row-major form since the vectors are rows. In this book, we use column-major form. Whichever is used, this is purely a notational diﬀerence. When the matrix is stored in memory, the last four values of the sixteen are the three translation values followed by a one.

4.1.2 Rotation

A rotation transform rotates a vector (position or direction) by a given angle around a given axis passing through the origin. Like a translation matrix, it is a rigid-body transform, i.e., it preserves the distances between points transformed, and preserves handedness (i.e., it never causes left and right to swap sides). These two types of transforms are clearly useful in computer graphics for positioning and orienting objects. An orientation matrix is a rotation matrix associated with a camera view or object that deﬁnes its orientation in space, i.e., its directions for up and forward.
In two dimensions, the rotation matrix is simple to derive. Assume that we have a vector, v = (vx, vy), which we parameterize as v = (vx, vy) = (r cos θ, r sin θ). If we were to rotate that vector by φ radians (counterclockwise), then we would get u = (r cos(θ + φ), r sin(θ + φ)). This can be rewritten as

u=

r cos(θ + φ) r sin(θ + φ)

=

r(cos θ cos φ − sin θ sin φ) r(sin θ cos φ + cos θ sin φ)

=

cos φ − sin φ sin φ cos φ

r cos θ r sin θ

= R(φ)v,

(4.4)

R(φ)

v

where we used the angle sum relation to expand cos(θ + φ) and sin(θ + φ). In three dimensions, commonly used rotation matrices are Rx(φ), Ry(φ), and Rz(φ), which

4.1. Basic Transforms

61

rotate an entity φ radians around the x-, y-, and z-axes, respectively. They are given

by Equations 4.5–4.7:

1 0

0 0

Rx(φ)

=

 


0 0

cos φ sin φ

− sin φ cos φ

0 0

 

,



(4.5)

00

01

 cos φ 0 sin φ 0 



Ry (φ)

=

 

0 − sin φ

1 0

0 cos φ

0 0

 

,



0 001

(4.6)

 cos φ − sin φ 0 0 

Rz (φ)

=

 


sin φ 0

cos φ 0

0 1

0 0

 

.



0

0 01

(4.7)

If the bottom row and rightmost column are deleted from a 4 × 4 matrix, a 3 × 3 matrix is obtained. For every 3 × 3 rotation matrix, R, that rotates φ radians around any axis, the trace (which is the sum of the diagonal elements in a matrix) is constant independent of the axis, and is computed as [997]:

tr(R) = 1 + 2 cos φ.

(4.8)

The eﬀect of a rotation matrix may be seen in Figure 4.4 on page 65. What characterizes a rotation matrix, Ri(φ), besides the fact that it rotates φ radians around axis i, is that it leaves all points on the rotation axis, i, unchanged. Note that R will also be used to denote a rotation matrix around any axis. The axis rotation matrices given above can be used in a series of three transforms to perform any arbitrary axis rotation. This procedure is discussed in Section 4.2.1. Performing a rotation around an arbitrary axis directly is covered in Section 4.2.4.
All rotation matrices have a determinant of one and are orthogonal. This also holds for concatenations of any number of these transforms. There is another way to obtain the inverse: R−i 1(φ) = Ri(−φ), i.e., rotate in the opposite direction around the same axis.

Example: Rotation Around a Point. Assume that we want to rotate an object by φ radians around the z-axis, with the center of rotation being a certain point, p. What is the transform? This scenario is depicted in Figure 4.2. Since a rotation around a point is characterized by the fact that the point itself is unaﬀected by the rotation, the transform starts by translating the object so that p coincides with the origin, which is done with T(−p). Thereafter follows the actual rotation: Rz(φ). Finally, the object has to be translated back to its original position using T(p). The resulting transform, X, is then given by

X = T(p)Rz(φ)T(−p).

(4.9)

Note the order of the matrices above.

62

4. Transforms

Figure 4.2. Example of rotation around a speciﬁc point p.

4.1.3 Scaling

A scaling matrix, S(s) = S(sx, sy, sz), scales an entity with factors sx, sy, and sz along the x-, y-, and z-directions, respectively. This means that a scaling matrix can be used to enlarge or diminish an object. The larger the si, i ∈ {x, y, z}, the larger the scaled entity gets in that direction. Setting any of the components of s to 1 naturally
avoids a change in scaling in that direction. Equation 4.10 shows S:

 sx 0 0 0 

S(s)

=

 



0 0

sy 0

0 sz

0 0

 

.



0 0 01

(4.10)

Figure 4.4 on page 65 illustrates the eﬀect of a scaling matrix. The scaling operation is called uniform if sx = sy = sz and nonuniform otherwise. Sometimes the terms isotropic and anisotropic scaling are used instead of uniform and nonuniform. The inverse is S−1(s) = S(1/sx, 1/sy, 1/sz).
Using homogeneous coordinates, another valid way to create a uniform scaling matrix is by manipulating matrix element at position (3, 3), i.e., the element at the lower right corner. This value aﬀects the w-component of the homogeneous coordinate, and so scales every coordinate of a point (not direction vectors) transformed by the matrix. For example, to scale uniformly by a factor of 5, the elements at (0, 0), (1, 1), and (2, 2) in the scaling matrix can be set to 5, or the element at (3, 3) can be set to 1/5. The two diﬀerent matrices for performing this are shown below:

5 0 0 0

S

=

0 0

5 0

0 5

0 0

,

0001

1 0 0 0 

S′

=

0 0

1 0

0 1

0 0

 . 

0 0 0 1/5

(4.11)

In contrast to using S for uniform scaling, using S′ must always be followed by homogenization. This may be ineﬃcient, since it involves divides in the homogenization

4.1. Basic Transforms

63

process; if the element at the lower right (position (3, 3)) is 1, no divides are necessary. Of course, if the system always does this division without testing for 1, then there is no extra cost.
A negative value on one or three of the components of s gives a type of reﬂection matrix, also called a mirror matrix. If only two scale factors are −1, then we will rotate π radians. It should be noted that a rotation matrix concatenated with a reﬂection matrix is also a reﬂection matrix. Hence, the following is a reﬂection matrix:

cos(π/2) sin(π/2) − sin(π/2) cos(π/2)

1 0

0 −1

=

0 −1

−1 0

.

(4.12)

rotation

reﬂection

Reﬂection matrices usually require special treatment when detected. For example, a triangle with vertices in a counterclockwise order will get a clockwise order when transformed by a reﬂection matrix. This order change can cause incorrect lighting and backface culling to occur. To detect whether a given matrix reﬂects in some manner, compute the determinant of the upper left 3 × 3 elements of the matrix. If the value is negative, the matrix is reﬂective. For example, the determinant of the matrix in Equation 4.12 is 0 · 0 − (−1) · (−1) = −1.
Example: Scaling in a Certain Direction. The scaling matrix S scales along only the x-, y-, and z-axes. If scaling should be performed in other directions, a compound transform is needed. Assume that scaling should be done along the axes of the orthonormal, right-oriented vectors f x, f y, and f z. First, construct the matrix F, to change the basis, as below:

F=

fx 0

fy 0

fz 0

0 1

.

(4.13)

The idea is to make the coordinate system given by the three axes coincide with the

standard axes, then use the standard scaling matrix, and then transform back. The

ﬁrst step is carried out by multiplying with the transpose, i.e., the inverse, of F. Then

the actual scaling is done, followed by a transform back. The transform is shown in

Equation 4.14:

X = FS(s)FT .

(4.14)

4.1.4 Shearing
Another class of transforms is the set of shearing matrices. These can, for example, be used in games to distort an entire scene to create a psychedelic eﬀect or otherwise warp a model’s appearance. There are six basic shearing matrices, and they are denoted Hxy(s), Hxz(s), Hyx(s), Hyz(s), Hzx(s), and Hzy(s). The ﬁrst subscript is used to denote which coordinate is being changed by the shear matrix, while the second

64

4. Transforms

Figure 4.3. The eﬀect of shearing the unit square with Hxz(s). Both the y- and z-values are unaﬀected by the transform, while the x-value is the sum of the old x-value and s multiplied by the z-value, causing the square to become slanted. This transform is area-preserving, which can be seen in that the dashed areas are the same.

subscript indicates the coordinate which does the shearing. An example of a shear matrix, Hxz(s), is shown in Equation 4.15. Observe that the subscript can be used to ﬁnd the position of the parameter s in the matrix below; the x (whose numeric index is 0) identiﬁes row zero, and the z (whose numeric index is 2) identiﬁes column two, and so the s is located there:

1 0 s 0

Hxz (s)

=

 


0 0

1 0

0 1

0 0

 

.



0001

(4.15)

The eﬀect of multiplying this matrix with a point p yields a point: (px+spz py pz)T . Graphically, this is shown for the unit square in Figure 4.3. The inverse of Hij(s) (shearing the ith coordinate with respect to the jth coordinate, where i = j), is generated by shearing in the opposite direction, that is, H−ij1(s) = Hij(−s).
You can also use a slightly diﬀerent kind of shear matrix:

1 0 s 0

H′xy (s,

t)

=

 


0 0

1 0

t 1

0 0

 

.



0001

(4.16)

Here, however, both subscripts are used to denote that these coordinates are to be
sheared by the third coordinate. The connection between these two diﬀerent kinds of descriptions is H′ij(s, t) = Hik(s)Hjk(t), where k is used as an index to the third coordinate. The right matrix to use is a matter of taste. Finally, it should be noted
that since the determinant of any shear matrix |H| = 1, this is a volume-preserving
transformation, which also is illustrated in Figure 4.3.

4.1. Basic Transforms

65

Figure 4.4. This illustrates the order dependency when multiplying matrices. In the top row, the rotation matrix Rz(π/6) is applied followed by a scaling, S(s), where s = (2, 0.5, 1). The composite matrix is then S(s)Rz(π/6). In the bottom row, the matrices are applied in the reverse order, yielding Rz(π/6)S(s). The results are clearly diﬀerent. It generally holds that MN = NM, for arbitrary matrices M and N.
4.1.5 Concatenation of Transforms
Due to the noncommutativity of the multiplication operation on matrices, the order in which the matrices occur matters. Concatenation of transforms is therefore said to be order-dependent.
As an example of order dependency, consider two matrices, S and R. S(2, 0.5, 1) scales the x-component by a factor two and the y-component by a factor 0.5. Rz(π/6) rotates π/6 radians counterclockwise around the z-axis (which points outward from page of this book in a right-handed coordinate system). These matrices can be multiplied in two ways, with the results being entirely diﬀerent. The two cases are shown in Figure 4.4.
The obvious reason to concatenate a sequence of matrices into a single one is to gain eﬃciency. For example, imagine that you have a game scene that has several million vertices, and that all objects in the scene must be scaled, rotated, and ﬁnally translated. Now, instead of multiplying all vertices with each of the three matrices, the three matrices are concatenated into a single matrix. This single matrix is then applied to the vertices. This composite matrix is C = TRS. Note the order here. The scaling matrix, S, should be applied to the vertices ﬁrst, and therefore appears to the right in the composition. This ordering implies that TRSp = (T(R(Sp))), where p is a point to be transformed. Incidentally, TRS is the order commonly used by scene graph systems.

66

4. Transforms

It is worth noting that while matrix concatenation is order-dependent, the matrices can be grouped as desired. For example, say that with TRSp you would like to compute the rigid-body motion transform TR once. It is valid to group these two matrices together, (TR)(Sp), and replace with the intermediate result. Thus, matrix concatenation is associative.

4.1.6 The Rigid-Body Transform

When a person grabs a solid object, say a pen from a table, and moves it to another

location, perhaps to a shirt pocket, only the object’s orientation and location change,

while the shape of the object generally is not aﬀected. Such a transform, consisting

of concatenations of only translations and rotations, is called a rigid-body transform.

It has the characteristic of preserving lengths, angles, and handedness.

Any rigid-body matrix, X, can be written as the concatenation of a translation

matrix, T(t), and a rotation matrix, R. Thus, X has the appearance of the matrix in

Equation 4.17:

 r00 r01 r02 tx 

X

=

T(t)R

=

 



r10 r20

r11 r21

r12 r22

ty tz

 

.



(4.17)

0 0 01

The inverse of X is computed as X−1 = (T(t)R)−1 = R−1T(t)−1 = RT T(−t). Thus, to compute the inverse, the upper left 3 × 3 matrix of R is transposed, and the translation values of T change sign. These two new matrices are multiplied together in opposite order to obtain the inverse. Another way to compute the inverse of X is to consider R (making R appear as 3 × 3 matrix) and X in the following notation (notation described on page 6 with Equation 1.2):

rT0,

 R¯ = r,0 r,1 r,2 = rT1, ,


rT2,

=⇒

X=

R¯ 0T

t 1

,

(4.18)

where r,0 means the ﬁrst column of the rotation matrix (i.e., the comma indicates any value from 0 to 2, while the second subscript is 0) and rT0, is the ﬁrst row of the column matrix. Note that 0 is a 3 × 1 column vector ﬁlled with zeros. Some calculations yield
the inverse in the expression shown in Equation 4.19:

X−1 =

r0, 0

r1, 0

r2, 0

−R¯ T t 1

.

(4.19)

4.1. Basic Transforms

67

‘

u u
c v
r

l
Figure 4.5. The geometry involved in computing a transform that orients the camera at c, with up vector u′, to look at the point l. For that purpose, we need to compute r, u, and v.

Example: Orienting the Camera. A common task in graphics is to orient the camera so that it looks at a certain position. Here we will present what gluLookAt() (from the OpenGL Utility Library, GLU for short) does. Even though this function call itself is not used much nowadays, the task remains common. Assume that the camera is located at c, that we want the camera to look at a target l, and that a given up direction of the camera is u′, as illustrated in Figure 4.5. We want to compute a basis consisting of three vectors, {r, u, v}. We start by computing the view vector as v = (c − l)/||c − l||, i.e., the normalized vector from the target to the camera position. A vector looking to the “right” can then be computed as r = −(v × u′)/||v × u′||. The u′ vector is often not guaranteed to be pointing precisely up, so the ﬁnal up vector is another cross product, u = v × r, which is guaranteed to be normalized since both v and r are normalized and perpendicular by construction. In the camera transform matrix, M, that we will construct, the idea is to ﬁrst translate everything so the camera position is at the origin, (0, 0, 0), and then change the basis so that r is aligned with (1, 0, 0), u with (0, 1, 0), and v with (0, 0, 1). This is done by

rx ry rz 0 1 0 0 −tx rx ry rz −t · r

M

=

ux vx

uy vy

uz vz

0 0 0 0

1 0

0 1

−ty −tz

 


=

ux vx

uy vy

uz vz

−t −t

· ·

u 
v

.

0 0 0 1 000 1

000 1

(4.20)

change of basis

translation

Note that when concatenating the translation matrix with the change of basis matrix, the translation −t is to the right since it should be applied ﬁrst. One way to remember where to put the components of r, u, and v is the following. We want r to become (1, 0, 0), so when multiplying a change of basis matrix with (1, 0, 0), we can see that the ﬁrst row in the matrix must be the elements of r, since r · r = 1. Furthermore, the second row and the third row must consist of vectors that are perpendicular to r, i.e., r · x = 0. When applying the same thinking also to u and v, we arrive at the change of basis matrix above.

68
y triangle

4. Transforms

x x
Figure 4.6. On the left is the original geometry, a triangle and its normal shown from the side. The middle illustration shows what happens if the model is scaled along the x- axis by 0.5 and the normal uses the same matrix. The right ﬁgure shows the proper transform of the normal.
4.1.7 Normal Transform
A single matrix can be used to consistently transform points, lines, triangles, and other geometry. The same matrix can also transform tangent vectors following along these lines or on the surfaces of triangles. However, this matrix cannot always be used to transform one important geometric property, the surface normal (and the vertex lighting normal). Figure 4.6 shows what can happen if this same matrix is used.
Instead of multiplying by the matrix itself, the proper method is to use the transpose of the matrix’s adjoint [227]. Computation of the adjoint is described in our online linear algebra appendix. The adjoint is always guaranteed to exist. The normal is not guaranteed to be of unit length after being transformed, so typically needs to be normalized.
The traditional answer for transforming the normal is that the transpose of the inverse is computed [1794]. This method normally works. The full inverse is not necessary, however, and occasionally cannot be created. The inverse is the adjoint divided by the original matrix’s determinant. If this determinant is zero, the matrix is singular, and the inverse does not exist.
Even computing just the adjoint for a full 4 × 4 matrix can be expensive, and is usually not necessary. Since the normal is a vector, translation will not aﬀect it. Furthermore, most modeling transforms are aﬃne. They do not change the w-component of the homogeneous coordinate passed in, i.e., they do not perform projection. Under these (common) circumstances, all that is needed for normal transformation is to compute the adjoint of the upper left 3 × 3 components.
Often even this adjoint computation is not needed. Say we know the transform matrix is composed entirely of a concatenation of translations, rotations, and uniform scaling operations (no stretching or squashing). Translations do not aﬀect the normal. The uniform scaling factors simply change the length of the normal. What is left is a series of rotations, which always yields a net rotation of some sort, nothing more.

4.1. Basic Transforms

69

The transpose of the inverse can be used to transform normals. A rotation matrix is deﬁned by the fact that its transpose is its inverse. Substituting to get the normal transform, two transposes (or two inverses) give the original rotation matrix. Putting it all together, the original transform itself can also be used directly to transform normals under these circumstances.
Finally, fully renormalizing the normal produced is not always necessary. If only translations and rotations are concatenated together, the normal will not change length when transformed by the matrix, so no renormalizing is needed. If uniform scalings are also concatenated, the overall scale factor (if known, or extracted—Section 4.2.3) can be used to directly normalize the normals produced. For example, if we know that a series of scalings were applied that makes the object 5.2 times larger, then normals transformed directly by this matrix are renormalized by dividing them by 5.2. Alternately, to create a normal transform matrix that would produce normalized results, the original matrix’s 3 × 3 upper left could be divided by this scale factor once.
Note that normal transforms are not an issue in systems where, after transformation, the surface normal is derived from the triangle (e.g., using the cross product of the triangle’s edges). Tangent vectors are diﬀerent than normals in nature, and are always directly transformed by the original matrix.
4.1.8 Computation of Inverses
Inverses are needed in many cases, for example, when changing back and forth between coordinate systems. Depending on the available information about a transform, one of the following three methods of computing the inverse of a matrix can be used:
• If the matrix is a single transform or a sequence of simple transforms with given parameters, then the matrix can be computed easily by “inverting the parameters” and the matrix order. For example, if M = T(t)R(φ), then M−1 = R(−φ)T(−t). This is simple and preserves the accuracy of the transform, which is important when rendering huge worlds [1381].
• If the matrix is known to be orthogonal, then M−1 = MT , i.e., the transpose is the inverse. Any sequence of rotations is a rotation, and so is orthogonal.
• If nothing is known, then the adjoint method, Cramer’s rule, LU decomposition, or Gaussian elimination could be used to compute the inverse. Cramer’s rule and the adjoint method are generally preferable, as they have fewer branch operations; “if” tests are good to avoid on modern architectures. See Section 4.1.7 on how to use the adjoint to invert transform normals.
The purpose of the inverse computation can also be taken into account when optimizing. For example, if the inverse is to be used for transforming vectors, then only the 3 × 3 upper left part of the matrix normally needs to be inverted (see the previous section).

70

4. Transforms

4.2 Special Matrix Transforms and Operations
In this section, several matrix transforms and operations that are essential to real-time graphics will be introduced and derived. First, we present the Euler transform (along with its extraction of parameters), which is an intuitive way to describe orientations. Then we touch upon retrieving a set of basic transforms from a single matrix. Finally, a method is derived that rotates an entity around an arbitrary axis.

4.2.1 The Euler Transform

This transform is an intuitive way to construct a matrix to orient yourself (i.e., the

camera) or any other entity in a certain direction. Its name comes from the great

Swiss mathematician Leonhard Euler (1707–1783).

First, some kind of default view direction must be established. Most often it

lies along the negative z-axis with the head oriented along the y-axis, as depicted in

Figure 4.7. The Euler transform is the multiplication of three matrices, namely the

rotations shown in the ﬁgure. More formally, the transform, denoted E, is given by

Equation 4.21:

E(h, p, r) = Rz(r)Rx(p)Ry(h).

(4.21)

The order of the matrices can be chosen in 24 diﬀerent ways [1636]; we present this one because it is commonly used. Since E is a concatenation of rotations, it is also clearly orthogonal. Therefore its inverse can be expressed as E−1 = ET = (RzRxRy)T = RTy RTx RTz , although it is, of course, easier to use the transpose of E directly.
The Euler angles h, p, and r represent in which order and how much the head, pitch, and roll should rotate around their respective axes. Sometimes the angles are all called “rolls,” e.g., our “head” is the “y-roll” and our “pitch” is the “x-roll.” Also, “head” is sometimes known as “yaw,” such as in ﬂight simulation.
This transform is intuitive and therefore easy to discuss in layperson’s language. For example, changing the head angle makes the viewer shake their head “no,” changing the pitch makes them nod, and rolling makes them tilt their head sideways. Rather than talking about rotations around the x-, y-, and z-axes, we talk about altering the head, pitch, and roll. Note that this transform can orient not only the camera, but also any object or entity as well. These transforms can be performed using the global axes of the world space or relative to a local frame of reference.
It is important to note that some presentations of Euler angles give the z-axis as the initial up direction. This diﬀerence is purely a notational change, though a potentially confusing one. In computer graphics there is a division in how the world is regarded and thus how content is formed: y-up or z-up. Most manufacturing processes, including 3D printing, consider the z-direction to be up in world space; aviation and sea vehicles consider −z to be up. Architecture and GIS normally use z-up, as a building plan or map is two-dimensional, x and y. Media-related modeling systems often consider the y-direction as up in world coordinates, matching how we always describe a camera’s screen up direction in computer graphics. The diﬀerence between

4.2. Special Matrix Transforms and Operations

71

Figure 4.7. The Euler transform and how it relates to the way you change the head, pitch, and roll angles. The default view direction is shown, looking along the negative z-axis with the up direction along the y-axis.
these two world up vector choices is just a 90◦ rotation (and possibly a reﬂection) away, but not knowing which is assumed can lead to problems. In this volume we use a world direction of y-up unless otherwise noted.
We also want to point out that the camera’s up direction in its view space has nothing in particular to do with the world’s up direction. Roll your head and the view is tilted, with its world-space up direction diﬀering from the world’s. As another example, say the world uses y-up and our camera looks straight down at the terrain below, a bird’s eye view. This orientation means the camera has pitched 90◦ forward, so that its up direction in world space is (0, 0, −1). In this orientation the camera has no y-component and instead considers −z to be up in world space, but “y is up” remains true in view space, by deﬁnition.
While useful for small angle changes or viewer orientation, Euler angles have some other serious limitations. It is diﬃcult to work with two sets of Euler angles in combi-

72

4. Transforms

nation. For example, interpolation between one set and another is not a simple matter of interpolating each angle. In fact, two diﬀerent sets of Euler angles can give the same orientation, so any interpolation should not rotate the object at all. These are some of the reasons that using alternate orientation representations such as quaternions, discussed later in this chapter, are worth pursuing. With Euler angles, you can also get something called gimbal lock, which will be explained next in Section 4.2.2.

4.2.2 Extracting Parameters from the Euler Transform

In some situations, it is useful to have a procedure that extracts the Euler parameters, h, p, and r, from an orthogonal matrix. This procedure is shown in Equation 4.22:

 e00 e01 e02  E(h, p, r) =  e10 e11 e12  = Rz(r)Rx(p)Ry(h).
e20 e21 e22

(4.22)

Here we abandoned the 4 × 4 matrices for 3 × 3 matrices, since the latter provide all the necessary information for a rotation matrix. That is, the rest of the equivalent 4 × 4 matrix always contains zeros and a one in the lower right position.
Concatenating the three rotation matrices in Equation 4.22 yields

E=

cos r cos h−sin r sin p sin h −sin r cos p cos r sin h+sin r sin p cos h

sin r cos h+cos r sin p sin h cos r cos p sin r sin h−cos r sin p cos h

−cos p sin h

sin p

cos p cos h

. (4.23)

From this it is apparent that the pitch parameter is given by sin p = e21. Also, dividing e01 by e11, and similarly dividing e20 by e22, gives rise to the following extraction equations for the head and roll parameters:

e01 e11

=

− sin r cos r

=

− tan r

and

e20 e22

=

− sin h cos h

=

− tan h.

(4.24)

Thus, the Euler parameters h (head), p (pitch), and r (roll) are extracted from a matrix E using the function atan2(y,x) (see page 8 in Chapter 1) as in Equation 4.25:

h = atan2(−e20, e22), p = arcsin(e21), r = atan2(−e01, e11).

(4.25)

However, there is a special case we need to handle. If cos p = 0, we have gimbal lock

(Section 4.2.2) and rotation angles r and h will rotate around the same axis (though

possibly in diﬀerent directions, depending on whether the p rotation angle was −π/2

or π/2), so only one angle needs to be derived. If we arbitrarily set h = 0 [1769],

we get

cos r sin r cos p sin r sin p 

E = sin r cos r cos p − cos r sin p .

(4.26)

0

sin p

cos p

4.2. Special Matrix Transforms and Operations

73

Since p does not aﬀect the values in the ﬁrst column, when cos p = 0 we can use sin r/ cos r = tan r = e10/e00, which gives r = atan2(e10, e00).
Note that from the deﬁnition of arcsin, −π/2 ≤ p ≤ π/2, which means that if E was created with a value of p outside this interval, the original parameter cannot be extracted. That h, p, and r are not unique means that more than one set of the Euler parameters can be used to yield the same transform. More about Euler angle conversion can be found in Shoemake’s 1994 article [1636]. The simple method outlined above can result in problems with numerical instability, which is avoidable at some cost in speed [1362].
When you use Euler transforms, something called gimbal lock may occur [499, 1633]. This happens when rotations are made so that one degree of freedom is lost. For example, say the order of transforms is x/y/z. Consider a rotation of π/2 around just the y-axis, the second rotation performed. Doing so rotates the local z-axis to be aligned with the original x-axis, so that the ﬁnal rotation around z is redundant.
Mathematically, we have already seen gimbal lock in Equation 4.26, where we assumed cos p = 0, i.e., p = ±π/2 + 2πk, where k is an integer. With such a value of p, we have lost one degree of freedom since the matrix only depends on one angle, r + h or r − h (but not both at the same time).
While Euler angles are commonly presented as being in x/y/z order in modeling systems, a rotation around each local axis, other orderings are feasible. For example, z/x/y is used in animation and z/x/z in both animation and physics. All are valid ways of specifying three separate rotations. This last ordering, z/x/z, can be superior for some applications, as only when rotating π radians around x (a half-rotation) does gimbal lock occur. There is no perfect sequence that avoids gimbal lock. Euler angles nonetheless are commonly used, as animators prefer curve editors to specify how angles change over time [499].
Example: Constraining a Transform. Imagine you are holding a (virtual) wrench that is gripping a bolt. To get the bolt into place, you have to rotate the wrench around the x-axis. Now assume that your input device (mouse, VR gloves, space-ball, etc.) gives you a rotation matrix, i.e., a rotation, for the movement of the wrench. The problem is that it is likely to be wrong to apply this transform to the wrench, which should rotate around only the x-axis. To restrict the input transform, called P, to be a rotation around the x-axis, simply extract the Euler angles, h, p, and r, using the method described in this section, and then create a new matrix Rx(p). This is then the sought-after transform that will rotate the wrench around the x-axis (if P now contains such a movement).

4.2.3 Matrix Decomposition
Up to this point we have been working under the assumption that we know the origin and history of the transformation matrix we are using. This is often not the case.

74

4. Transforms

For example, nothing more than a concatenated matrix may be associated with some transformed object. The task of retrieving various transforms from a concatenated matrix is called matrix decomposition.
There are many reasons to retrieve a set of transformations. Uses include:
• Extracting just the scaling factors for an object.
• Finding transforms needed by a particular system. (For example, some systems may not allow the use of an arbitrary 4 × 4 matrix.)
• Determining whether a model has undergone only rigid-body transforms.
• Interpolating between keyframes in an animation where only the matrix for the object is available.
• Removing shears from a rotation matrix.
We have already presented two decompositions, those of deriving the translation and rotation matrix for a rigid-body transformation (Section 4.1.6) and deriving the Euler angles from an orthogonal matrix (Section 4.2.2).
As we have seen, it is trivial to retrieve the translation matrix, as we simply need the elements in the last column of the 4 × 4 matrix. We can also determine if a reﬂection has occurred by checking whether the determinant of the matrix is negative. To separate out the rotation, scaling, and shears takes more determined eﬀort.
Fortunately, there are several articles on this topic, as well as code available online. Thomas [1769] and Goldman [552, 553] each present somewhat diﬀerent methods for various classes of transformations. Shoemake [1635] improves upon their techniques for aﬃne matrices, as his algorithm is independent of frame of reference and attempts to decompose the matrix to obtain rigid-body transforms.
4.2.4 Rotation about an Arbitrary Axis
Sometimes it is convenient to have a procedure that rotates an entity by some angle around an arbitrary axis. Assume that the rotation axis, r, is normalized and that a transform should be created that rotates α radians around r.
To do this, we ﬁrst transform to a space where the axis around which we want to rotate is the x-axis. This is done with a rotation matrix, called M. Then the actual rotation is performed, and we transform back using M−1 [314]. This procedure is illustrated in Figure 4.8.
To compute M, we need to ﬁnd two axes that are orthonormal both to r and to each other. We concentrate on ﬁnding the second axis, s, knowing that the third axis, t, will be the cross product of the ﬁrst and the second axis, t = r × s. A numerically stable way to do this is to ﬁnd the smallest component (in absolute value) of r, and set it to 0. Swap the two remaining components, and then negate the ﬁrst of them

4.2. Special Matrix Transforms and Operations

75

Figure 4.8. Rotation about an arbitrary axis, r, is accomplished by ﬁnding an orthonormal basis formed by r, s, and t. We then align this basis with the standard basis so that r is aligned with the x-axis. The rotation around the x-axis is performed there, and ﬁnally we transform back.

(in fact, either of the nonzero components could be negated). Mathematically, this is expressed as [784]:

 

(0, −rz, ry),

if

|rx| ≤ |ry|

and

|rx| ≤ |rz|,

¯s = (−rz, 0, rx), if |ry| ≤ |rx| and |ry| ≤ |rz|,

 (−ry, rx, 0), if |rz| ≤ |rx| and |rz| ≤ |ry|,

s = ¯s/||¯s||,

(4.27)

t = r × s.

This guarantees that ¯s is orthogonal (perpendicular) to r, and that (r, s, t) is an

orthonormal basis. Frisvad [496] presents a method without any branches in the code,

which is faster but has lower accuracy. Max [1147] and Duﬀ et al. [388] improve the

accuracy of Frisvad’s method. Whichever technique is employed, these three vectors

are used to create a rotation matrix:

 rT 

M =  sT  .

(4.28)

tT

This matrix transforms the vector r into the x-axis, s into the y-axis, and t into the

z-axis. So, the ﬁnal transform for rotating α radians around the normalized vector r

is then

X = MT Rx(α)M.

(4.29)

In words, this means that ﬁrst we transform so that r is the x-axis (using M), then we rotate α radians around this x-axis (using Rx(α)), and then we transform back using the inverse of M, which in this case is MT because M is orthogonal.

Another method for rotating around an arbitrary, normalized axis r by φ radians has been presented by Goldman [550]. Here, we simply present his transform:

R=
 cos φ + (1−cos φ)rx2 (1−cos φ)rxry +rz sin φ
(1−cos φ)rxrz −ry sin φ

(1−cos φ)rxry −rz sin φ cos φ + (1−cos φ)ry2
(1−cos φ)ryrz +rx sin φ

(1−cos φ)rxrz +ry sin φ
(1−cos φ)ryrz −rx sin φ . cos φ + (1−cos φ)rz2

(4.30)

76

4. Transforms

In Section 4.3.2, we present yet another method for solving this problem, using quaternions. Also in that section are more eﬃcient algorithms for related problems, such as rotation from one vector to another.

4.3 Quaternions
Although quaternions were invented back in 1843 by Sir William Rowan Hamilton as an extension to the complex numbers, it was not until 1985 that Shoemake [1633] introduced them to the ﬁeld of computer graphics.1 Quaternions are used to represent rotations and orientations. They are superior to both Euler angles and matrices in several ways. Any three-dimensional orientation can be expressed as a single rotation around a particular axis. Given this axis & angle representation, translating to or from a quaternion is straightforward, while Euler angle conversion in either direction is challenging. Quaternions can be used for stable and constant interpolation of orientations, something that cannot be done well with Euler angles.
A complex number has a real and an imaginary part. Ea√ch is represented by two real numbers, the second real number being multiplied by −1. Similarly, quaternions have four parts. The ﬁrst three values are closely related to axis of rotation, with the angle of rotation aﬀecting all four parts (more about this in Section 4.3.2). Each quaternion is represented by four real numbers, each associated with a diﬀerent part. Since quaternions have four components, we choose to represent them as vectors, but to diﬀerentiate them, we put a hat on them: qˆ. We begin with some mathematical background on quaternions, which is then used to construct a variety of useful transforms.

4.3.1 Mathematical Background
We start with the deﬁnition of a quaternion.

Deﬁnition. A quaternion qˆ can be deﬁned in the following ways, all equivalent.

qˆ = (qv, qw) = iqx + jqy + kqz + qw = qv + qw, qv = iqx + jqy + kqz = (qx, qy, qz), i2 = j2 = k2 = −1, jk = −kj = i, ki = −ik = j, ij = −ji = k.

(4.31)

The variable qw is called the real part of a quaternion, qˆ. The imaginary part is qv, and i, j, and k are called imaginary units.

For the imaginary part, qv, we can use all the normal vector operations, such as addition, scaling, dot product, cross product, and more. Using the deﬁnition of the quaternion, the multiplication operation between two quaternions, qˆ and ˆr, is derived
1In fairness, Robinson [1502] used quaternions in 1958 for rigid-body simulations.

4.3. Quaternions

77

as shown below. Note that the multiplication of the imaginary units is noncommutative.

Multiplication: qˆˆr = (iqx + jqy + kqz + qw)(irx + jry + krz + rw) = i(qyrz − qzry + rwqx + qwrx) + j(qzrx − qxrz + rwqy + qwry) + k(qxry − qyrx + rwqz + qwrz) + qwrw − qxrx − qyry − qzrz = (qv × rv + rwqv + qwrv, qwrw − qv · rv).

(4.32)

As can be seen in this equation, we use both the cross product and the dot product to compute the multiplication of two quaternions.
Along with the deﬁnition of the quaternion, the deﬁnitions of addition, conjugate, norm, and an identity are needed:

Addition:

qˆ + ˆr = (qv, qw) + (rv, rw) = (qv + rv, qw + rw).

Conjugate:

qˆ∗ = (qv, qw)∗ = (−qv, qw).

Norm:

n(qˆ) = =

qˆqˆ∗ = qˆ∗qˆ = qv · qv + qw2 qx2 + qy2 + qz2 + qw2 .

(4.33)

Identity:

ˆi = (0, 1).

√ When n(qˆ) = qˆqˆ∗ is simpliﬁed (result shown above), the imaginary parts cancel

out and only a real part remains. The norm is sometimes denoted ||qˆ|| = n(qˆ) [1105]. A consequence of the above is that a multiplicative inverse, denoted by qˆ−1, can be derived. The equation qˆ−1qˆ = qˆqˆ−1 = 1 must hold for the inverse (as is common for

a multiplicative inverse). We derive a formula from the deﬁnition of the norm:

n(qˆ)2

=

qˆqˆ∗

⇐⇒

qˆqˆ∗ n(qˆ)2

=

1.

(4.34)

This gives the multiplicative inverse as shown below:

Inverse:

qˆ−1

=

1 n(qˆ)2

qˆ∗

.

(4.35)

The formula for the inverse uses scalar multiplication, which is an operation derived
from the multiplication seen in Equation 4.3.1: sqˆ = (0, s)(qv, qw) = (sqv, sqw), and qˆs = (qv, qw)(0, s) = (sqv, sqw), which means that scalar multiplication is commutative: sqˆ = qˆs = (sqv, sqw).

78

4. Transforms

The following collection of rules are simple to derive from the deﬁnitions:

Conjugate rules:

(qˆ∗)∗ = qˆ, (qˆ + ˆr)∗ = qˆ∗ + ˆr∗,
(qˆˆr)∗ = ˆr∗qˆ∗.

(4.36)

Norm rules:
Laws of Multiplication: Linearity:

n(qˆ∗) = n(qˆ), n(qˆˆr) = n(qˆ)n(ˆr).
pˆ(sqˆ + tˆr) = spˆqˆ + tpˆˆr, (spˆ + tqˆ)ˆr = spˆˆr + tqˆˆr.

(4.37) (4.38)

Associativity:

pˆ(qˆˆr) = (pˆqˆ)ˆr.

A unit quaternion, qˆ = (qv, qw), is such that n(qˆ) = 1. From this it follows that qˆ may be written as

qˆ = (sin φuq, cos φ) = sin φuq + cos φ, for some three-dimensional vector uq, such that ||uq|| = 1, because

(4.39)

n(qˆ) = n(sin φuq, cos φ) = sin2 φ(uq · uq) + cos2 φ = sin2 φ + cos2 φ = 1

(4.40)

if and only if uq · uq = 1 = ||uq||2. As will be seen in the next section, unit quaternions are perfectly suited for creating rotations and orientations in a most eﬃcient way. But
before that, some extra operations will be introduced for unit quaternions.
For complex numbers, a two-dimensional unit vector can be written as cos φ + i sin φ = eiφ. The equivalent for quaternions is

qˆ = sin φuq + cos φ = eφuq .

(4.41)

The log and the power functions for unit quaternions follow from Equation 4.41:

Logarithm: log(qˆ) = log(eφuq ) = φuq,

Power:

qˆt = (sin φuq + cos φ)t = eφtuq = sin(φt)uq + cos(φt).

(4.42)

4.3. Quaternions

79

Figure 4.9. Illustration of the rotation transform represented by a unit quaternion, qˆ = (sin φuq, cos φ). The transform rotates 2φ radians around the axis uq.

4.3.2 Quaternion Transforms

We will now study a subclass of the quaternion set, namely those of unit length, called unit quaternions. The most important fact about unit quaternions is that they can represent any three-dimensional rotation, and that this representation is extremely compact and simple.
Now we will describe what makes unit quaternions so useful for rotations and orientations. First, put the four coordinates of a point or vector p = (px py pz pw)T into the components of a quaternion pˆ, and assume that we have a unit quaternion qˆ = (sin φuq, cos φ). One can prove that

qˆ pˆ qˆ −1

(4.43)

rotates pˆ (and thus the point p) around the axis uq by an angle 2φ. Note that since qˆ is a unit quaternion, qˆ−1 = qˆ∗. See Figure 4.9.
Any nonzero real multiple of qˆ also represents the same transform, which means that qˆ and −qˆ represent the same rotation. That is, negating the axis, uq, and the real part, qw, creates a quaternion that rotates exactly as the original quaternion does. It also means that the extraction of a quaternion from a matrix can return either qˆ or −qˆ.
Given two unit quaternions, qˆ and ˆr, the concatenation of ﬁrst applying qˆ and then ˆr to a quaternion, pˆ (which can be interpreted as a point p), is given by Equation 4.44:

ˆr(qˆpˆqˆ∗)ˆr∗ = (ˆrqˆ)pˆ(ˆrqˆ)∗ = cˆpˆcˆ∗.

(4.44)

Here, cˆ = ˆrqˆ is the unit quaternion representing the concatenation of the unit quaternions qˆ and ˆr.

Matrix Conversion Since one often needs to combine several diﬀerent transforms, and most of them are in matrix form, a method is needed to convert Equation 4.43 into a matrix. A quaternion,

