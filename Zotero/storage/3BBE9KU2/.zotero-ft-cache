IN ACTION
SECOND EDITION
Anthony Williams
MANNING

Praise for the first edition

“It’s not just the best current treatment of C++11’s threading facilities ... it’s likely to remain the best for some time to come.”
—Scott Meyers, author of Effective C++ and More Effective C++

“Simplifies the dark art of C++ multithreading.”

—Rick Wagner, Red Hat

“Reading this made my brain hurt. But it’s a good hurt.” —Joshua Heyer, Ingersoll Rand

“Anthony shows how to put concurrency into practice.” —Roger Orr, OR/2 Limited

“A thoughtful, in-depth guide to the new concurrency standard for C++ straight from the mouth of one the horses.”
—Neil Horlock, Director, Credit Suisse

“Any serious C++ developers should understand the contents of this important book.” —Dr. Jamie Allsop, Development Director

C++ Concurrency in Action
Second Edition
ANTHONY WILLIAMS
MANNING
SHELTER ISLAND

For online information and ordering of this and other Manning books, please visit www.manning.com. The publisher offers discounts on this book when ordered in quantity. For more information, please contact
Special Sales Department Manning Publications Co. 20 Baldwin Road PO Box 761 Shelter Island, NY 11964 Email: orders@manning.com
©2019 by Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.

Manning Publications Co. 20 Baldwin Road PO Box 761 Shelter Island, NY 11964

Development editors: Cynthia Kane, Jennifer Stout Technical development editor: Alain Couniot
Review editor: Aleksandar Dragosavljevic´ Production editor: Janet Vail
Copy editors: Safis Editing, Heidi Ward Proofreader: Melody Dolab Technical proofreader: Frédéric Flayol
Typesetter: Dennis Dalinnik Cover designer: Marija Tudor

ISBN: 9781617294693 Printed in the United States of America 1 2 3 4 5 6 7 8 9 10 – SP – 24 23 22 21 20 19

To Kim, Hugh, and Erin

brief contents
1 ■ Hello, world of concurrency in C++! 1 2 ■ Managing threads 16 3 ■ Sharing data between threads 36 4 ■ Synchronizing concurrent operations 72 5 ■ The C++ memory model and operations on
atomic types 124 6 ■ Designing lock-based concurrent data structures 173 7 ■ Designing lock-free concurrent data structures 205 8 ■ Designing concurrent code 251 9 ■ Advanced thread management 300 10 ■ Parallel algorithms 327 11 ■ Testing and debugging multithreaded applications 339
vi

contents
preface xiii acknowledgments xv about this book xvii about the author xx about the cover illustration xxi
1 Hello, world of concurrency in C++! 1 1.1 What is concurrency? 2 Concurrency in computer systems 2 ■ Approaches to concurrency 4 ■ Concurrency vs. parallelism 6 1.2 Why use concurrency? 7 Using concurrency for separation of concerns 7 ■ Using concurrency for performance: task and data parallelism 8 When not to use concurrency 9 1.3 Concurrency and multithreading in C++ 10 History of multithreading in C++ 10 ■ Concurrency support in the C++11 standard 11 ■ More support for concurrency and parallelism in C++14 and C++17 12 ■ Efficiency in the C++ Thread Library 12 ■ Platform-specific facilities 13 1.4 Getting started 13 Hello, Concurrent World 14
vii

viii

CONTENTS

2 Managing threads 16 2.1 Basic thread management 17 Launching a thread 17 ■ Waiting for a thread to complete 20 Waiting in exceptional circumstances 20 ■ Running threads in the background 22
2.2 Passing arguments to a thread function 24 2.3 Transferring ownership of a thread 27 2.4 Choosing the number of threads at runtime 31 2.5 Identifying threads 34

3 Sharing data between threads 36 3.1 Problems with sharing data between threads 37 Race conditions 38 ■ Avoiding problematic race conditions 39
3.2 Protecting shared data with mutexes 40 Using mutexes in C++ 41 ■ Structuring code for protecting shared data 42 ■ Spotting race conditions inherent in interfaces 44 Deadlock: the problem and a solution 51 ■ Further guidelines for avoiding deadlock 53 ■ Flexible locking with std::unique_lock 59 ■ Transferring mutex ownership between scopes 61 ■ Locking at an appropriate granularity 62
3.3 Alternative facilities for protecting shared data 64 Protecting shared data during initialization 65 ■ Protecting rarely updated data structures 68 ■ Recursive locking 70
4 Synchronizing concurrent operations 72 4.1 Waiting for an event or other condition 73 Waiting for a condition with condition variables 74 Building a thread-safe queue with condition variables 76
4.2 Waiting for one-off events with futures 81 Returning values from background tasks 82 ■ Associating a task with a future 84 ■ Making (std::)promises 87 ■ Saving an exception for the future 89 ■ Waiting from multiple threads 90
4.3 Waiting with a time limit 93 Clocks 93 ■ Durations 94 ■ Time points 96 ■ Functions that accept timeouts 98

CONTENTS

ix

4.4 Using synchronization of operations to simplify code 99 Functional programming with futures 99 ■ Synchronizing operations with message passing 104 ■ Continuation-style concurrency with the Concurrency TS 108 ■ Chaining continuations 110 ■ Waiting for more than one future 114 Waiting for the first future in a set with when_any 115 Latches and barriers in the Concurrency TS 118 ■ A basic latch type: std::experimental::latch 118 ■ std::experimental::barrier: a basic barrier 120 ■ std::experimental::flex_barrier— std::experimental::barrier’s flexible friend 121
5 The C++ memory model and operations on atomic types 124 5.1 Memory model basics 125 Objects and memory locations 125 ■ Objects, memory locations, and concurrency 126 ■ Modification orders 127
5.2 Atomic operations and types in C++ 128 The standard atomic types 128 ■ Operations on std::atomic_flag 132 ■ Operations on std::atomic<bool> 134 Operations on std::atomic<T*>: pointer arithmetic 137 Operations on standard atomic integral types 138 The std::atomic<> primary class template 138 Free functions for atomic operations 140
5.3 Synchronizing operations and enforcing ordering 142 The synchronizes-with relationship 143 ■ The happens-before relationship 145 ■ Memory ordering for atomic operations 146 Release sequences and synchronizes-with 164 ■ Fences 166 Ordering non-atomic operations with atomics 168 ■ Ordering non-atomic operations 169
6 Designing lock-based concurrent data structures 173 6.1 What does it mean to design for concurrency? 174 Guidelines for designing data structures for concurrency 175
6.2 Lock-based concurrent data structures 176 A thread-safe stack using locks 176 ■ A thread-safe queue using locks and condition variables 179 ■ A thread-safe queue using fine-grained locks and condition variables 183
6.3 Designing more complex lock-based data structures 194 Writing a thread-safe lookup table using locks 194 ■ Writing a thread-safe list using locks 199

x

CONTENTS

7 Designing lock-free concurrent data structures 205 7.1 Definitions and consequences 206 Types of nonblocking data structures 206 ■ Lock-free data structures 207 ■ Wait-free data structures 208 ■ The pros and cons of lock-free data structures 208
7.2 Examples of lock-free data structures 209 Writing a thread-safe stack without locks 210 ■ Stopping those pesky leaks: managing memory in lock-free data structures 214 Detecting nodes that can’t be reclaimed using hazard pointers 218 Detecting nodes in use with reference counting 226 ■ Applying the memory model to the lock-free stack 232 ■ Writing a thread-safe queue without locks 236
7.3 Guidelines for writing lock-free data structures 248 Guideline: use std::memory_order_seq_cst for prototyping 248 Guideline: use a lock-free memory reclamation scheme 248 Guideline: watch out for the ABA problem 249 ■ Guideline: identify busy-wait loops and help the other thread 249
8 Designing concurrent code 251 8.1 Techniques for dividing work between threads 252 Dividing data between threads before processing begins 253 Dividing data recursively 254 ■ Dividing work by task type 258
8.2 Factors affecting the performance of concurrent code 260 How many processors? 261 ■ Data contention and cache ping-pong 262 ■ False sharing 264 ■ How close is your data? 265 ■ Oversubscription and excessive task switching 266
8.3 Designing data structures for multithreaded performance 266 Dividing array elements for complex operations 267 ■ Data access patterns in other data structures 269
8.4 Additional considerations when designing for concurrency 270 Exception safety in parallel algorithms 271 ■ Scalability and Amdahl’s law 277 ■ Hiding latency with multiple threads 279 Improving responsiveness with concurrency 280

CONTENTS

xi

8.5 Designing concurrent code in practice 282 A parallel implementation of std::for_each 282 ■ A parallel implementation of std::find 284 ■ A parallel implementation of std::partial_sum 290
9 Advanced thread management 300 9.1 Thread pools 301 The simplest possible thread pool 301 ■ Waiting for tasks submitted to a thread pool 303 ■ Tasks that wait for other tasks 307 ■ Avoiding contention on the work queue 310 Work stealing 311
9.2 Interrupting threads 315 Launching and interrupting another thread 316 ■ Detecting that a thread has been interrupted 318 ■ Interrupting a condition variable wait 318 ■ Interrupting a wait on std::condition_variable_any 321 ■ Interrupting other blocking calls 323 ■ Handling interruptions 324 Interrupting background tasks on application exit 325
10 Parallel algorithms 327 10.1 Parallelizing the standard library algorithms 327 10.2 Execution policies 328 General effects of specifying an execution policy 328 std::execution::sequenced_policy 330 std::execution::parallel_policy 330 std::execution::parallel_unsequenced_policy 331
10.3 The parallel algorithms from the C++ Standard Library 331 Examples of using parallel algorithms 334 Counting visits 336
11 Testing and debugging multithreaded applications 339 11.1 Types of concurrency-related bugs 340 Unwanted blocking 340 ■ Race conditions 341
11.2 Techniques for locating concurrency-related bugs 342 Reviewing code to locate potential bugs 342 ■ Locating concurrency-related bugs by testing 344 ■ Designing for testability 346 ■ Multithreaded testing techniques 347 Structuring multithreaded test code 350 ■ Testing the performance of multithreaded code 352

xii

CONTENTS

appendix A Brief reference for some C++11 language features 354 appendix B Brief comparison of concurrency libraries 382 appendix C A message-passing framework and complete ATM example 384 appendix D C++ Thread Library reference 401
index 551

preface
I encountered the concept of multithreaded code while working at my first job after I left college. We were writing a data processing application that had to populate a database with incoming data records. There was a lot of data, but each record was independent and required a reasonable amount of processing before it could be inserted into the database. To take full advantage of the power of our 10-CPU UltraSPARC, we ran the code in multiple threads, each thread processing its own set of incoming records. We wrote the code in C++, using POSIX threads, and made a fair number of mistakes—multithreading was new to all of us—but we got there in the end. It was also while working on this project that I first became aware of the C++ Standards Committee and the freshly published C++ Standard.
I have had a keen interest in multithreading and concurrency ever since. Where others saw it as difficult, complex, and a source of problems, I saw it as a powerful tool that could enable your code to take advantage of the available hardware to run faster. Later on, I would learn how it could be used to improve the responsiveness and performance of applications even on single-core hardware, by using multiple threads to hide the latency of time-consuming operations such as I/O. I also learned how it worked at the OS level and how Intel CPUs handled task switching.
Meanwhile, my interest in C++ brought me in contact with the ACCU and then the C++ Standards panel at BSI, as well as Boost. I followed the initial development of the Boost Thread Library with interest, and when it was abandoned by the original developer, I jumped at the chance to get involved. I was the primary developer and maintainer of the Boost Thread Library for a number of years, though I have since handed that responsibility on.
xiii

xiv

PREFACE

As the work of the C++ Standards Committee shifted from fixing defects in the existing standard to writing proposals for the C++11 standard (named C++0x in the hope that it would be finished by 2009, and then officially C++11, because it was finally published in 2011), I got more involved with BSI and started drafting proposals of my own. Once it became clear that multithreading was on the agenda, I jumped in with both feet and authored or co-authored many of the multithreading and concurrencyrelated proposals that shaped this part of the standard. I have continued to be involved with the concurrency group as we worked on the changes for C++17, the Concurrency TS, and proposals for the future. I feel privileged to have had the opportunity to combine two of my major computer-related interests—C++ and multithreading—in this way.
This book draws on all my experience with both C++ and multithreading and aims to teach other C++ developers how to use the C++17 Thread Library and Concurrency TS safely and efficiently. I also hope to impart some of my enthusiasm for the subject along the way.

acknowledgments
I will start by saying a big “Thank you” to my wife, Kim, for all the love and support she has given me while writing this book. The first edition occupied a significant part of my spare time for the four years before publication, and the second edition has again required a significant investment of time, and without her patience, support, and understanding, I couldn’t have managed it.
Second, I would like to thank the team at Manning who have made this book possible: Marjan Bace, publisher; Michael Stephens, associate publisher; Cynthia Kane, my development editor; Aleksandar Dragosavljevic´, review editor; Safis Editing and Heidi Ward, my copyeditors; and Melody Dolab, my proofreader. Without their efforts you would not be reading this book right now.
I would also like to thank the other members of the C++ Standards Committee who wrote committee papers on the multithreading facilities: Andrei Alexandrescu, Pete Becker, Bob Blainer, Hans Boehm, Beman Dawes, Lawrence Crowl, Peter Dimov, Jeff Garland, Kevlin Henney, Howard Hinnant, Ben Hutchings, Jan Kristofferson, Doug Lea, Paul McKenney, Nick McLaren, Clark Nelson, Bill Pugh, Raul Silvera, Herb Sutter, Detlef Vollmann, and Michael Wong, plus all those who commented on the papers, discussed them at the committee meetings, and otherwise helped shaped the multithreading and concurrency support in C++11, C++14, C++17, and the Concurrency TS.
Finally, I would like to thank the following people, whose suggestions have greatly improved this book: Dr. Jamie Allsop, Peter Dimov, Howard Hinnant, Rick Molloy, Jonathan Wakely, and Dr. Russel Winder, with special thanks to Russel for his detailed
xv

xvi

ACKNOWLEDGMENTS

reviews and to Frédéric Flayol, who, as technical proofreader, painstakingly checked all the content for outright errors in the final manuscript during production. (Any remaining mistakes are, of course, all mine.) In addition, I’d like to thank my panel of reviewers for the second edition: Al Norman, Andrei de Araújo Formiga, Chad Brewbaker, Dwight Wilkins, Hugo Filipe Lopes, Vieira Durana, Jura Shikin, Kent R. Spillner, Maria Gemini, Mateusz Malenta, Maurizio Tomasi, Nat Luengnaruemitchai, Robert C. Green II, Robert Trausmuth, Sanchir Kartiev, and Steven Parr. Also, thanks to the readers of the MEAP edition who took the time to point out errors or highlight areas that needed clarifying.

about this book
This book is an in-depth guide to the concurrency and multithreading facilities from the new C++ Standard, from the basic usage of std::thread, std::mutex, and std:: async, to the complexities of atomic operations and the memory model.
Roadmap
The first four chapters introduce the various library facilities provided by the library and show how they can be used.
Chapter 5 covers the low-level nitty-gritty of the memory model and atomic operations, including how atomic operations can be used to impose ordering constraints on other code, and marks the end of the introductory chapters.
Chapters 6 and 7 start the coverage of higher-level topics, with some examples of how to use the basic facilities to build more complex data structures—lock-based data structures in chapter 6, and lock-free data structures in chapter 7.
Chapter 8 continues the higher-level topics, with guidelines for designing multithreaded code, coverage of the issues that affect performance, and example implementations of various parallel algorithms.
Chapter 9 covers thread management—thread pools, work queues, and interrupting operations.
Chapter 10 covers the new parallelism support from C++17, which comes in the form of additional overloads for many of the Standard Library algorithms.
Chapter 11 covers testing and debugging—types of bugs, techniques for locating them, how to test for them, and so forth.
xvii

xviii

ABOUT THIS BOOK

The appendixes include a brief description of some of the new language facilities introduced with the new standard that are relevant to multithreading, the implementation details of the message-passing library mentioned in chapter 4, and a complete reference to the C++17 Thread Library.
Who should read this book
If you're writing multithreaded code in C++, you should read this book. If you're using the new multithreading facilities from the C++ Standard Library, this book is an essential guide. If you’re using alternative thread libraries, the guidelines and techniques from the later chapters should still prove useful.
A good working knowledge of C++ is assumed, though familiarity with the new language features is not—these are covered in appendix A. Prior knowledge or experience of multithreaded programming is not assumed, though it may be useful.
How to use this book
If you’ve never written multithreaded code before, I suggest reading this book sequentially from beginning to end, though possibly skipping the more detailed parts of chapter 5. Chapter 7 relies heavily on the material in chapter 5, so if you skipped chapter 5, you should save chapter 7 until you’ve read it.
If you haven’t used the new C++11 language facilities before, it might be worth skimming appendix A before you start to ensure that you’re up to speed with the examples in the book. The uses of the new language facilities are highlighted in the text, though, and you can always flip to the appendix if you encounter something you haven’t seen before.
If you have extensive experience with writing multithreaded code in other environments, the beginning chapters are probably still worth skimming so you can see how the facilities you know map onto the new standard C++ ones. If you’re going to be doing any low-level work with atomic variables, chapter 5 is a must. Chapter 8 is worth reviewing to ensure that you’re familiar with things like exception safety in multithreaded C++. If you have a particular task in mind, the index and table of contents should help you find a relevant section quickly.
Once you’re up to speed on the use of the C++ Thread Library, appendix D should continue to be useful, such as for looking up the exact details of each class and function call. You may also like to dip back into the main chapters from time to time to refresh your memory on a particular construct or to look at the sample code.
Code conventions and downloads
All source code in listings or in text is in a fixed-width font like this to separate it from ordinary text. Code annotations accompany many of the listings, highlighting important concepts. In some cases, numbered bullets link to explanations that follow the listing.

ABOUT THIS BOOK

xix

Source code for all working examples in this book is available for download from the publisher’s website at www.manning.com/books/c-plus-plus-concurrency-in-actionsecond-edition. You may also download the source code from github at https://github .com/anthonywilliams/ccia_code_samples.
Software requirements
To use the code from this book unchanged, you’ll need a recent C++ compiler that supports the C++17 language features used in the examples (see appendix A), and you’ll need a copy of the C++ Standard Thread Library.
At the time of writing, the latest versions of g++, clang++, and Microsoft Visual Studio all ship with implementations of the C++17 Standard Thread Library. They also support most of the language features from the appendix, and those features that aren't supported are coming soon.
My company, Just Software Solutions Ltd, sells a complete implementation of the C++11 Standard Thread Library for several older compilers, along with an implementation of the Concurrency TS for newer versions of clang, gcc, and Microsoft Visual Studio.1 This implementation has been used for testing the examples in this book.
The Boost Thread Library2 provides an API that’s based on the C++11 Standard Thread Library proposals and is portable to many platforms. Most of the examples from the book can be modified to work with the Boost Thread Library by judicious replacement of std:: with boost:: and use of the appropriate #include directives. There are a few facilities that are either not supported (such as std::async) or have different names (such as boost::unique_future) in the Boost Thread Library.
Book forum
Purchase of C++ Concurrency in Action, Second Edition includes free access to a private web forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the author and from other users. To access the forum, go to www.manning.com/books/c-plus-plus-concurrencyin-action-second-edition. You can also learn more about Manning’s forums and the rules of conduct at https://forums.manning.com/forums/about.
Manning’s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and the author can take place. It’s not a commitment to any specific amount of participation on the part of the author, whose contribution to the book’s forum remains voluntary (and unpaid). We suggest you try asking the author some challenging questions, lest his interest stray!
The forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.

1 The just::thread implementation of the C++ Standard Thread Library, http://www.stdthread.co.uk. 2 The Boost C++ library collection, http://www.boost.org.

about the author
Anthony Williams is a UK-based developer, consultant, and trainer with over 20 years of experience in C++. He has been an active member of the BSI C++ Standards Panel since 2001, and is the author or coauthor of many of the C++ Standards Committee papers that led up to the inclusion of the thread library in the C++11 Standard. He continues to work on new facilities to enhance the C++ concurrency toolkit, both with standards proposals, and implementations of those facilities for the just::thread Pro extensions to the C++ thread library from Just Software Solutions Ltd. Anthony lives in the far west of Cornwall, England.
xx

about the cover illustration
The illustration on the cover of C++ Concurrency in Action is captioned “Habit of a Lady of Japan.” The image is taken from the four-volume Collection of the Dress of Different Nations by Thomas Jefferys, published in London between 1757 and 1772. The collection includes beautiful hand-colored copperplate engravings of costumes from around the world and has influenced theatrical costume design since its publication. The diversity of the drawings in the compendium speaks vividly of the richness of the costumes presented on the London stage over 200 years ago. The costumes, both historical and contemporaneous, offered a glimpse into the dress customs of people living in different times and in different countries, making them come alive for London theater audiences.
Dress codes have changed in the last century, and the diversity by region, so rich in the past, has faded away. It’s now often hard to tell the inhabitant of one continent from another. Perhaps, trying to view it optimistically, we’ve traded a cultural and visual diversity for a more varied personal life—or a more varied and interesting intellectual and technical life.
We at Manning celebrate the inventiveness, the initiative, and the fun of the computer business with book covers based on the rich diversity of the regional and theatrical life of two centuries ago, brought back to life by the pictures from this collection.
xxi

Hello, world of concurrency in C++!
This chapter covers
 What is meant by concurrency and multithreading  Why you might want to use concurrency and
multithreading in your applications  Some of the history of the support for
concurrency in C++  What a simple multithreaded C++ program
looks like
These are exciting times for C++ users. Thirteen years after the original C++ Standard was published in 1998, the C++ Standards Committee gave the language and its supporting library a major overhaul. The new C++ Standard (referred to as C++11 or C++0x) was published in 2011 and brought with it a swath of changes that made working with C++ easier and more productive. The Committee also committed to a new “train model” of releases, with a new C++ Standard to be published every three years. So far, we've had two of these publications: the C++14 Standard in 2014, and the C++17 Standard in 2017, as well as several Technical Specifications describing extensions to the C++ Standard.
1

2

CHAPTER 1 Hello, world of concurrency in C++!

One of the most significant new features in the C++11 Standard was the support of multithreaded programs. For the first time, the C++ Standard acknowledged the existence of multithreaded applications in the language and provided components in the library for writing multithreaded applications. This made it possible to write multithreaded C++ programs without relying on platform-specific extensions and enabled you to write portable multithreaded code with guaranteed behavior. It also came at a time when programmers were increasingly looking to concurrency in general, and multithreaded programming in particular, to improve application performance. The C++14 and C++17 Standards have built upon this baseline to provide further support for writing multithreaded programs in C++, as have the Technical Specifications. There’s a Technical Specification for concurrency extensions, and another for parallelism, though the latter has been incorporated into C++17.
This book is about writing programs in C++ using multiple threads for concurrency and the C++ language features and library facilities that make it possible. I’ll start by explaining what I mean by concurrency and multithreading and why you would want to use concurrency in your applications. After a quick detour into why you might not want to use it in your applications, we’ll go through an overview of the concurrency support in C++, and we’ll round off this chapter with a simple example of C++ concurrency in action. Readers experienced with developing multithreaded applications may wish to skip the early sections. In subsequent chapters, we’ll cover more extensive examples and look at the library facilities in more depth. The book will finish with an in-depth reference to all the C++ Standard Library facilities for multithreading and concurrency.
So, what do I mean by concurrency and multithreading?

1.1 What is concurrency?
At the simplest and most basic level, concurrency is about two or more separate activities happening at the same time. We encounter concurrency as a natural part of life; we can walk and talk at the same time or perform different actions with each hand, and we each go about our lives independently of each other—you can watch football while I go swimming, and so on.

1.1.1

Concurrency in computer systems
When we talk about concurrency in terms of computers, we mean a single system performing multiple independent activities in parallel, rather than sequentially, or one after the other. This isn’t a new phenomenon. Multitasking operating systems that allow a single desktop computer to run multiple applications at the same time through task switching have been commonplace for many years, as have high-end server machines with multiple processors that enable genuine concurrency. What’s new is the increased prevalence of computers that can genuinely run multiple tasks in parallel rather than giving the illusion of doing so.

What is concurrency?

3

Historically, most desktop computers have had one processor, with a single processing unit or core, and this remains true for many desktop machines today. Such a machine can only perform one task at a time, but it can switch between tasks many times per second. By doing a bit of one task and then a bit of another and so on, it appears that the tasks are happening concurrently. This is called task switching. We still talk about concurrency with such systems; because the task switches are so fast, you can’t tell at which point a task may be suspended as the processor switches to another one. The task switching provides the illusion of concurrency to both the user and the applications themselves. Because there is only the illusion of concurrency, the behavior of applications may be subtly different when executing in a single-processor task-switching environment compared to when executing in an environment with true concurrency. In particular, incorrect assumptions about the memory model (covered in chapter 5) may not show up in such an environment. This is discussed in more depth in chapter 10.
Computers containing multiple processors have been used for servers and highperformance computing tasks for years, and computers based on processors with more than one core on a single chip (multicore processors) are becoming increasingly common as desktop machines. Whether they have multiple processors or multiple cores within a processor (or both), these computers are capable of genuinely running more than one task in parallel. We call this hardware concurrency.
Figure 1.1 shows an idealized scenario of a computer with precisely two tasks to do, each divided into 10 equally sized chunks. On a dual-core machine (which has two processing cores), each task can execute on its own core. On a single-core machine doing task switching, the chunks from each task are interleaved. But they are also spaced out a bit (in figure 1.1, this is shown by the gray bars separating the chunks being thicker than the separator bars shown for the dual-core machine); in order to do the interleaving, the system has to perform a context switch every time it changes from one task to another, and this takes time. In order to perform a context switch, the OS has to save the CPU state and instruction pointer for the currently running task, work out which task to switch to, and reload the CPU state for the task being switched to. The CPU will then potentially have to load the memory for the instructions and data for the new task into the cache, which can prevent the CPU from executing any instructions, causing further delay.

Dual core

Core 1 Core 2

Single core
Figure 1.1 Two approaches to concurrency: parallel execution on a dual-core machine versus task switching on a single-core machine

4

CHAPTER 1 Hello, world of concurrency in C++!

Though the availability of concurrency in the hardware is most obvious with multiprocessor or multicore systems, some processors can execute multiple threads on a single core. The important factor to consider is the number of hardware threads, which is the measure of how many independent tasks the hardware can genuinely run concurrently. Even with a system that has genuine hardware concurrency, it’s easy to have more tasks than the hardware can run in parallel, so task switching is still used in these cases. For example, on a typical desktop computer there may be hundreds of tasks running, performing background operations, even when the computer is nominally idle. It’s the task switching that allows these background tasks to run and you to run your word processor, compiler, editor, and web browser (or any combination of applications) all at once. Figure 1.2 shows task switching among four tasks on a dual-core machine, again for an idealized scenario with the tasks divided neatly into equally sized chunks. In practice, many issues will make the divisions uneven and the scheduling irregular. Some of these issues are covered in chapter 8 when we look at factors affecting the performance of concurrent code.

Dual core

Core 1 Core 2

Figure 1.2 Task switching of four tasks on two cores

All the techniques, functions, and classes covered in this book can be used whether your application is running on a machine with one single-core processor or with many multicore processors, and are not affected by whether the concurrency is achieved through task switching or by genuine hardware concurrency. But as you may imagine, how you make use of concurrency in your application may well depend on the amount of hardware concurrency available. This is covered in chapter 8, where I discuss the issues involved in designing concurrent code in C++.

1.1.2

Approaches to concurrency
Imagine, for a moment, a pair of programmers working together on a software project. If your developers are in separate offices, they can go about their work peacefully, without being disturbed by each other, and they each have their own set of reference manuals. But communication isn’t straightforward; rather than turning around and talking to each other, they have to use the phone or email, or get up and walk to the other’s office. Also, you have the overhead of two offices to manage and multiple copies of reference manuals to purchase.
Now imagine that you move your developers into the same office. They can now talk to each other freely to discuss the design of the application, and they can easily

What is concurrency?

5

draw diagrams on paper or on a whiteboard to help with design ideas or explanations. You have only one office to manage, and one set of resources will often suffice. On the negative side, they might find it harder to concentrate, and there may be issues with sharing resources (“Where’s the reference manual gone now?”).
These two ways of organizing your developers illustrate the two basic approaches to concurrency. Each developer represents a thread, and each office represents a process. The first approach is to have multiple single-threaded processes, which is similar to having each developer in their own office, and the second approach is to have multiple threads in a single process, which is like having two developers in the same office. You can combine these in an arbitrary fashion and have multiple processes, some of which are multithreaded and some of which are single-threaded, but the principles are the same. Let’s now have a brief look at these two approaches to concurrency in an application.

CONCURRENCY WITH MULTIPLE PROCESSES

The first way to make use of concurrency within an application is to divide the application into multiple, separate, single-threaded processes that are run at the same time,

Process 1 Thread

much as you can run your web browser and word proces-

sor at the same time. These separate processes can then

pass messages to each other through all the normal interprocess communication channels (signals, sockets, files, pipes, and so on), as shown in figure 1.3. One downside is that such communication between processes is often

Interprocess communication
Operating system

either complicated to set up or slow, or both, because

Thread

operating systems typically provide a lot of protection between processes to avoid one process accidentally modi-

Process 2

fying data belonging to another process. Another downside is that there’s an inherent overhead in running multiple processes: it takes time to start a process, the operating

Figure 1.3 Communication between a pair of processes running concurrently

system must devote internal resources to managing the

process, and so forth.

It’s not all negative: the added protection operating systems typically provide

between processes and the higher-level communication mechanisms mean that it

can be easier to write safe concurrent code with processes rather than threads.

Indeed, environments such as that provided for the Erlang (www.erlang.org/) pro-

gramming language use processes as the fundamental building block of concur-

rency to great effect.

Using separate processes for concurrency also has an additional advantage—you

can run the separate processes on distinct machines connected over a network. Though

this increases the communication cost, on a carefully designed system it can be a cost-

effective way of increasing the available parallelism and improving performance.

6

CHAPTER 1 Hello, world of concurrency in C++!

CONCURRENCY WITH MULTIPLE THREADS The alternative approach to concurrency is to run multiple

Process

threads in a single process. Threads are much like light-

Thread 1

weight processes: each thread runs independently of the

others, and each may run a different sequence of instructions. But all threads in a process share the same address

Shared memory

space, and most of the data can be accessed directly from

all threads—global variables remain global, and pointers or references to objects or data can be passed around among

Thread 2

threads. Although it’s often possible to share memory

among processes, this is complicated to set up and often Figure 1.4 Communication

hard to manage, because memory addresses of the same data aren’t necessarily the same in different processes. Figure 1.4 shows two threads within a process communicating

between a pair of threads running concurrently in a single process

through shared memory.

The shared address space and lack of protection of data between threads makes

the overhead associated with using multiple threads much smaller than that from

using multiple processes, because the operating system has less bookkeeping to do.

But the flexibility of shared memory also comes with a price: if data is accessed by mul-

tiple threads, the application programmer must ensure that the view of data seen by

each thread is consistent whenever it’s accessed. The issues surrounding sharing data

between threads, and the tools to use and guidelines to follow to avoid problems, are

covered throughout this book, notably in chapters 3, 4, 5, and 8. The problems aren’t

insurmountable, provided suitable care is taken when writing the code, but they do

mean that a great deal of thought must go into the communication between threads.

The low overhead associated with launching and communicating between multi-

ple threads within a process compared to launching and communicating between

multiple single-threaded processes means that this is the favored approach to concur-

rency in mainstream languages, including C++, despite the potential problems arising

from the shared memory. In addition, the C++ Standard doesn’t provide any intrinsic

support for communication between processes, so applications that use multiple pro-

cesses will have to rely on platform-specific APIs to do so. This book therefore focuses

exclusively on using multithreading for concurrency, and future references to concur-

rency assume that this is achieved by using multiple threads.

There’s another word that gets used a lot around multithreaded code: parallelism.

Let’s clarify the differences.

1.1.3

Concurrency vs. parallelism
Concurrency and parallelism have largely overlapping meanings with respect to multithreaded code. Indeed, to many they mean the same thing. The difference is primarily a matter of nuance, focus, and intent. Both terms are about running multiple tasks simultaneously, using the available hardware, but parallelism is much more

Why use concurrency?

7

performance-oriented. People talk about parallelism when their primary concern is taking advantage of the available hardware to increase the performance of bulk data processing, whereas people talk about concurrency when their primary concern is separation of concerns, or responsiveness. This dichotomy is not cut and dried, and there is still considerable overlap in meaning, but it can help clarify discussions to know of this distinction. Throughout this book, there will be examples of both.
Having clarified what we mean by concurrency and parallelism, let’s look at why you would use concurrency in your applications.

1.2 Why use concurrency?
There are two main reasons to use concurrency in an application: separation of concerns and performance. In fact, I’d go so far as to say that they’re almost the only reasons to use concurrency; anything else boils down to one or the other (or maybe even both) when you look hard enough (well, except for reasons like “because I want to”).

1.2.1

Using concurrency for separation of concerns
Separation of concerns is almost always a good idea when writing software; by grouping related bits of code together and keeping unrelated bits of code apart, you can make your programs easier to understand and test, and less likely to contain bugs. You can use concurrency to separate distinct areas of functionality, even when the operations in these distinct areas need to happen at the same time; without the explicit use of concurrency, you either have to write a task-switching framework or actively make calls to unrelated areas of code during an operation.
Consider a processing-intensive application with a user interface, such as a DVD player application for a desktop computer. This application fundamentally has two sets of responsibilities. Not only does it have to read the data from the disk, decode the images and sound, and send them to the graphics and sound hardware in a timely fashion so the DVD plays without glitches, but it must also take input from the user, such as when the user clicks Pause or Return To Menu, or even Quit. In a single thread, the application has to check for user input at regular intervals during the playback, conflating the DVD playback code with the user interface code. By using multithreading to separate these concerns, the user interface code and DVD playback code no longer have to be so closely intertwined; one thread can handle the user interface and another the DVD playback. There will have to be interaction between them, such as when the user clicks Pause, but now these interactions are directly related to the task at hand.
This gives the illusion of responsiveness, because the user interface thread can typically respond immediately to a user request, even if the response is to display a busy cursor or a Please Wait message while the request is conveyed to the thread doing the work. Similarly, separate threads are often used to run tasks that must run continuously in the background, such as monitoring the filesystem for changes in a desktop search application. Using threads in this way generally makes the logic in each thread

8

CHAPTER 1 Hello, world of concurrency in C++!

much simpler, because the interactions between them can be limited to clearly identifiable points, rather than having to intersperse the logic of the different tasks.
In this case, the number of threads is independent of the number of CPU cores available, because the division into threads is based on the conceptual design rather than an attempt to increase throughput.

1.2.2

Using concurrency for performance: task and data parallelism
Multiprocessor systems have existed for decades, but until recently they were mostly found only in supercomputers, mainframes, and large server systems. But chip manufacturers have increasingly been favoring multicore designs with 2, 4, 16, or more processors on a single chip over better performance with a single core. Consequently, multicore desktop computers, and even multicore embedded devices, are now increasingly prevalent. The increased computing power of these machines comes not from running a single task faster but from running multiple tasks in parallel. In the past, programmers have been able to sit back and watch their programs get faster with each new generation of processors, without any effort on their part. But now, as Herb Sutter put it, “The free lunch is over.”1 If software is to take advantage of this increased computing power, it must be designed to run multiple tasks concurrently. Programmers must therefore take heed, and those who have hitherto ignored concurrency must now look to add it to their toolbox.
There are two ways to use concurrency for performance. The first, and most obvious, is to divide a single task into parts and run each in parallel, reducing the total runtime. This is task parallelism. Although this sounds straightforward, it can be quite a complex process, because there may be many dependencies between the various parts. The divisions may be either in terms of processing—one thread performs one part of the algorithm while another thread performs a different part—or in terms of data—each thread performs the same operation on different parts of the data. This latter approach is called data parallelism.
Algorithms that are readily susceptible to such parallelism are frequently called embarrassingly parallel. Despite the implication that you might be embarrassed to have code so easy to parallelize, this is a good thing; other terms I’ve encountered for such algorithms are naturally parallel and conveniently concurrent. Embarrassingly parallel algorithms have good scalability properties—as the number of available hardware threads goes up, the parallelism in the algorithm can be increased to match. Such an algorithm is the perfect embodiment of the adage, “Many hands make light work.” For those parts of the algorithm that aren’t embarrassingly parallel, you might be able to divide the algorithm into a fixed (and therefore not scalable) number of parallel tasks. Techniques for dividing tasks between threads are covered in chapters 8 and 10.
The second way to use concurrency for performance is to use the available parallelism to solve bigger problems; rather than processing one file at a time, process 2, or

1 “The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software,” Herb Sutter, Dr. Dobb’s Journal, 30(3), March 2005. http://www.gotw.ca/publications/concurrency-ddj.htm.

Why use concurrency?

9

10, or 20, as appropriate. Although this is an application of data parallelism, by performing the same operation on multiple sets of data concurrently, there’s a different focus. It still takes the same amount of time to process one chunk of data, but now more data can be processed in the same amount of time. Obviously, there are limits to this approach, and this won’t be beneficial in all cases, but the increase in throughput that comes from this approach can make new things possible—increased resolution in video processing, for example, if different areas of the picture can be processed in parallel.

1.2.3

When not to use concurrency
It’s just as important to know when not to use concurrency as it is to know when to use it. Fundamentally, the only reason not to use concurrency is when the benefit isn’t worth the cost. Code using concurrency is harder to understand in many cases, so there’s a direct intellectual cost to writing and maintaining multithreaded code, and the additional complexity can also lead to more bugs. Unless the potential performance gain is large enough or the separation of concerns is clear enough to justify the additional development time required to get it right and the additional costs associated with maintaining multithreaded code, don’t use concurrency.
Also, the performance gain might not be as large as expected; there’s an inherent overhead associated with launching a thread, because the OS has to allocate the associated kernel resources and stack space and then add the new thread to the scheduler, all of which takes time. If the task being run on the thread is completed quickly, the time taken by the task may be dwarfed by the overhead of launching the thread, possibly making the overall performance of the application worse than if the task had been executed directly by the spawning thread.
Furthermore, threads are a limited resource. If you have too many threads running at once, this consumes OS resources and may make the system as a whole run slower. Not only that, but using too many threads can exhaust the available memory or address space for a process, because each thread requires a separate stack space. This is particularly a problem for 32-bit processes with a flat architecture where there’s a 4 GB limit to the available address space: if each thread has a 1 MB stack (as is typical on many systems), then the address space would be used up with 4,096 threads, without allowing any space for code, static data, or heap data. Although 64-bit (or larger) systems don’t have this direct address-space limit, they still have finite resources: if you run too many threads, this will eventually cause problems. Though thread pools (see chapter 9) can be used to limit the number of threads, they aren’t a silver bullet, and they do have their own issues.
If the server side of a client/server application launches a separate thread for each connection, this will work fine for a small number of connections, but can quickly exhaust system resources by launching too many threads if the same technique is used for a high-demand server that has to handle many connections. In this scenario, careful use of thread pools can provide optimal performance (see chapter 9).

10

CHAPTER 1 Hello, world of concurrency in C++!

Finally, the more threads you have running, the more context switching the operating system has to do. Each context switch takes time that could be spent doing useful work, so at some point, adding an extra thread will reduce the overall application performance rather than increase it. For this reason, if you’re trying to achieve the best possible performance of the system, it’s necessary to adjust the number of threads running to take into account the available hardware concurrency (or lack of it).
The use of concurrency for performance is like any other optimization strategy: it has the potential to greatly improve the performance of your application, but it can also complicate the code, making it harder to understand and more prone to bugs. Therefore, it’s only worth doing for those performance-critical parts of the application where there’s the potential for measurable gain. Of course, if the potential for performance gains is only secondary to clarity of design or separation of concerns, it may still be worth using a multithreaded design.
Assuming that you’ve decided you do want to use concurrency in your application, whether for performance, separation of concerns, or because it’s “multithreading Monday,” what does that mean for C++ programmers?

1.3 Concurrency and multithreading in C++
Standardized support for concurrency through multithreading is a relatively new thing for C++. It’s only since the C++11 Standard that you’ve been able to write multithreaded code without resorting to platform-specific extensions. In order to understand the rationale behind lots of the decisions in the Standard C++ Thread Library, it’s important to understand the history.

1.3.1

History of multithreading in C++
The 1998 C++ Standard doesn’t acknowledge the existence of threads, and the operational effects of the various language elements are written in terms of a sequential abstract machine. Not only that, but the memory model isn’t formally defined, so you can’t write multithreaded applications without compiler-specific extensions to the 1998 C++ Standard.
Compiler vendors are free to add extensions to the language, and the prevalence of C APIs for multithreading—such as those in the POSIX C standard and the Microsoft Windows API—has led many C++ compiler vendors to support multithreading with various platform-specific extensions. This compiler support is generally limited to allowing the use of the corresponding C API for the platform and ensuring that the C++ Runtime Library (such as the code for the exception-handling mechanism) works in the presence of multiple threads. Although few compiler vendors have provided a formal multithreading-aware memory model, the behavior of the compilers and processors has been sufficiently good that a large number of multithreaded C++ programs have been written.
Not content with using the platform-specific C APIs for handling multithreading, C++ programmers have looked to their class libraries to provide object-oriented

Concurrency and multithreading in C++

11

multithreading facilities. Application frameworks, such as MFC, and general-purpose C++ libraries, such as Boost and ACE, have accumulated sets of C++ classes that wrap the underlying platform-specific APIs and provide higher-level facilities for multithreading that simplify tasks. Although the precise details of the class libraries vary considerably, particularly in the area of launching new threads, the overall shape of the classes has a lot in common. One particularly important design that’s common to many C++ class libraries, and that provides considerable benefit to the programmer, is the use of the Resource Acquisition Is Initialization (RAII) idiom with locks to ensure that mutexes are unlocked when the relevant scope is exited.
For many cases, the multithreading support of existing C++ compilers combined with the availability of platform-specific APIs and platform-independent class libraries, such as Boost and ACE, provide a solid foundation on which to write multithreaded C++ code, and as a result, there are probably millions of lines of C++ code written as part of multithreaded applications. But the lack of standard support means that there are occasions where the lack of a thread-aware memory model causes problems, particularly for those who try to gain higher performance by using knowledge of the processor hardware or for those writing cross-platform code where the behavior of the compilers varies between platforms.

1.3.2

Concurrency support in the C++11 standard
All this changed with the release of the C++11 Standard. Not only is there a threadaware memory model, but the C++ Standard Library was extended to include classes for managing threads (see chapter 2), protecting shared data (see chapter 3), synchronizing operations between threads (see chapter 4), and low-level atomic operations (see chapter 5).
The C++11 Thread Library is heavily based on the prior experience accumulated through the use of the C++ class libraries mentioned previously. In particular, the Boost Thread Library was used as the primary model on which the new library is based, with many of the classes sharing their names and structure with the corresponding ones from Boost. As the standard has evolved, this has been a two-way flow, and the Boost Thread Library has itself changed to match the C++ Standard in many respects, so users transitioning from Boost should find themselves at home.
Concurrency support is one of the changes with the C++11 Standard—as mentioned at the beginning of this chapter, there are many enhancements to the language to make programmers’ lives easier. Although these are generally outside the scope of this book, some of those changes have had a direct impact on the Thread Library and the ways in which it can be used. Appendix A provides a brief introduction to these language features.

12

CHAPTER 1 Hello, world of concurrency in C++!

1.3.3

More support for concurrency and parallelism in C++14 and C++17
The only specific support for concurrency and parallelism added in C++14 was a new mutex type for protecting shared data (see chapter 3). But C++17 adds considerably more: a full suite of parallel algorithms (see chapter 10) for starters. Both of these Standards enhance the core language and the rest of the Standard Library, and these enhancements can simplify the writing of multithreaded code.
As mentioned previously, there’s also a Technical Specification for concurrency, which describes extensions to the functions and classes provided by the C++ Standard, especially around synchronizing operations between threads (see chapter 4).
The support for atomic operations directly in C++ enables programmers to write efficient code with defined semantics without the need for platform-specific assembly language. This is a real boon for those trying to write efficient, portable code; not only does the compiler take care of the platform specifics, but the optimizer can be written to take into account the semantics of the operations, enabling better optimization of the program as a whole.

1.3.4

Efficiency in the C++ Thread Library
One of the concerns that developers involved in high-performance computing often raise regarding C++ in general, and C++ classes that wrap low-level facilities—such as those in the new Standard C++ Thread Library specifically—is that of efficiency. If you’re after the utmost in performance, it’s important to understand the implementation costs associated with using any high-level facilities, compared to using the underlying low-level facilities directly. This cost is the abstraction penalty.
The C++ Standards Committee was aware of this when designing the C++ Standard Library in general and the Standard C++ Thread Library in particular; one of the design goals has been that there should be little or no benefit to be gained from using the lower-level APIs directly, where the same facility is to be provided. The library has therefore been designed to allow for efficient implementation (with a low abstraction penalty) on most major platforms.
Another goal of the C++ Standards Committee has been to ensure that C++ provides sufficient low-level facilities for those wishing to work close to the metal for the ultimate performance. To this end, along with the new memory model comes a comprehensive atomic operations library for direct control over individual bits and bytes and the inter-thread synchronization and visibility of any changes. These atomic types and the corresponding operations can now be used in many places where developers would previously have chosen to drop down to platform-specific assembly language. Code using the new standard types and operations is more portable and easier to maintain.
The C++ Standard Library also provides higher-level abstractions and facilities that make writing multithreaded code easier and less error-prone. Sometimes the use of these facilities comes with a performance cost because of the additional code that

Getting started

13

must be executed. But this performance cost doesn’t necessarily imply a higher abstraction penalty; in general, the cost is no higher than would be incurred by writing equivalent functionality by hand, and the compiler may inline much of the additional code anyway.
In some cases, the high-level facilities provide additional functionality beyond what may be required for a specific use. Most of the time this isn’t an issue: you don’t pay for what you don’t use. On rare occasions, this unused functionality will impact the performance of other code. If you’re aiming for performance and the cost is too high, you may be better off handcrafting the desired functionality from lower-level facilities. In the vast majority of cases, the additional complexity and chance of errors far outweigh the potential benefits from a small performance gain. Even if profiling does demonstrate that the bottleneck is in the C++ Standard Library facilities, it may be due to poor application design rather than a poor library implementation. For example, if too many threads are competing for a mutex, it will impact the performance significantly. Rather than trying to shave a small fraction of time off the mutex operations, it would probably be more beneficial to restructure the application so that there’s less contention on the mutex. Designing applications to reduce contention is covered in chapter 8.
In those rare cases where the C++ Standard Library doesn’t provide the performance or behavior required, it might be necessary to use platform-specific facilities.

1.3.5

Platform-specific facilities
Although the C++ Thread Library provides reasonably comprehensive facilities for multithreading and concurrency, on any given platform there will be platform-specific facilities that go beyond what’s offered. In order to gain easy access to those facilities without giving up the benefits of using the Standard C++ Thread Library, the types in the C++ Thread Library may offer a native_handle() member function that allows the underlying implementation to be directly manipulated using a platform-specific API. By its nature, any operations performed using native_handle() are entirely platform dependent and beyond of the scope of this book (and the Standard C++ Library itself).
Before even considering using platform-specific facilities, it’s important to understand what the Standard Library provides, so let’s get started with an example.

1.4 Getting started
OK, so you have a nice, shiny C++11/C++14/C++17 compiler. What’s next? What does a multithreaded C++ program look like? It looks much like any other C++ program, with the usual mix of variables, classes, and functions. The only real distinction is that some functions might be running concurrently, so you need to ensure that shared data is safe for concurrent access, as described in chapter 3. In order to run functions concurrently, specific functions and objects must be used to manage the different threads.

14

CHAPTER 1 Hello, world of concurrency in C++!

1.4.1

Hello, Concurrent World
Let’s start with a classic example: a program to print “Hello World.” A simple Hello World program that runs in a single thread is shown here, to serve as a baseline when we move to multiple threads:

#include <iostream> int main() {
std::cout<<"Hello World\n"; }

All this program does is write “Hello World” to the standard output stream. Let’s compare it to the simple Hello Concurrent World program shown in the following listing, which starts a separate thread to display the message.

Listing 1.1 A simple Hello Concurrent World program
#include <iostream> #include <thread> void hello() {
std::cout<<"Hello Concurrent World\n"; } int main() {
std::thread t(hello); 3 t.join(); }

The first difference is the extra #include <thread>. The declarations for the multithreading support in the Standard C++ Library are in new headers: the functions and classes for managing threads are declared in <thread>, whereas those for protecting shared data are declared in other headers.
Second, the code for writing the message has been moved to a separate function. This is because every thread has to have an initial function, where the new thread of execution begins. For the initial thread in an application, this is main(), but for every other thread it’s specified in the constructor of a std::thread object—in this case, the std::thread object named t has the new hello() function as its initial function.
This is the next difference: rather than writing directly to standard output or calling hello() from main(), this program launches a new thread to do it, bringing the thread count to two—the initial thread that starts at main() and the new thread that starts at hello().
After the new thread has been launched, the initial thread continues execution. If it didn’t wait for the new thread to finish, it would merrily continue to the end of main() and end the program—possibly before the new thread had a chance to run. This is why the call to join() is there—as described in chapter 2, this causes the calling

Summary

15

thread (in main()) to wait for the thread associated with the std::thread object, in this case, t.
If this seems like a lot of effort to write a message to standard output, it is—as described in section 1.2.3, it’s generally not worth the effort to use multiple threads for such a simple task, especially if the initial thread has nothing to do in the meantime. Later in the book, you’ll work through examples of scenarios where there’s a clear gain to using multiple threads.
Summary
In this chapter, I covered what’s meant by concurrency and multithreading and why you’d choose to use it (or not) in your applications. I also covered the history of multithreading in C++, from the complete lack of support in the 1998 standard, through various platform-specific extensions, to proper multithreading support in the C++11 Standard, and on to the C++14 and C++17 standards and the Technical Specification for concurrency. This support has come in time to allow programmers to take advantage of the greater hardware concurrency becoming available with newer CPUs, as chip manufacturers choose to add more processing power in the form of multiple cores that allow more tasks to be executed concurrently, rather than increasing the execution speed of a single core.
I also showed how simple using the classes and functions from the C++ Standard Library can be in the examples in section 1.4. In C++, using multiple threads isn’t complicated in and of itself; the complexity lies in designing the code so that it behaves as intended.
After the examples of section 1.4, it’s time for something with a bit more substance. In chapter 2, we’ll look at the classes and functions available for managing threads.

Managing threads
This chapter covers
 Starting threads, and various ways of specifying code to run on a new thread
 Waiting for a thread to finish versus leaving it to run
 Uniquely identifying threads
OK, so you’ve decided to use concurrency for your application. In particular, you’ve decided to use multiple threads. What now? How do you launch these threads, check that they’ve finished, and keep tabs on them? The C++ Standard Library makes most thread-management tasks relatively easy, with almost everything managed through the std::thread object associated with a given thread, as you’ll see. For those tasks that aren’t so straightforward, the library provides the flexibility to build what you need from the basic building blocks.
In this chapter, I’ll start by covering the basics: launching a thread, waiting for it to finish, or running it in the background. We’ll then look at passing additional parameters to the thread function when it’s launched and how to transfer ownership of a thread from one std::thread object to another. Finally, we’ll look at choosing the number of threads to use and identifying particular threads.
16

Basic thread management

17

2.1 Basic thread management
Every C++ program has at least one thread, which is started by the C++ runtime: the thread running main(). Your program can then launch additional threads that have another function as the entry point. These threads then run concurrently with each other and with the initial thread. In the same way that the program exits when it returns from main(), when the specified entry point function returns, the thread exits. As you’ll see, if you have a std::thread object for a thread, you can wait for it to finish; but first you have to start it, so let’s look at launching threads.

2.1.1

Launching a thread
As you saw in chapter 1, threads are started by constructing a std::thread object that specifies the task to run on that thread. In the simplest case, that task is a plain, ordinary void-returning function that takes no parameters. This function runs on its own thread until it returns, and then the thread stops. At the other extreme, the task could be a function object that takes additional parameters and performs a series of independent operations that are specified through some kind of messaging system while it’s running, and the thread stops only when it’s signaled to do so, again via some kind of messaging system. It doesn’t matter what the thread is going to do or where it’s launched from, but starting a thread using the C++ Standard Library always boils down to constructing a std::thread object:

void do_some_work(); std::thread my_thread(do_some_work);

This is about as simple as it gets. Of course, you have to make sure that the <thread> header is included so the compiler can see the definition of the std::thread class. As with much of the C++ Standard Library, std::thread works with any callable type, so you can pass an instance of a class with a function call operator to the std::thread constructor instead:
class background_task { public:
void operator()() const {
do_something(); do_something_else(); } }; background_task f; std::thread my_thread(f);

In this case, the supplied function object is copied into the storage belonging to the newly created thread of execution and invoked from there. It’s therefore essential that the copy behaves equivalently to the original, or the result may not be what’s expected.

18

CHAPTER 2 Managing threads

One thing to consider when passing a function object to the thread constructor is to avoid what’s dubbed “C++’s most vexing parse.” If you pass a temporary rather than a named variable, the syntax can be the same as that of a function declaration, in which case the compiler interprets it as such, rather than an object definition. For example,
std::thread my_thread(background_task());
declares a my_thread function that takes a single parameter (of type pointer-to-afunction-taking-no-parameters-and-returning-a-background_task-object) and returns a std::thread object, rather than launching a new thread. You can avoid this by naming your function object as shown previously, by using an extra set of parentheses, or by using the new uniform initialization syntax; for example:
std::thread my_thread((background_task())); std::thread my_thread{background_task()};
In the first example, the extra parentheses prevent interpretation as a function declaration, allowing my_thread to be declared as a variable of type std::thread. The second example uses the new uniform initialization syntax with braces rather than parentheses, and thus would also declare a variable.
One type of callable object that avoids this problem is a lambda expression. This is a new feature from C++11 which allows you to write a local function, possibly capturing some local variables and avoiding the need to pass additional arguments (see section 2.2). For full details on lambda expressions, see appendix A, section A.5. The previous example can be written using a lambda expression as follows:
std::thread my_thread([]{ do_something(); do_something_else();
});
Once you’ve started your thread, you need to explicitly decide whether to wait for it to finish (by joining with it—see section 2.1.2) or leave it to run on its own (by detaching it—see section 2.1.3). If you don’t decide before the std::thread object is destroyed, then your program is terminated (the std::thread destructor calls std::terminate()). It’s therefore imperative that you ensure that the thread is correctly joined or detached, even in the presence of exceptions. See section 2.1.3 for a technique to handle this scenario. Note that you only have to make this decision before the std::thread object is destroyed—the thread itself may well have finished long before you join with it or detach it, and if you detach it, then if the thread is still running, it will continue to do so, and may continue running long after the std::thread object is destroyed; it will only stop running when it finally returns from the thread function.
If you don’t wait for your thread to finish, you need to ensure that the data accessed by the thread is valid until the thread has finished with it. This isn’t a new

Basic thread management

19

problem—even in single-threaded code it’s undefined behavior to access an object after it’s been destroyed—but the use of threads provides an additional opportunity to encounter such lifetime issues.
One situation in which you can encounter such problems is when the thread function holds pointers or references to local variables and the thread hasn’t finished when the function exits. The following listing shows an example of such a scenario.

Listing 2.1 A function that returns while a thread still has access to local variables

struct func

{

int& i;

func(int& i_):i(i_){}

void operator()()

{

for(unsigned j=0;j<1000000;++j)

{

do_something(i); } }

Potential access to dangling reference

};

void oops()

{ int some_local_state=0; func my_func(some_local_state);

Don’t wait for thread to finish

std::thread my_thread(my_func); my_thread.detach(); }

New thread might still be running

In this case, the new thread associated with my_thread will probably still be running when oops exits, because you’ve explicitly decided not to wait for it by calling detach(). If the thread is still running, you have the scenario shown in table 2.1: the next call to do_something(i) will access an already destroyed variable. This is like normal singlethreaded code—allowing a pointer or reference to a local variable to persist beyond the function exit is never a good idea—but it’s easier to make the mistake with multithreaded code, because it isn’t necessarily immediately apparent that this has happened.

Table 2.1 Accessing a local variable with a detached thread after it has been destroyed

Main thread

New thread

Constructs my_func with reference to some_local_state

Starts new thread my_thread

Started

Calls func::operator()

Detaches my_thread

Running func::operator(); may call do_something with reference to some_local_state

20

CHAPTER 2 Managing threads

Table 2.1 Accessing a local variable with a detached thread after it has been destroyed (continued)

Main thread

New thread

Destroys some_local_state
Exits oops

Still running
Still running func::operator(); may call do_something with reference to some_local_state => undefined behavior

One common way to handle this scenario is to make the thread function self-contained and copy the data into the thread rather than sharing the data. If you use a callable object for your thread function, that object is copied into the thread, so the original object can be destroyed immediately. But you still need to be wary of objects containing pointers or references, such as in listing 2.1. In particular, it’s a bad idea to create a thread within a function that has access to the local variables in that function, unless the thread is guaranteed to finish before the function exits.
Alternatively, you can ensure that the thread has completed execution before the function exits by joining with the thread.

2.1.2

Waiting for a thread to complete
If you need to wait for a thread to complete, you can do this by calling join() on the associated std::thread instance. In the case of listing 2.1, replacing the call to my_thread.detach() before the closing brace of the function body with a call to my_thread.join() would therefore be sufficient to ensure that the thread was finished before the function was exited and thus before the local variables were destroyed. In this case, it would mean there was little point in running the function on a separate thread, because the first thread wouldn’t be doing anything useful in the meantime, but in real code the original thread would either have work to do or would have launched several threads to do useful work before waiting for all of them to complete.
join() is a simple and brute-force technique—either you wait for a thread to finish or you don’t. If you need more fine-grained control over waiting for a thread, such as to check whether a thread is finished, or to wait only a certain period of time, then you have to use alternative mechanisms such as condition variables and futures, which we’ll look at in chapter 4. The act of calling join() also cleans up any storage associated with the thread, so the std::thread object is no longer associated with the nowfinished thread; it isn’t associated with any thread. This means that you can call join() only once for a given thread; once you’ve called join(), the std::thread object is no longer joinable, and joinable() will return false.

2.1.3

Waiting in exceptional circumstances
As mentioned earlier, you need to ensure that you’ve called either join() or detach() before a std::thread object is destroyed. If you’re detaching a thread, you can usually call detach() immediately after the thread has been started, so this isn’t a problem. But if you’re intending to wait for the thread, you need to carefully pick the

Basic thread management

21

place in the code where you call join(). This means that the call to join() is liable to be skipped if an exception is thrown after the thread has been started but before the call to join().
To avoid your application being terminated when an exception is thrown, you therefore need to make a decision about what to do in this case. In general, if you were intending to call join() in a non-exceptional case, you also need to call join() in the presence of an exception to avoid accidental lifetime problems. The next listing shows some simple code that does just that.

Listing 2.2 Waiting for a thread to finish

struct func; void f() {

See definition in listing 2.1

int some_local_state=0;

func my_func(some_local_state);

std::thread t(my_func);

try

{

do_something_in_current_thread();

}

catch(...)

{

t.join();

throw;

}

t.join();

}

The code in listing 2.2 uses a try/catch block to ensure that a thread with access to local state is finished before the function exits, whether the function exits normally, or by an exception. The use of try/catch blocks is verbose, and it’s easy to get the scope slightly wrong, so this isn’t an ideal scenario. If it’s important to ensure that the thread completes before the function exits—whether because it has a reference to other local variables or for any other reason—then it’s important to ensure this is the case for all possible exit paths, whether normal or exceptional, and it’s desirable to provide a simple, concise mechanism for doing so.
One way of doing this is to use the standard Resource Acquisition Is Initialization (RAII) idiom and provide a class that does the join() in its destructor, as in the following listing. See how it simplifies the f() function.

Listing 2.3 Using RAII to wait for a thread to complete
class thread_guard {
std::thread& t; public:
explicit thread_guard(std::thread& t_): t(t_)

22

CHAPTER 2 Managing threads

{}

~thread_guard()

{

if(t.joinable())

{

t.join();

}

}

thread_guard(thread_guard const&)=delete;

thread_guard& operator=(thread_guard const&)=delete;

};

struct func; void f() {

See definition in listing 2.1

int some_local_state=0;

func my_func(some_local_state);

std::thread t(my_func);

thread_guard g(t);

do_something_in_current_thread();

}

When the execution of the current thread reaches the end of f, the local objects are destroyed in reverse order of construction. Consequently, the thread_guard object, g, is destroyed first, and the thread is joined with, in the destructor. This even happens if the function exits because do_something_in_current_thread throws an exception.
The destructor of thread_guard in listing 2.3 first tests to see if the std::thread object is joinable() before calling join(). This is important, because join() can be called only once for a given thread of execution, so it would be a mistake to do so if the thread had already been joined.
The copy constructor and copy-assignment operators are marked =delete to ensure that they’re not automatically provided by the compiler. Copying or assigning such an object would be dangerous, because it might then outlive the scope of the thread it was joining. By declaring them as deleted, any attempt to copy a thread_guard object will generate a compilation error. See appendix A, section A.2, for more about deleted functions.
If you don’t need to wait for a thread to finish, you can avoid this exception-safety issue by detaching it. This breaks the association of the thread with the std::thread object and ensures that std::terminate() won’t be called when the std::thread object is destroyed, even though the thread is still running in the background.

2.1.4

Running threads in the background
Calling detach() on a std::thread object leaves the thread to run in the background, with no direct means of communicating with it. It’s no longer possible to wait for that thread to complete; if a thread becomes detached, it isn’t possible to obtain a std::thread object that references it, so it can no longer be joined. Detached threads truly run in the background; ownership and control are passed over to the

Basic thread management

23

C++ Runtime Library, which ensures that the resources associated with the thread are correctly reclaimed when the thread exits.
Detached threads are often called daemon threads after the UNIX concept of a daemon process that runs in the background without any explicit user interface. Such threads are typically long-running; they run for almost the entire lifetime of the application, performing a background task such as monitoring the filesystem, clearing unused entries out of object caches, or optimizing data structures. At the other extreme, it may make sense to use a detached thread where there’s another mechanism for identifying when the thread has completed or where the thread is used for a fire-and-forget task.
As you’ve saw in section 2.1.2, you detach a thread by calling the detach() member function of the std::thread object. After the call completes, the std::thread object is no longer associated with the actual thread of execution and is therefore no longer joinable:
std::thread t(do_background_work); t.detach(); assert(!t.joinable());
In order to detach the thread from a std::thread object, there must be a thread to detach: you can’t call detach() on a std::thread object with no associated thread of execution. This is exactly the same requirement as for join(), and you can check it in exactly the same way—you can only call t.detach() for a std::thread object t when t.joinable() returns true.
Consider an application such as a word processor that can edit multiple documents at once. There are many ways to handle this, both at the UI level and internally. One way that’s increasingly common at the moment is to have multiple, independent, top-level windows, one for each document being edited. Although these windows appear to be completely independent, each with its own menus, they’re running within the same instance of the application. One way to handle this internally is to run each document-editing window in its own thread; each thread runs the same code but with different data relating to the document being edited and the corresponding window properties. Opening a new document therefore requires starting a new thread. The thread handling the request isn’t going to care about waiting for that other thread to finish, because it’s working on an unrelated document, so this makes it a prime candidate for running a detached thread.
The following listing shows a simple code outline for this approach.
Listing 2.4 Detaching a thread to handle other documents
void edit_document(std::string const& filename) {
open_document_and_display_gui(filename); while(!done_editing()) {
user_command cmd=get_user_input();

24

CHAPTER 2 Managing threads

if(cmd.type==open_new_document) {
std::string const new_name=get_filename_from_user(); std::thread t(edit_document,new_name); t.detach(); } else { process_user_input(cmd); } } }
If the user chooses to open a new document, you prompt them for the document to open, start a new thread to open that document, and then detach it. Because the new thread is doing the same operation as the current thread but on a different file, you can reuse the same function (edit_document) with the newly chosen filename as the supplied argument.
This example also shows a case where it’s helpful to pass arguments to the function used to start a thread: rather than just passing the name of the function to the std::thread constructor, you also pass in the filename parameter. Although other mechanisms could be used to do this, such as using a function object with member data instead of an ordinary function with parameters, the C++ Standard Library provides you with an easy way of doing it.
2.2 Passing arguments to a thread function
As shown in listing 2.4, passing arguments to the callable object or function is fundamentally as simple as passing additional arguments to the std::thread constructor. But it’s important to bear in mind that by default, the arguments are copied into internal storage, where they can be accessed by the newly created thread of execution, and then passed to the callable object or function as rvalues as if they were temporaries. This is done even if the corresponding parameter in the function is expecting a reference. Here’s an example:
void f(int i,std::string const& s); std::thread t(f,3,”hello”);
This creates a new thread of execution associated with t, which calls f(3,”hello”). Note that even though f takes a std::string as the second parameter, the string literal is passed as a char const* and converted to a std::string only in the context of the new thread. This is particularly important when the argument supplied is a pointer to an automatic variable, as follows:
void f(int i,std::string const& s); void oops(int some_param) {
char buffer[1024]; sprintf(buffer, "%i",some_param);

Passing arguments to a thread function

25

std::thread t(f,3,buffer); t.detach(); }

In this case, it’s the pointer to the local variable buffer that’s passed through to the new thread and there’s a significant chance that the oops function will exit before the buffer has been converted to a std::string on the new thread, thus leading to undefined behavior. The solution is to cast to std::string before passing the buffer to the std::thread constructor:

void f(int i,std::string const& s); void not_oops(int some_param) {
char buffer[1024]; sprintf(buffer,"%i",some_param); std::thread t(f,3,std::string(buffer)); t.detach(); }

Using std::string avoids dangling pointer

In this case, the problem is that you were relying on the implicit conversion of the pointer to the buffer into the std::string object expected as a function parameter, but this conversion happens too late because the std::thread constructor copies the supplied values as is, without converting to the expected argument type.
It’s not possible to get the reverse scenario: the object is copied, and you wanted a non-const reference, because this won't compile. You might try this if the thread is updating a data structure that’s passed in by reference; for example:
void update_data_for_widget(widget_id w,widget_data& data); void oops_again(widget_id w) {
widget_data data; std::thread t(update_data_for_widget,w,data); display_status(); t.join(); process_widget_data(data); }

Although update_data_for_widget expects the second parameter to be passed by reference, the std::thread constructor doesn’t know that; it’s oblivious to the types of the arguments expected by the function and blindly copies the supplied values. But the internal code passes copied arguments as rvalues in order to work with move-only types, and will thus try to call update_data_for_widget with an rvalue. This will fail to compile because you can't pass an rvalue to a function that expects a non-const reference. For those of you familiar with std::bind, the solution will be readily apparent: you need to wrap the arguments that need to be references in std::ref. In this case, if you change the thread invocation to
std::thread t(update_data_for_widget,w,std::ref(data));

26

CHAPTER 2 Managing threads

then update_data_for_widget will be correctly passed a reference to data rather than a temporary copy of data, and the code will now compile successfully.
If you’re familiar with std::bind, the parameter-passing semantics will be unsurprising, because both the operation of the std::thread constructor and the operation of std::bind are defined in terms of the same mechanism. This means that, for example, you can pass a member function pointer as the function, provided you supply a suitable object pointer as the first argument:
class X { public:
void do_lengthy_work(); }; X my_x; std::thread t(&X::do_lengthy_work,&my_x);
This code will invoke my_x.do_lengthy_work() on the new thread, because the address of my_x is supplied as the object pointer. You can also supply arguments to such a member function call: the third argument to the std::thread constructor will be the first argument to the member function, and so forth.
Another interesting scenario for supplying arguments is where the arguments can’t be copied but can only be moved: the data held within one object is transferred over to another, leaving the original object empty. An example of such a type is std::unique_ptr, which provides automatic memory management for dynamically allocated objects. Only one std::unique_ptr instance can point to a given object at a time, and when that instance is destroyed, the pointed-to object is deleted. The move constructor and move assignment operator allow the ownership of an object to be transferred around between std::unique_ptr instances (see appendix A, section A.1.1, for more on move semantics). Such a transfer leaves the source object with a NULL pointer. This moving of values allows objects of this type to be accepted as function parameters or returned from functions. Where the source object is temporary, the move is automatic, but where the source is a named value, the transfer must be requested directly by invoking std::move(). The following example shows the use of std::move to transfer ownership of a dynamic object into a thread:
void process_big_object(std::unique_ptr<big_object>); std::unique_ptr<big_object> p(new big_object); p->prepare_data(42); std::thread t(process_big_object,std::move(p));
By specifying std::move(p) in the std::thread constructor, the ownership of big_ object is transferred first into internal storage for the newly created thread and then into process_big_object.
Several of the classes in the C++ Standard Library exhibit the same ownership semantics as std::unique_ptr, and std::thread is one of them. Though std::thread instances don’t own a dynamic object in the same way as std::unique_ptr does, they do

Transferring ownership of a thread

27

own a resource: each instance is responsible for managing a thread of execution. This ownership can be transferred between instances, because instances of std::thread are movable, even though they aren’t copyable. This ensures that only one object is associated with a particular thread of execution at any one time while allowing programmers the option of transferring that ownership between objects.

2.3 Transferring ownership of a thread
Suppose you want to write a function that creates a thread to run in the background, but passes ownership of the new thread back to the calling function rather than waiting for it to complete; or maybe you want to do the reverse: create a thread and pass ownership in to some function that should wait for it to complete. In either case, you need to transfer ownership from one place to another.
This is where the move support of std::thread comes in. As described in the previous section, many resource-owning types in the C++ Standard Library, such as std::ifstream and std::unique_ptr, are movable but not copyable, and std::thread is one of them. This means that the ownership of a particular thread of execution can be moved between std::thread instances, as in the following example. The example shows the creation of two threads of execution and the transfer of ownership of those threads among three std::thread instances, t1, t2, and t3:

void some_function(); void some_other_function(); std::thread t1(some_function); std::thread t2=std::move(t1); t1=std::thread(some_other_function); std::thread t3; t3=std::move(t2); t1=std::move(t3);

This assignment will terminate the program!

First, a new thread is started and associated with t1. Ownership is then transferred over to t2 when t2 is constructed, by invoking std::move() to explicitly move ownership. At this point, t1 no longer has an associated thread of execution; the thread running some_function is now associated with t2.
Then, a new thread is started and associated with a temporary std::thread object. The subsequent transfer of ownership into t1 doesn’t require a call to std::move() to explicitly move ownership, because the owner is a temporary object—moving from temporaries is automatic and implicit.
t3 is default-constructed, which means that it’s created without any associated thread of execution. Ownership of the thread currently associated with t2 is transferred into t3, again with an explicit call to std::move(), because t2 is a named object. After all these moves, t1 is associated with the thread running some_other_function, t2 has no associated thread, and t3 is associated with the thread running some_function.
The final move transfers ownership of the thread running some_function back to t1 where it started. But in this case t1 already had an associated thread (which was running some_other_function), so std::terminate() is called to terminate the program.

28

CHAPTER 2 Managing threads

This is done for consistency with the std::thread destructor. You saw in section 2.1.1 that you must explicitly wait for a thread to complete or detach it before destruction, and the same applies to assignment: you can’t just drop a thread by assigning a new value to the std::thread object that manages it.
The move support in std::thread means that ownership can readily be transferred out of a function, as shown in the following listing.
Listing 2.5 Returning a std::thread from a function
std::thread f() {
void some_function(); return std::thread(some_function); } std::thread g() { void some_other_function(int); std::thread t(some_other_function,42); return t; }
Likewise, if ownership should be transferred into a function, it can accept an instance of std::thread by value as one of the parameters, as shown here:
void f(std::thread t); void g() {
void some_function(); f(std::thread(some_function)); std::thread t(some_function); f(std::move(t)); }
One benefit of the move support of std::thread is that you can build on the thread_guard class from listing 2.3 and have it take ownership of the thread. This avoids any unpleasant consequences should the thread_guard object outlive the thread it was referencing, and it also means that no one else can join or detach the thread once ownership has been transferred into the object. Because this would primarily be aimed at ensuring that threads are completed before a scope is exited, I named this class scoped_thread. The implementation is shown in the following listing, along with a simple example.
Listing 2.6 scoped_thread and example usage
class scoped_thread {
std::thread t; public:
explicit scoped_thread(std::thread t_): t(std::move(t_))

Transferring ownership of a thread

29

{

if(!t.joinable())

throw std::logic_error(“No thread”);

}

~scoped_thread()

{

t.join();

}

scoped_thread(scoped_thread const&)=delete;

scoped_thread& operator=(scoped_thread const&)=delete;

};

struct func;

See listing 2.1

void f()

{

int some_local_state;

scoped_thread t{std::thread(func(some_local_state))};

do_something_in_current_thread();

}

The example is similar to listing 2.3, but the new thread is passed in directly to scoped_thread rather than having to create a separate named variable for it. When the initial thread reaches the end of f, the scoped_thread object is destroyed and then joins with the thread supplied to the constructor. Whereas with the thread_guard class from listing 2.3 the destructor had to check that the thread was still joinable, you can do that in the constructor and throw an exception if it’s not.
One of the proposals for C++17 was for a joining_thread class that would be similar to std::thread, except that it would automatically join in the destructor much like scoped_thread does. This didn't get consensus in the committee, so it wasn’t accepted into the standard (though it’s still on track for C++20 as std::jthread), but it’s relatively easy to write. One possible implementation is shown in the next listing.

Listing 2.7 A joining_thread class

class joining_thread {
std::thread t; public:
joining_thread() noexcept=default; template<typename Callable,typename ... Args> explicit joining_thread(Callable&& func,Args&& ... args):
t(std::forward<Callable>(func),std::forward<Args>(args)...) {} explicit joining_thread(std::thread t_) noexcept:
t(std::move(t_)) {} joining_thread(joining_thread&& other) noexcept:
t(std::move(other.t)) {} joining_thread& operator=(joining_thread&& other) noexcept {
if(joinable()) join();

30

CHAPTER 2 Managing threads

t=std::move(other.t); return *this; } joining_thread& operator=(std::thread other) noexcept { if(joinable())
join(); t=std::move(other); return *this; } ~joining_thread() noexcept { if(joinable())
join(); } void swap(joining_thread& other) noexcept {
t.swap(other.t); } std::thread::id get_id() const noexcept{
return t.get_id(); } bool joinable() const noexcept {
return t.joinable(); } void join() {
t.join(); } void detach() {
t.detach(); } std::thread& as_thread() noexcept {
return t; } const std::thread& as_thread() const noexcept {
return t; } };
The move support in std::thread also allows for containers of std::thread objects, if those containers are move-aware (like the updated std::vector<>). This means that you can write code like that in the following listing, which spawns a number of threads and then waits for them to finish.
Listing 2.8 Spawns some threads and waits for them to finish
void do_work(unsigned id); void f()

Choosing the number of threads at runtime

31

{

std::vector<std::thread> threads;

for(unsigned i=0;i<20;++i)

{

threads.emplace_back(do_work,i);

Spawns threads

}

for(auto& entry: threads) entry.join();
}

Calls join() on each thread in turn

If the threads are being used to subdivide the work of an algorithm, this is often what’s required; before returning to the caller, all threads must have finished. The simple structure of listing 2.8 implies that the work done by the threads is self-contained, and the result of their operations is purely the side effects on shared data. If f() were to return a value to the caller that depended on the results of the operations performed by these threads, then as written, this return value would have to be determined by examining the shared data after the threads had terminated. Alternative schemes for transferring the results of operations between threads are discussed in chapter 4.
Putting std::thread objects in a std::vector is a step toward automating the management of those threads: rather than creating separate variables for those threads and joining with them directly, they can be treated as a group. You can take this a step further by creating a dynamic number of threads determined at runtime, rather than creating a fixed number, as in listing 2.8.

2.4 Choosing the number of threads at runtime
One feature of the C++ Standard Library that helps here is std::thread::hardware_ concurrency(). This function returns an indication of the number of threads that can truly run concurrently for a given execution of a program. On a multicore system it might be the number of CPU cores, for example. This is only a hint, and the function might return 0 if this information isn’t available, but it can be a useful guide for splitting a task among threads.
Listing 2.9 shows a simple implementation of a parallel version of std::accumulate. In real code you'll probably want to use the parallel version of std::reduce described in chapter 10, rather than implementing it yourself, but this illustrates the basic idea. It divides the work among the threads, with a minimum number of elements per thread in order to avoid the overhead of too many threads. Note that this implementation assumes that none of the operations will throw an exception, even though exceptions are possible; the std::thread constructor will throw if it can’t start a new thread of execution, for example. Handling exceptions in such an algorithm is beyond the scope of this simple example and will be covered in chapter 8.

Listing 2.9 A naïve parallel version of std::accumulate
template<typename Iterator,typename T> struct accumulate_block

32

CHAPTER 2 Managing threads

{ void operator()(Iterator first,Iterator last,T& result) { result=std::accumulate(first,last,result); }
}; template<typename Iterator,typename T> T parallel_accumulate(Iterator first,Iterator last,T init) {
unsigned long const length=std::distance(first,last); if(!length)
return init; unsigned long const min_per_thread=25; unsigned long const max_threads=
(length+min_per_thread-1)/min_per_thread; unsigned long const hardware_threads=
std::thread::hardware_concurrency(); unsigned long const num_threads=
std::min(hardware_threads!=0?hardware_threads:2,max_threads); unsigned long const block_size=length/num_threads; std::vector<T> results(num_threads); std::vector<std::thread> threads(num_threads-1); Iterator block_start=first; for(unsigned long i=0;i<(num_threads-1);++i) {
Iterator block_end=block_start; std::advance(block_end,block_size); threads[i]=std::thread(
accumulate_block<Iterator,T>(), block_start,block_end,std::ref(results[i])); block_start=block_end; } accumulate_block<Iterator,T>()( block_start,last,results[num_threads-1]);
for(auto& entry: threads) entry.join();
return std::accumulate(results.begin(),results.end(),init); }

Although this is a long function, it’s straightforward. If the input range is empty, you return the initial value supplied as the init parameter value. Otherwise, there’s at least one element in the range, so you can divide the number of elements to process by the minimum block size in order to give the maximum number of threads . This is to avoid creating 32 threads on a 32-core machine when you have only five values in the range.
The number of threads to run is the minimum of your calculated maximum and the number of hardware threads. You don’t want to run more threads than the hardware can support (which is called oversubscription), because the context switching will mean that more threads will decrease the performance. If the call to std::thread:: hardware_concurrency() returned 0, you’d substitute a number of your choice; in

Choosing the number of threads at runtime

33

this case I’ve chosen 2. You don’t want to run too many threads because that would slow things down on a single-core machine, but likewise you don’t want to run too few because you’d be passing up the available concurrency.
The number of entries for each thread to process is the length of the range divided by the number of threads. If you’re worrying about cases where the number doesn’t divide evenly, don’t—you’ll handle that later.
Now that you know how many threads you have, you can create a std::vector<T> for the intermediate results and a std::vector<std::thread> for the threads. Note that you need to launch one fewer thread than num_threads, because you already have one.
Launching the threads is a simple loop: advance the block_end iterator to the end of the current block and launch a new thread to accumulate the results for this block. The start of the next block is the end of this one.
After you’ve launched all the threads, this thread can then process the final block. This is where you take account of any uneven division: you know the end of the final block must be last, and it doesn’t matter how many elements are in that block.
Once you’ve accumulated the results for the last block, you can wait for all the threads you spawned with std::for_each, as in listing 2.8, and then add up the results with a final call to std::accumulate.
Before you leave this example, it’s worth pointing out that where the addition operator for the type T isn’t associative (such as for float or double), the results of this parallel_accumulate may vary from those of std::accumulate because of the grouping of the range into blocks. Also, the requirements on the iterators are slightly more stringent: they must be at least forward iterators, whereas std::accumulate can work with single-pass input iterators, and T must be default-constructible so that you can create the results vector. These sorts of requirement changes are common with parallel algorithms; by their nature they’re different in order to make them parallel, and this has consequences for the results and requirements. Implementing parallel algorithms is covered in more depth in chapter 8, and chapter 10 covers the standard supplied ones from C++17 (the equivalent to the parallel_accumulate described here being the parallel form of std::reduce). It’s also worth noting that because you can’t return a value directly from a thread, you must pass in a reference to the relevant entry in the results vector. Alternative ways of returning results from threads are addressed through the use of futures in chapter 4.
In this case, all the information required by each thread was passed in when the thread was started, including the location in which to store the result of its calculation. This isn’t always the case; sometimes it’s necessary to be able to identify the threads in some way for part of the processing. You could pass in an identifying number, such as the value of i in listing 2.8, but if the function that needs the identifier is several levels deep in the call stack and could be called from any thread, it’s inconvenient to have to do it that way. When we were designing the C++ Standard Library we foresaw this need, so each thread has a unique identifier.

34

CHAPTER 2 Managing threads

2.5 Identifying threads
Thread identifiers are of type std::thread::id and can be retrieved in two ways. First, the identifier for a thread can be obtained from its associated std::thread object by calling the get_id() member function. If the std::thread object doesn’t have an associated thread of execution, the call to get_id() returns a default-constructed std::thread::id object, which indicates “not any thread.” Alternatively, the identifier for the current thread can be obtained by calling std::this_thread:: get_id(), which is also defined in the <thread> header.
Objects of type std::thread::id can be freely copied and compared; they wouldn’t be of much use as identifiers otherwise. If two objects of type std::thread::id are equal, they represent the same thread, or both are holding the “not any thread” value. If two objects aren’t equal, they represent different threads, or one represents a thread and the other is holding the “not any thread” value.
The C++ Standard Library doesn’t limit you to checking whether thread identifiers are the same or not; objects of type std::thread::id offer the complete set of comparison operators, which provide a total ordering for all distinct values. This allows them to be used as keys in associative containers, or sorted, or compared in any other way that you as a programmer may see fit. The comparison operators provide a total order for all non-equal values of std::thread::id, so they behave as you’d intuitively expect: if a<b and b<c, then a<c, and so forth. The Standard Library also provides std::hash<std::thread::id> so that values of type std::thread::id can be used as keys in the new unordered associative containers too.
Instances of std::thread::id are often used to check whether a thread needs to perform some operation. For example, if threads are used to divide work, as in listing 2.9, the initial thread that launched the others might need to perform its work slightly differently in the middle of the algorithm. In this case it could store the result of std::this_thread::get_id() before launching the other threads, and then the core part of the algorithm (which is common to all threads) could check its own thread ID against the stored value:
std::thread::id master_thread; void some_core_part_of_algorithm() {
if(std::this_thread::get_id()==master_thread) {
do_master_thread_work(); } do_common_work(); }

Alternatively, the std::thread::id of the current thread could be stored in a data structure as part of an operation. Later operations on that same data structure could then check the stored ID against the ID of the thread performing the operation to determine what operations are permitted/required.

Summary

35

Similarly, thread IDs could be used as keys into associative containers where specific data needs to be associated with a thread and alternative mechanisms such as thread-local storage aren’t appropriate. Such a container could, for example, be used by a controlling thread to store information about each of the threads under its control or for passing information between threads.
The idea is that std::thread::id will suffice as a generic identifier for a thread in most circumstances; it’s only if the identifier has semantic meaning associated with it (such as being an index into an array) that alternatives should be necessary. You can even write out an instance of std::thread::id to an output stream such as std::cout:
std::cout<<std::this_thread::get_id();
The exact output you get is strictly implementation-dependent; the only guarantee given by the standard is that thread IDs that compare as equal should produce the same output, and those that aren’t equal should give different output. This is therefore primarily useful for debugging and logging, but the values have no semantic meaning, so there’s not much more that could be said anyway.
Summary
In this chapter, I covered the basics of thread management with the C++ Standard Library: starting threads, waiting for them to finish, and not waiting for them to finish because you want them to run in the background. We also saw how to pass arguments into the thread function when a thread is started, how to transfer the responsibility for managing a thread from one part of the code to another, and how groups of threads can be used to divide work. Finally, we discussed identifying threads in order to associate data or behavior with specific threads that’s inconvenient to associate through alternative means. Although you can do quite a lot with purely independent threads that each operate on separate data, sometimes it’s desirable to share data among threads while they’re running. Chapter 3 discusses the issues surrounding sharing data directly among threads, and chapter 4 covers more general issues surrounding synchronizing operations with and without shared data.

Sharing data between threads
This chapter covers
 Problems with sharing data between threads  Protecting data with mutexes  Alternative facilities for protecting shared data
One of the key benefits of using threads for concurrency is the potential to easily and directly share data between them, so now that we’ve covered starting and managing threads, let’s look at the issues surrounding shared data.
Imagine for a moment that you’re sharing an apartment with a friend. There’s only one kitchen and one bathroom. Unless you’re particularly friendly, you can’t both use the bathroom at the same time, and if your roommate occupies the bathroom for a long time, it can be frustrating if you need to use it. Likewise, though it might be possible to both cook meals at the same time, if you have a combined oven and grill, it’s not going to end well if one of you tries to grill some sausages at the same time as the other is baking a cake. Furthermore, we all know the frustration of sharing a space and getting halfway through a task only to find that someone has borrowed something we need or changed something from the way we left it.
It’s the same with threads. If you’re sharing data between threads, you need to have rules for which thread can access which bit of data when, and how any updates
36

Problems with sharing data between threads

37

are communicated to the other threads that care about that data. The ease with which data can be shared between multiple threads in a single process is not only a benefit— it can also be a big drawback. Incorrect use of shared data is one of the biggest causes of concurrency-related bugs, and the consequences can be far worse than sausageflavored cakes.
This chapter is about sharing data safely between threads in C++, avoiding the potential problems that can arise, and maximizing the benefits.
3.1 Problems with sharing data between threads
When it comes down to it, the problems with sharing data between threads are all due to the consequences of modifying data. If all shared data is read-only, there’s no problem, because the data read by one thread is unaffected by whether or not another thread is reading the same data. But if data is shared between threads, and one or more threads start modifying the data, there’s a lot of potential for trouble. In this case, you must take care to ensure that everything works out OK.
One concept that’s widely used to help programmers reason about their code is invariants—statements that are always true about a particular data structure, such as “this variable contains the number of items in the list.” These invariants are often broken during an update, especially if the data structure is of any complexity or the update requires modification of more than one value.
Consider a doubly linked list, where each node holds a pointer to both the next node in the list and the previous one. One of the invariants is that if you follow a “next” pointer from one node (A) to another (B), the “previous” pointer from that node (B) points back to the first node (A). In order to remove a node from the list, the nodes on either side have to be updated to point to each other. Once one has been updated, the invariant is broken until the node on the other side has been updated too; after the update has completed, the invariant holds again.
The steps in deleting an entry from such a list are shown in figure 3.1:
a Identify the node to delete: N. b Update the link from the node prior to N to point to the node after N. c Update the link from the node after N to point to the node prior to N. d Delete node N.
As you can see in figure 3.1, between steps b and c, the links going in one direction are inconsistent with the links going in the opposite direction, and the invariant is broken.
The simplest potential problem with modifying data that’s shared between threads is that of broken invariants. If you don’t do anything special to ensure otherwise, if one thread is reading the doubly linked list while another is removing a node, it’s quite possible for the reading thread to see the list with a node only partially removed (because only one of the links has been changed, as in step b of figure 3.1), so the invariant is broken. The consequences of this broken invariant can vary; if the other

38

CHAPTER 3 Sharing data between threads

thread is reading the list items from left to right in the diagram, it will skip the node being deleted. On the other hand, if the second thread is trying to delete the rightmost node in the diagram, it might end up permanently corrupting the data structure and eventually crashing the program. Whatever the outcome, this is an example of one of the most common causes of bugs in concurrent code: a race condition.

Figure 3.1 Deleting a node from a doubly linked list

3.1.1

Race conditions
Suppose you’re buying tickets to see a movie at the movie theater. If it’s a big theater, multiple cashiers will be taking money so more than one person can buy tickets at the same time. If someone at another cashier’s desk is also buying tickets for the same

Problems with sharing data between threads

39

movie as you are, which seats are available for you to choose from depends on whether the other person books first or you do. If there are only a few seats left, this difference can be quite crucial: it might literally be a race to see who gets the last tickets. This is an example of a race condition: which seats you get (or even whether you get tickets) depends on the relative ordering of the two purchases.
In concurrency, a race condition is anything where the outcome depends on the relative ordering of execution of operations on two or more threads; the threads race to perform their respective operations. Most of the time, this is quite benign because all possible outcomes are acceptable, even though they may change with different relative orderings. For example, if two threads are adding items to a queue for processing, it generally doesn’t matter which item gets added first, provided that the invariants of the system are maintained. It’s when the race condition leads to broken invariants that there’s a problem, such as with the doubly linked list example mentioned. When talking about concurrency, the term race condition is usually used to mean a problematic race condition; benign race conditions aren’t so interesting and aren’t a cause of bugs. The C++ Standard also defines the term data race to mean the specific type of race condition that arises because of concurrent modification to a single object (see section 5.1.2 for details); data races cause the dreaded undefined behavior.
Problematic race conditions typically occur where completing an operation requires modification of two or more distinct pieces of data, such as the two link pointers in the example. Because the operation must access two separate pieces of data, these must be modified in separate instructions, and another thread could potentially access the data structure when only one of them has been completed. Race conditions can often be hard to find and hard to duplicate because the window of opportunity is small. If the modifications are done as consecutive CPU instructions, the chance of the problem exhibiting on any one run-through is small, even if the data structure is being accessed by another thread concurrently. As the load on the system increases, and the number of times the operation is performed increases, the chance of the problematic execution sequence occurring also increases. It’s almost inevitable that such problems will show up at the most inconvenient time. Because race conditions are generally timing-sensitive, they can often disappear entirely when the application is run under the debugger, because the debugger affects the timing of the program, even if only slightly.
If you’re writing multithreaded programs, race conditions can easily be the bane of your existence; a great deal of the complexity in writing software that uses concurrency comes from avoiding problematic race conditions.

3.1.2

Avoiding problematic race conditions
There are several ways to deal with problematic race conditions. The simplest option is to wrap your data structure with a protection mechanism to ensure that only the thread performing a modification can see the intermediate states where the invariants are broken. From the point of view of other threads accessing that data structure, such

40

CHAPTER 3 Sharing data between threads

modifications either haven’t started or have completed. The C++ Standard Library provides several of these mechanisms, which are described in this chapter.
Another option is to modify the design of your data structure and its invariants so that modifications are done as a series of indivisible changes, each of which preserves the invariants. This is generally referred to as lock-free programming and is difficult to get right. If you’re working at this level, the nuances of the memory model and identifying which threads can potentially see which set of values can get complicated. The memory model is covered in chapter 5, and lock-free programming is discussed in chapter 7.
Another way of dealing with race conditions is to handle the updates to the data structure as a transaction, just as updates to a database are done within a transaction. The required series of data modifications and reads is stored in a transaction log and then committed in a single step. If the commit can’t proceed because the data structure has been modified by another thread, the transaction is restarted. This is termed software transactional memory (STM), and it’s an active research area at the time of writing. It won’t be covered in this book, because there’s no direct support for STM in C++ (though there is a Technical Specification for Transactional Memory Extensions to C++1). But the basic idea of doing something privately and then committing in a single step is something that I’ll come back to later.
The most basic mechanism for protecting shared data provided by the C++ Standard is the mutex, so we’ll look at that first.
3.2 Protecting shared data with mutexes
So, you have a shared data structure such as the linked list from the previous section, and you want to protect it from race conditions and the potential broken invariants that can ensue. Wouldn’t it be nice if you could mark all the pieces of code that access the data structure as mutually exclusive, so that if any thread was running one of them, any other thread that tried to access that data structure had to wait until the first thread was finished? That would make it impossible for a thread to see a broken invariant except when it was the thread doing the modification.
Well, this isn’t a fairy tale wish—it’s precisely what you get if you use a synchronization primitive called a mutex (mutual exclusion). Before accessing a shared data structure, you lock the mutex associated with that data, and when you’ve finished accessing the data structure, you unlock the mutex. The Thread Library then ensures that once one thread has locked a specific mutex, all other threads that try to lock the same mutex have to wait until the thread that successfully locked the mutex unlocks it. This ensures that all threads see a self-consistent view of the shared data, without any broken invariants.
Mutexes are the most general of the data-protection mechanisms available in C++, but they’re not a silver bullet; it’s important to structure your code to protect the right
1 ISO/IEC TS 19841:2015—Technical Specification for C++ Extensions for Transactional Memory http://www .iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=66343.

Protecting shared data with mutexes

41

data (see section 3.2.2) and avoid race conditions inherent in your interfaces (see section 3.2.3). Mutexes also come with their own problems in the form of a deadlock (see section 3.2.4) and protecting either too much or too little data (see section 3.2.8). Let’s start with the basics.

3.2.1

Using mutexes in C++
In C++, you create a mutex by constructing an instance of std::mutex, lock it with a call to the lock() member function, and unlock it with a call to the unlock() member function. But it isn’t recommended practice to call the member functions directly, because this means that you have to remember to call unlock() on every code path out of a function, including those due to exceptions. Instead, the Standard C++ Library provides the std::lock_guard class template, which implements that RAII idiom for a mutex; it locks the supplied mutex on construction and unlocks it on destruction, ensuring a locked mutex is always correctly unlocked. The following listing shows how to protect a list that can be accessed by multiple threads using std::mutex, along with std::lock_guard. Both of these are declared in the <mutex> header.

Listing 3.1 Protecting a list with a mutex

#include <list>

#include <mutex>

#include <algorithm>

B

std::list<int> some_list;

std::mutex some_mutex;

c

void add_to_list(int new_value)

{

std::lock_guard<std::mutex> guard(some_mutex);

d

some_list.push_back(new_value);

}

bool list_contains(int value_to_find)

{

e

std::lock_guard<std::mutex> guard(some_mutex);

return std::find(some_list.begin(),some_list.end(),value_to_find)

!= some_list.end();

}

In listing 3.1, there’s a single global variable B, and it’s protected with a corresponding global instance of std::mutex c. The use of std::lock_guard<std::mutex> in add_to_list() d, and again in list_contains() e, means that the accesses in
these functions are mutually exclusive: list_contains() will never see the list partway through a modification by add_to_list().
C++17 has a new feature called class template argument deduction, which means that for simple class templates like std::lock_guard, the template argument list can
often be omitted. d and e can be reduced to

std::lock_guard guard(some_mutex);

42

CHAPTER 3 Sharing data between threads

on a C++17 compiler. As we will see in section 3.2.4, C++17 also introduces an enhanced version of lock guard called std::scoped_lock, so in a C++17 environment, this may well be written as
std::scoped_lock guard(some_mutex);

For clarity of code and compatibility with older compilers, I’ll continue to use std::lock_guard and specify the template arguments in other code snippets.
Although there are occasions where this use of global variables is appropriate, in the majority of cases it’s common to group the mutex and the protected data together in a class rather than use global variables. This is a standard application of objectoriented design rules: by putting them in a class, you’re clearly marking them as related, and you can encapsulate the functionality and enforce the protection. In this case, the add_to_list and list_contains functions would become member functions of the class, and the mutex and protected data would both become private members of the class, making it much easier to identify which code has access to the data and thus which code needs to lock the mutex. If all the member functions of the class lock the mutex before accessing any other data members and unlock it when done, the data is nicely protected from all comers.
Well, that’s not quite true, as the astute among you will have noticed: if one of the member functions returns a pointer or reference to the protected data, then it doesn’t matter that the member functions all lock the mutex in a nice, orderly fashion, because you’ve blown a big hole in the protection. Any code that has access to that pointer or reference can now access (and potentially modify) the protected data without locking the mutex. Protecting data with a mutex therefore requires careful interface design to ensure that the mutex is locked before there’s any access to the protected data and that there are no backdoors.

3.2.2

Structuring code for protecting shared data
As you’ve seen, protecting data with a mutex is not quite as easy as slapping an std::lock_guard object in every member function; one stray pointer or reference, and all that protection is for nothing. At one level, checking for stray pointers or references is easy; as long as none of the member functions return a pointer or reference to the protected data to their caller either via their return value or via an out parameter, the data is safe. If you dig a little deeper, it’s not that straightforward—nothing ever is. As well as checking that the member functions don’t pass out pointers or references to their callers, it’s also important to check that they don’t pass these pointers or references in to functions they call that aren’t under your control. This is just as dangerous: those functions might store the pointer or reference in a place where it can later be used without the protection of the mutex. Particularly dangerous in this regard are functions that are supplied at runtime via a function argument or other means, as in the next listing.

Protecting shared data with mutexes

43

Listing 3.2 Accidentally passing out a reference to protected data

class some_data

{

int a;

std::string b;

public:

void do_something();

};

class data_wrapper

{

private:

some_data data;

std::mutex m;

public:

template<typename Function>

void process_data(Function func)

{

std::lock_guard<std::mutex> l(m);

} };

func(data);

Pass “protected” data to
B user-supplied function

some_data* unprotected;

void malicious_function(some_data& protected_data)

{

unprotected=&protected_data;

}

data_wrapper x;

void foo() {
x.process_data(malicious_function);

c Pass in a malicious
function

unprotected->do_something(); }

Unprotected access
d to protected data

In this example, the code in process_data looks harmless enough, nicely protected
with std::lock_guard, but the call to the user-supplied func function B means that foo can pass in malicious_function to bypass the protection c and then call do_something() without the mutex being locked d.
Fundamentally, the problem with this code is that it hasn’t done what you set out to do: mark all the pieces of code that access the data structure as mutually exclusive. In this case, it missed the code in foo() that calls unprotected->do_something(). Unfortunately, this part of the problem isn’t something the C++ Thread Library can help you with; it’s up to you as programmers to lock the right mutex to protect your data. On the upside, you have a guideline to follow, which will help you in these cases: Don’t pass pointers and references to protected data outside the scope of the lock, whether by returning them from a function, storing them in externally visible memory, or passing them as arguments to user-supplied functions.
Although this is a common mistake when trying to use mutexes to protect shared data, it’s far from the only potential pitfall. As you’ll see in the next section, it’s still possible to have race conditions, even when data is protected with a mutex.

44

CHAPTER 3 Sharing data between threads

3.2.3

Spotting race conditions inherent in interfaces
Just because you’re using a mutex or other mechanism to protect shared data, it doesn’t mean you’re protected from race conditions; you still have to ensure that the appropriate data is protected. Consider the doubly linked list example again. In order for a thread to safely delete a node, you need to ensure that you’re preventing concurrent accesses to three nodes: the node being deleted and the nodes on either side. If you protected access to the pointers of each node individually, you’d be no better off than with code that used no mutexes, because the race condition could still happen— it’s not the individual nodes that need protecting for the individual steps but the whole data structure, for the whole delete operation. The easiest solution in this case is to have a single mutex that protects the entire list, as in listing 3.1.
Just because individual operations on the list are safe, you’re not out of the woods yet; you can still get race conditions, even with a simple interface. Consider a stack data structure like the std::stack container adapter shown in listing 3.3. Aside from the constructors and swap(), there are only five things you can do to a std::stack: you can push() a new element onto the stack, pop() an element off the stack, read the top() element, check whether it’s empty(), and read the number of elements—the size() of the stack. If you change top() so that it returns a copy rather than a reference (so you’re following the guideline from section 3.2.2) and protect the internal data with a mutex, this interface is still inherently subject to race conditions. This problem is not unique to a mutex-based implementation; it’s an interface problem, so the race conditions would still occur with a lock-free implementation.

Listing 3.3 The interface to the std::stack container adapter

template<typename T,typename Container=std::deque<T> > class stack { public:
explicit stack(const Container&); explicit stack(Container&& = Container()); template <class Alloc> explicit stack(const Alloc&); template <class Alloc> stack(const Container&, const Alloc&); template <class Alloc> stack(Container&&, const Alloc&); template <class Alloc> stack(stack&&, const Alloc&); bool empty() const; size_t size() const; T& top(); T const& top() const; void push(T const&); void push(T&&); void pop(); void swap(stack&&); template <class... Args> void emplace(Args&&... args); };

New in C++14

The problem here is that the results of empty() and size() can’t be relied on. Although they might be correct at the time of the call, once they’ve returned, other threads are

Protecting shared data with mutexes

45

free to access the stack and might push() new elements onto or pop() the existing ones off of the stack before the thread that called empty() or size() could use that information.
In particular, if the stack instance is not shared, it’s safe to check for empty() and then call top() to access the top element if the stack is not empty, as follows:

stack<int> s;

if(!s.empty())

b

{

int const value=s.top();

c

s.pop();

do_something(value);

d

}

Not only is it safe in single-threaded code, it’s expected: calling top() on an empty stack is undefined behavior. With a shared stack object, this call sequence is no longer safe, because there might be a call to pop() from another thread that removes the last
element in between the call to empty() B and the call to top() c. This is therefore a
classic race condition, and the use of a mutex internally to protect the stack contents doesn’t prevent it; it’s a consequence of the interface.
What’s the solution? Well, this problem happens as a consequence of the design of the interface, so the solution is to change the interface. But that still begs the question: what changes need to be made? In the simplest case, you could declare that top() will throw an exception if there aren’t any elements in the stack when it’s called. Though this directly addresses this issue, it makes for more cumbersome programming, because now you need to be able to catch an exception, even if the call to empty() returned false. This makes the call to empty() an optimization to avoid the overhead of throwing an exception if the stack is already empty (though if the state changes between the call to empty() and the call to top(), then the exception will still be thrown), rather than a necessary part of the design.
If you look closely at the previous snippet, there’s also potential for another race
condition, but this time between the call to top() c and the call to pop() d. Con-
sider two threads running the previous snippet of code and both referencing the same stack object, s. This isn’t an unusual situation; when using threads for performance, it’s quite common to have several threads running the same code on different data, and a shared stack object is ideal for dividing work between them (though more commonly, a queue is used for this purpose—see the examples in chapters 6 and 7). Suppose that initially the stack has two elements, so you don’t have to worry about the race between empty() and top() on either thread, and consider the potential execution patterns.
If the stack is protected by a mutex internally, only one thread can be running a stack member function at any one time, so the calls get nicely interleaved, but the calls to do_something() can run concurrently. One possible execution is shown in table 3.1.

46

CHAPTER 3 Sharing data between threads

Table 3.1 A possible ordering of operations on a stack from two threads

Thread A

Thread B

if(!s.empty())
int const value=s.top();
s.pop(); do_something(value);

if(!s.empty())
int const value=s.top();
s.pop(); do_something(value);

As you can see, if these are the only threads running, there’s nothing in between the two calls to top() to modify the stack, so both threads will see the same value. Not only that, but there are no calls to top() between the calls to pop(). Consequently, one of the two values on the stack is discarded without ever having been read, whereas the other is processed twice. This is yet another race condition and far more insidious than the undefined behavior of the empty()/top() race; there’s never anything obviously wrong going on, and the consequences of the bug are likely far removed from the cause, although they obviously depend on exactly what do_something() does.
This calls for a more radical change to the interface, one that combines the calls to top() and pop() under the protection of the mutex. Tom Cargill2 pointed out that a combined call can lead to issues if the copy constructor for the objects on the stack can throw an exception. This problem was dealt with fairly comprehensively from an exception-safety point of view by Herb Sutter,3 but the potential for race conditions brings something new to the mix.
For those of you who aren’t aware of the issue, consider stack<vector<int>>. Now, a vector is a dynamically sized container, so when you copy a vector, the library has to allocate some more memory from the heap in order to copy the contents. If the system is heavily loaded or there are significant resource constraints, this memory allocation can fail, so the copy constructor for vector might throw a std::bad_alloc exception. This is likely if the vector contains a lot of elements. If the pop() function was defined to return the value popped, as well as remove it from the stack, you have a potential problem: the value being popped is returned to the caller only after the stack has been modified, but the process of copying the data to return to the caller might throw an exception. If this happens, the data popped is lost; it has been removed from

2 Tom Cargill, “Exception Handling: A False Sense of Security,” in C++ Report 6, no. 9 (November–December 1994). Also available at http://www.informit.com/content/images/020163371x/supplements/Exception_ Handling_Article.html.
3 Herb Sutter, Exceptional C++: 47 Engineering Puzzles, Programming Problems, and Solutions (Addison Wesley Professional, 1999).

Protecting shared data with mutexes

47

the stack, but the copy was unsuccessful! The designers of the std::stack interface helpfully split the operation in two: get the top element (top()) and then remove it from the stack (pop()), so that if you can’t safely copy the data, it stays on the stack. If the problem was lack of heap memory, maybe the application can free some memory and try again.
Unfortunately, it’s precisely this split that you’re trying to avoid in eliminating the race condition! Thankfully, there are alternatives, but they aren’t without cost.
OPTION 1: PASS IN A REFERENCE The first option is to pass a reference to a variable in which you want to receive the popped value as an argument in the call to pop():
std::vector<int> result; some_stack.pop(result);

This works well for many cases, but it has the distinct disadvantage that it requires the calling code to construct an instance of the stack’s value type prior to the call, in order to pass this in as the target. For some types this is impractical, because constructing an instance is expensive in terms of time or resources. For other types this isn’t always possible, because the constructors require parameters that aren’t necessarily available at this point in the code. Finally, it requires that the stored type be assignable. This is an important restriction: many user-defined types do not support assignment, though they may support move construction or even copy construction (and allow return by value).
OPTION 2: REQUIRE A NO-THROW COPY CONSTRUCTOR OR MOVE CONSTRUCTOR There’s only an exception safety problem with a value-returning pop() if the return by value can throw an exception. Many types have copy constructors that don’t throw exceptions, and with the new rvalue-reference support in the C++ Standard (see appendix A, section A.1), many more types will have a move constructor that doesn’t throw exceptions, even if their copy constructor does. One valid option is to restrict the use of your thread-safe stack to those types that can safely be returned by value without throwing an exception.
Although this is safe, it’s not ideal. Even though you can detect at compile time the existence of a copy or move constructor that doesn’t throw an exception using the std::is_nothrow_copy_constructible and std::is_nothrow_move_constructible type traits, it’s quite limiting. There are many more user-defined types with copy constructors that can throw and don’t have move constructors than there are types with copy and/or move constructors that can’t throw (although this might change as people get used to the rvalue-reference support in C++11). It would be unfortunate if such types couldn’t be stored in your thread-safe stack.
OPTION 3: RETURN A POINTER TO THE POPPED ITEM The third option is to return a pointer to the popped item rather than return the item by value. The advantage here is that pointers can be freely copied without throwing an

48

CHAPTER 3 Sharing data between threads

exception, so you’ve avoided Cargill’s exception problem. The disadvantage is that returning a pointer requires a means of managing the memory allocated to the object, and for simple types such as integers, the overhead of such memory management can exceed the cost of returning the type by value. For any interface that uses this option, std::shared_ptr would be a good choice of pointer type; not only does it avoid memory leaks, because the object is destroyed once the last pointer is destroyed, but the library is in full control of the memory allocation scheme and doesn’t have to use new and delete. This can be important for optimization purposes: requiring that each object in the stack be allocated separately with new would impose quite an overhead compared to the original non-thread-safe version.
OPTION 4: PROVIDE BOTH OPTION 1 AND EITHER OPTION 2 OR 3 Flexibility should never be ruled out, especially in generic code. If you’ve chosen option 2 or 3, it’s relatively easy to provide option 1 as well, and this provides users of your code the ability to choose whichever option is most appropriate for them at little additional cost.
EXAMPLE DEFINITION OF A THREAD-SAFE STACK Listing 3.4 shows the class definition for a stack with no race conditions in the interface and that implements options 1 and 3: there are two overloads of pop(), one that takes a reference to a location in which to store the value and one that returns std::shared_ptr<>. It has a simple interface, with only two functions: push() and pop().

Listing 3.4 An outline class definition for a thread-safe stack

#include <exception> #include <memory>

For std::shared_ptr<>

struct empty_stack: std::exception

{

const char* what() const noexcept;

};

template<typename T>

class threadsafe_stack { public:
threadsafe_stack();

Assignment b
operator is deleted

threadsafe_stack(const threadsafe_stack&);

threadsafe_stack& operator=(const threadsafe_stack&) = delete;

void push(T new_value);

std::shared_ptr<T> pop();

void pop(T& value);

bool empty() const;

};

By paring down the interface you allow for maximum safety; even operations on the whole stack are restricted. The stack itself can’t be assigned, because the assignment
operator is deleted B (see appendix A, section A.2), and there’s no swap() function.

Protecting shared data with mutexes

49

It can, however, be copied, assuming the stack elements can be copied. The pop() functions throw an empty_stack exception if the stack is empty, so everything still works even if the stack is modified after a call to empty(). As mentioned in the description of option 3, the use of std::shared_ptr allows the stack to take care of the memory-allocation issues and avoid excessive calls to new and delete if desired. Your five stack operations have now become three: push(), pop(), and empty(). Even empty() is superfluous. This simplification of the interface allows for better control over the data; you can ensure that the mutex is locked for the entirety of an operation. The following listing shows a simple implementation that’s a wrapper around std::stack<>.

Listing 3.5 A fleshed-out class definition for a thread-safe stack

#include <exception>

#include <memory>

#include <mutex>

#include <stack>

struct empty_stack: std::exception

{

const char* what() const throw();

};

template<typename T>

class threadsafe_stack

{

private:

std::stack<T> data;

mutable std::mutex m;

public:

threadsafe_stack(){}

threadsafe_stack(const threadsafe_stack& other)

{

b Copy performed

std::lock_guard<std::mutex> lock(other.m);

in constructor

data=other.data;

body

}

threadsafe_stack& operator=(const threadsafe_stack&) = delete;

void push(T new_value)

{

std::lock_guard<std::mutex> lock(m);

data.push(std::move(new_value));

}

std::shared_ptr<T> pop() {
std::lock_guard<std::mutex> lock(m);

Check for empty before trying to pop value

if(data.empty()) throw empty_stack();

std::shared_ptr<T> const res(std::make_shared<T>(data.top()));

data.pop(); return res; }

Allocate return value before modifying stack

void pop(T& value)

{

std::lock_guard<std::mutex> lock(m);

if(data.empty()) throw empty_stack();

50

CHAPTER 3 Sharing data between threads

value=data.top(); data.pop(); } bool empty() const { std::lock_guard<std::mutex> lock(m); return data.empty(); } };
This stack implementation is copyable—the copy constructor locks the mutex in the source object and then copies the internal stack. You do the copy in the constructor
body B rather than the member initializer list in order to ensure that the mutex is
held across the copy. As the discussion of top() and pop() shows, problematic race conditions in inter-
faces arise because of locking at too small a granularity; the protection doesn’t cover the entirety of the desired operation. Problems with mutexes can also arise from locking at too large a granularity; the extreme situation is a single global mutex that protects all shared data. In a system where there’s a significant amount of shared data, this can eliminate any performance benefits of concurrency, because the threads are forced to run one at a time, even when they’re accessing different bits of data. The first versions of the Linux kernel that were designed to handle multi-processor systems used a single global kernel lock. Although this worked, it meant that a two-processor system typically had much worse performance than two single-processor systems, and performance on a four-processor system was nowhere near that of four single-processor systems. There was too much contention for the kernel, so the threads running on the additional processors were unable to perform useful work. Later revisions of the Linux kernel have moved to a more fine-grained locking scheme, so the performance of a four-processor system is much nearer the ideal of four times that of a single-processor system, because there’s far less contention.
One issue with fine-grained locking schemes is that sometimes you need more than one mutex locked in order to protect all the data in an operation. As described previously, sometimes the right thing to do is increase the granularity of the data covered by the mutexes, so that only one mutex needs to be locked. But sometimes that’s undesirable, such as when the mutexes are protecting separate instances of a class. In this case, locking at the next level up would mean either leaving the locking to the user or having a single mutex that protected all instances of that class, neither of which is particularly desirable.
If you end up having to lock two or more mutexes for a given operation, there’s another potential problem lurking in the wings: deadlock. This is almost the opposite of a race condition: rather than two threads racing to be first, each one is waiting for the other, so neither makes any progress.

Protecting shared data with mutexes

51

3.2.4

Deadlock: the problem and a solution
Imagine that you have a toy that comes in two parts, and you need both parts to play with it—a toy drum and drumstick, for example. Now imagine that you have two small children, both of whom like playing with it. If one of them gets both the drum and the drumstick, that child can merrily play the drum until tiring of it. If the other child wants to play, they have to wait, however sad that makes them. Now imagine that the drum and the drumstick are buried (separately) in the toy box, and your children both decide to play with them at the same time, so they go rummaging in the toy box. One finds the drum and the other finds the drumstick. Now they’re stuck; unless one decides to be nice and let the other play, each will hold on to whatever they have and demand that they be given the other piece, so neither gets to play.
Now imagine that you have not children arguing over toys but threads arguing over locks on mutexes: each of a pair of threads needs to lock both of a pair of mutexes to perform some operation, and each thread has one mutex and is waiting for the other. Neither thread can proceed, because each is waiting for the other to release its mutex. This scenario is called deadlock, and it’s the biggest problem with having to lock two or more mutexes in order to perform an operation.
The common advice for avoiding deadlock is to always lock the two mutexes in the same order: if you always lock mutex A before mutex B, then you’ll never deadlock. Sometimes this is straightforward, because the mutexes are serving different purposes, but other times it’s not so simple, such as when the mutexes are each protecting a separate instance of the same class. Consider, for example, an operation that exchanges data between two instances of the same class; in order to ensure that the data is exchanged correctly, without being affected by concurrent modifications, the mutexes on both instances must be locked. But if a fixed order is chosen (for example, the mutex for the instance supplied as the first parameter, then the mutex for the instance supplied as the second parameter), this can backfire: all it takes is for two threads to try to exchange data between the same two instances with the parameters swapped, and you have deadlock!
Thankfully, the C++ Standard Library has a cure for this in the form of std::lock—a function that can lock two or more mutexes at once without risk of deadlock. The example in the next listing shows how to use this for a simple swap operation.

Listing 3.6 Using std::lock() and std::lock_guard in a swap operation

class some_big_object; void swap(some_big_object& lhs,some_big_object& rhs); class X { private:
some_big_object some_detail; std::mutex m; public: X(some_big_object const& sd):some_detail(sd){}

52

CHAPTER 3 Sharing data between threads

friend void swap(X& lhs, X& rhs)

{

if(&lhs==&rhs)

return;

B

std::lock(lhs.m,rhs.m);

c

std::lock_guard<std::mutex> lock_a(lhs.m,std::adopt_lock);

std::lock_guard<std::mutex> lock_b(rhs.m,std::adopt_lock);

d

swap(lhs.some_detail,rhs.some_detail);

}

};

First, the arguments are checked to ensure they are different instances, because attempting to acquire a lock on std::mutex when you already hold it is undefined behavior. (A mutex that does permit multiple locks by the same thread is provided in the form of std::recursive_mutex. See section 3.3.3 for details.) Then, the call to
std::lock() B locks the two mutexes, and two std::lock_guard instances are constructed c and d, one for each mutex. The std::adopt_lock parameter is supplied
in addition to the mutex to indicate to the std::lock_guard objects that the mutexes are already locked, and they should adopt the ownership of the existing lock on the mutex rather than attempt to lock the mutex in the constructor.
This ensures that the mutexes are correctly unlocked on function exit in the general case where the protected operation might throw an exception; it also allows for a simple return. Also, it’s worth noting that locking either lhs.m or rhs.m inside the call to std::lock can throw an exception; in this case, the exception is propagated out of std::lock. If std::lock has successfully acquired a lock on one mutex and an exception is thrown when it tries to acquire a lock on the other mutex, this first lock is released automatically: std::lock provides all-or-nothing semantics with regard to locking the supplied mutexes.
C++17 provides additional support for this scenario, in the form of a new RAII template, std::scoped_lock<>. This is exactly equivalent to std::lock_guard<>, except that it is a variadic template, accepting a list of mutex types as template parameters, and a list of mutexes as constructor arguments. The mutexes supplied to the constructor are locked using the same algorithm as std::lock, so that when the constructor completes they are all locked, and they are then all unlocked in the destructor. The swap() operation from listing 3.6 can be rewritten as follows:

void swap(X& lhs, X& rhs)

{

if(&lhs==&rhs)

return;

std::scoped_lock guard(lhs.m,rhs.m);

b

swap(lhs.some_detail,rhs.some_detail);

}

This example uses another feature added with C++17: automatic deduction of class template parameters. If you have a C++17 compiler (which is likely if you’re using std::scoped_lock, because that is a C++17 library facility), the C++17 implicit class

Protecting shared data with mutexes

53

template parameter deduction mechanism will choose the correct mutex types from
the types of the objects passed to the constructor at object B. This line is equivalent
to the fully specified version:
std::scoped_lock<std::mutex,std::mutex> guard(lhs.m,rhs.m);

The existence of std::scoped_lock means that most of the cases where you would have used std::lock prior to C++17 can now be written using std::scoped_lock, with less potential for mistakes, which can only be a good thing!
Although std::lock (and std::scoped_lock<>) can help you avoid deadlock in those cases where you need to acquire two or more locks together, it doesn’t help if they’re acquired separately. In that case, you have to rely on your discipline as developers to ensure you don’t get deadlock. This isn’t easy: deadlocks are one of the nastiest problems to encounter in multithreaded code and are often unpredictable, with everything working fine the majority of the time. There are, however, some relatively simple rules that can help you to write deadlock-free code.

3.2.5

Further guidelines for avoiding deadlock
Deadlock doesn’t only occur with locks, although that’s the most frequent cause; you can create deadlock with two threads and no locks by having each thread call join() on the std::thread object for the other. In this case, neither thread can make progress because it’s waiting for the other to finish, like the children fighting over their toy. This simple cycle can occur anywhere that a thread can wait for another thread to perform some action if the other thread can simultaneously be waiting for the first thread, and it isn’t limited to two threads: a cycle of three or more threads will still cause deadlock. The guidelines for avoiding deadlock all boil down to one idea: don’t wait for another thread if there’s a chance it’s waiting for you. The individual guidelines provide ways of identifying and eliminating the possibility that the other thread is waiting for you.
AVOID NESTED LOCKS The first idea is the simplest: don’t acquire a lock if you already hold one. If you stick to this guideline, it’s impossible to get a deadlock from the lock usage alone because each thread only ever holds a single lock. You could still get deadlock from other things (like the threads waiting for each other), but mutex locks are probably the most common cause of deadlock. If you need to acquire multiple locks, do it as a single action with std::lock in order to acquire them without deadlock.
AVOID CALLING USER-SUPPLIED CODE WHILE HOLDING A LOCK This is a simple follow-on from the previous guideline. Because the code is usersupplied, you have no idea what it could do; it could do anything, including acquiring a lock. If you call user-supplied code while holding a lock, and that code acquires a lock, you’ve violated the guideline on avoiding nested locks and could get deadlock. Sometimes this is unavoidable; if you’re writing generic code, such as the stack in

54

CHAPTER 3 Sharing data between threads

section 3.2.3, every operation on the parameter type or types is user-supplied code. In this case, you need a new guideline.
ACQUIRE LOCKS IN A FIXED ORDER If you absolutely must acquire two or more locks, and you can’t acquire them as a single operation with std::lock, the next best thing is to acquire them in the same order in every thread. I touched on this in section 3.2.4 as one way of avoiding deadlock when acquiring two mutexes: the key is to define the order in a way that’s consistent between threads. In some cases, this is relatively easy. For example, look at the stack from section 3.2.3—the mutex is internal to each stack instance, but the operations on the data items stored in a stack require calling user-supplied code. You can, however, add the constraint that none of the operations on the data items stored in the stack should perform any operation on the stack itself. This puts the burden on the user of the stack, but it’s rather uncommon for the data stored in a container to access that container, and it’s quite apparent when this is happening, so it’s not a particularly difficult burden to carry.
In other cases, this isn’t so straightforward, as you discovered with the swap operation in section 3.2.4. At least in that case you could lock the mutexes simultaneously, but that’s not always possible. If you look back at the linked list example from section 3.1, you’ll see that one possibility for protecting the list is to have a mutex per node. Then, in order to access the list, threads must acquire a lock on every node they’re interested in. For a thread to delete an item, it must then acquire the lock on three nodes: the node being deleted and the nodes on either side, because they’re all being modified in some way. Likewise, to traverse the list, a thread must keep hold of the lock on the current node while it acquires the lock on the next one in the sequence, in order to ensure that the next pointer isn’t modified in the meantime. Once the lock on the next node has been acquired, the lock on the first can be released because it’s no longer necessary.
This hand-over-hand locking style allows multiple threads to access the list, provided each is accessing a different node. But in order to prevent deadlock, the nodes must always be locked in the same order: if two threads tried to traverse the list in opposite orders using hand-over-hand locking, they could deadlock with each other in the middle of the list. If nodes A and B are adjacent in the list, the thread going one way will try to hold the lock on node A and try to acquire the lock on node B. A thread going the other way would be holding the lock on node B and trying to acquire the lock on node A—a classic scenario for deadlock, as shown in figure 3.2.
Likewise, when deleting node B that lies between nodes A and C, if that thread acquires the lock on B before the locks on A and C, it has the potential to deadlock with a thread traversing the list. Such a thread would try to lock either A or C first (depending on the direction of traversal) but would then find that it couldn’t obtain a lock on B because the thread doing the deleting was holding the lock on B and trying to acquire the locks on A and C.

Protecting shared data with mutexes

55

Thread 1 Lock master entry mutex Read head node pointer Lock head node mutex Unlock master entry mutex
Read head → next pointer Lock next node mutex Read next → next pointer … Lock node A mutex Read A → next pointer (which is B)
Block trying to lock node B mutex
Deadlock!

Thread 2
Lock master entry mutex Lock tail node mutex Read tail → prev pointer Unlock tail node mutex … Lock node C mutex Read C → next pointer (which is B) Lock node B mutex Unlock node C mutex Read B → prev pointer (which is A) Block trying to lock node A mutex

Figure 3.2 Deadlock with threads traversing a list in opposite orders

One way to prevent deadlock here is to define an order of traversal, so a thread must always lock A before B and B before C. This would eliminate the possibility of deadlock at the expense of disallowing reverse traversal. Similar conventions can often be established for other data structures.
USE A LOCK HIERARCHY Although this is a particular case of defining lock ordering, a lock hierarchy can provide a means of checking that the convention is adhered to at runtime. The idea is that you divide your application into layers and identify all the mutexes that may be locked in any given layer. When code tries to lock a mutex, it isn’t permitted to lock that mutex if it already holds a lock from a lower layer. You can check this at runtime by assigning layer numbers to each mutex and keeping a record of which mutexes are locked by each thread. This is a common pattern, but the C++ Standard Library does not provide direct support for it, so you will need to write a custom hierarchical_mutex mutex type, the code for which is shown in listing 3.8.

56

CHAPTER 3 Sharing data between threads

The following listing shows an example of two threads using a hierarchical mutex.

Listing 3.7 Using a lock hierarchy to prevent deadlock

hierarchical_mutex high_level_mutex(10000);

b

hierarchical_mutex low_level_mutex(5000);

hierarchical_mutex other_mutex(6000); int do_low_level_stuff();

c d

int low_level_func()

{

e

std::lock_guard<hierarchical_mutex> lk(low_level_mutex);

return do_low_level_stuff();

}

void high_level_stuff(int some_param);

void high_level_func()

{

g

std::lock_guard<hierarchical_mutex> lk(high_level_mutex);

high_level_stuff(low_level_func());

}

f

void thread_a()

h

{

high_level_func();

}

void do_other_stuff();

void other_stuff()

{

high_level_func();

1)

do_other_stuff();

}

void thread_b()

i

{

std::lock_guard<hierarchical_mutex> lk(other_mutex);

j

other_stuff();

}

This code has three instances of hierarchical_mutex, (B, c, and d), which are
constructed with progressively lower hierarchy numbers. Because the mechanism is defined so that if you hold a lock on a hierarchical_mutex, then you can only acquire a lock on another hierarchical_mutex with a lower hierarchy number, this imposes restrictions on what the code can do.
Assuming do_low_level_stuff doesn’t lock any mutexes, low_level_func is the
bottom of your hierarchy, and locks the low_level_mutex e. high_level_func calls low_level_func f, while holding a lock on high_level_mutex g, but that’s OK, because the hierarchy level of high_level_mutex (B: 10000) is higher than that of low_level_mutex (c: 5000).
thread_a() h abides by the rules, so it runs fine. On the other hand, thread_b() i disregards the rules and therefore will fail at
runtime.

Protecting shared data with mutexes

57

First off, it locks other_mutex j, which has a hierarchy value of only 6000 d.
This means it should be somewhere midway in the hierarchy. When other_stuff()
calls high_level_func() 1), it’s violating the hierarchy: high_level_func() tries to
acquire the high_level_mutex, which has a value of 10000, considerably more than the current hierarchy value of 6000. The hierarchical_mutex will therefore report an error, possibly by throwing an exception or aborting the program. Deadlocks between hierarchical mutexes are impossible, because the mutexes themselves enforce the lock ordering. This does mean that you can’t hold two locks at the same time if they’re the same level in the hierarchy, so hand-over-hand locking schemes require that each mutex in the chain has a lower hierarchy value than the prior one, which may be impractical in some cases.
This example also demonstrates another point: the use of the std::lock_guard<> template with a user-defined mutex type. hierarchical_mutex is not part of the standard but is easy to write; a simple implementation is shown in listing 3.8. Even though it’s a user-defined type, it can be used with std::lock_guard<> because it implements the three member functions required to satisfy the mutex concept: lock(), unlock(), and try_lock(). You haven’t yet seen try_lock() used directly, but it’s fairly simple: if the lock on the mutex is held by another thread, it returns false rather than waiting until the calling thread can acquire the lock on the mutex. It may also be used by std::lock() internally, as part of the deadlock-avoidance algorithm.
The implementation of hierarchical_mutex uses a thread-local variable to store the current hierarchy value. This value is accessible to all mutex instances, but has a different value on each thread. This allows the code to check the behavior of each thread separately, and the code for each mutex can check whether or not the current thread is allowed to lock that mutex.

Listing 3.8 A simple hierarchical mutex

class hierarchical_mutex

{

std::mutex internal_mutex;

unsigned long const hierarchy_value;

unsigned long previous_hierarchy_value;

static thread_local unsigned long this_thread_hierarchy_value;

b

void check_for_hierarchy_violation()

{

if(this_thread_hierarchy_value <= hierarchy_value)

c

{

throw std::logic_error(“mutex hierarchy violated”);

}

}

void update_hierarchy_value()

{

previous_hierarchy_value=this_thread_hierarchy_value;

d

this_thread_hierarchy_value=hierarchy_value;

}

58

CHAPTER 3 Sharing data between threads

public:

explicit hierarchical_mutex(unsigned long value):

hierarchy_value(value),

previous_hierarchy_value(0)

{}

void lock()

{
e check_for_hierarchy_violation();

internal_mutex.lock();

update_hierarchy_value();

f

}

void unlock()

{

if(this_thread_hierarchy_value!=hierarchy_value)

throw std::logic_error(“mutex hierarchy violated”);

this_thread_hierarchy_value=previous_hierarchy_value;

internal_mutex.unlock();

}

bool try_lock()

{

check_for_hierarchy_violation();

if(!internal_mutex.try_lock())

h

return false;

update_hierarchy_value();

return true;

}

};

thread_local unsigned long

hierarchical_mutex::this_thread_hierarchy_value(ULONG_MAX);

j g
i

The key here is the use of the thread_local value representing the hierarchy value
for the current thread: this_thread_hierarchy_value B. It’s initialized to the maximum value i, so initially any mutex can be locked. Because it’s declared
thread_local, every thread has its own copy, so the state of the variable in one thread is entirely independent of the state of the variable when read from another thread. See appendix A, section A.8, for more information about thread_local.
So, the first time a thread locks an instance of hierarchical_mutex, the value of this_thread_hierarchy_value is ULONG_MAX. By its nature, this is greater than any
other value, so the check in check_for_hierarchy_violation() c passes. With that check out of the way, lock()delegates to the internal mutex for the locking e. Once this lock has succeeded, you can update the hierarchy value f.
If you now lock another hierarchical_mutex while holding the lock on this first one, the value of this_thread_hierarchy_value reflects the hierarchy value of the first mutex. The hierarchy value of this second mutex must now be less than that of
the mutex already held in order for the check c to pass.
Now, it’s important to save the previous value of the hierarchy value for the cur-
rent thread so you can restore it in unlock() g; otherwise you’d never be able to
lock a mutex with a higher hierarchy value again, even if the thread didn’t hold any locks. Because you store this previous hierarchy value only when you hold the

Protecting shared data with mutexes

59

internal_mutex d, and you restore it before you unlock the internal mutex g, you
can safely store it in the hierarchical_mutex itself, because it’s safely protected by the lock on the internal mutex. In order to avoid the hierarchy getting confused due to
out-of-order unlocking, you throw at j if the mutex being unlocked is not the most
recently locked one. Other mechanisms are possible, but this is the simplest. try_lock() works the same as lock(), except that if the call to try_lock() on the
internal_mutex fails h, then you don’t own the lock, so you don’t update the hierar-
chy value, and return false rather than true. Although detection is a runtime check, it’s at least not timing-dependent—you
don’t have to wait around for the rare conditions that cause deadlock to show up. Also, the design process required to divide the application and mutexes in this way can help eliminate many possible causes of deadlock before they even get written. It might be worth performing the design exercise even if you don’t go as far as writing the runtime checks.
EXTENDING THESE GUIDELINES BEYOND LOCKS As I mentioned back at the beginning of this section, deadlock doesn’t only occur with locks; it can occur with any synchronization construct that can lead to a wait cycle. It’s therefore worth extending these guidelines to cover those cases too. For example, just as you should avoid acquiring nested locks if possible, it’s a bad idea to wait for a thread while holding a lock, because that thread might need to acquire the lock in order to proceed. Similarly, if you’re going to wait for a thread to finish, it might be worth identifying a thread hierarchy, so that a thread waits only for threads lower down the hierarchy. One simple way to do this is to ensure that your threads are joined in the same function that started them, as described in sections 3.1.2 and 3.3.
Once you’ve designed your code to avoid deadlock, std::lock() and std:: lock_guard cover most of the cases of simple locking, but sometimes more flexibility is required. For those cases, the Standard Library provides the std::unique_lock template. Like std::lock_guard, this is a class template parameterized on the mutex type, and it also provides the same RAII-style lock management as std::lock_guard, but with a bit more flexibility.

3.2.6

Flexible locking with std::unique_lock
std::unique_lock provides a bit more flexibility than std::lock_guard by relaxing the invariants; an std::unique_lock instance doesn’t always own the mutex that it’s associated with. First off, as you can pass std::adopt_lock as a second argument to the constructor to have the lock object manage the lock on a mutex, you can also pass std::defer_lock as the second argument to indicate that the mutex should remain unlocked on construction. The lock can then be acquired later by calling lock() on the std::unique_lock object (not the mutex) or by passing the std:: unique_lock object to std::lock(). Listing 3.6 could easily have been written as shown in listing 3.9,
using std::unique_lock and std::defer_lock B, rather than std::lock_guard and
std::adopt_lock. The code has the same line count and is equivalent, apart from

60

CHAPTER 3 Sharing data between threads

one small thing: std::unique_lock takes more space and is slightly slower to use than std::lock_guard. The flexibility of allowing an std::unique_lock instance not to own the mutex comes at a price: this information has to be stored, and it has to be updated.

Listing 3.9 Using std::lock() and std::unique_lock in a swap operation

class some_big_object;

void swap(some_big_object& lhs,some_big_object& rhs);

class X

{

private:

some_big_object some_detail;

std::mutex m;

public:

X(some_big_object const& sd):some_detail(sd){} friend void swap(X& lhs, X& rhs) {

std::defer_lock B
leaves mutexes
unlocked.

if(&lhs==&rhs)

return;

std::unique_lock<std::mutex> lock_a(lhs.m,std::defer_lock);

std::unique_lock<std::mutex> lock_b(rhs.m,std::defer_lock);

std::lock(lock_a,lock_b); swap(lhs.some_detail,rhs.some_detail); }

Mutexes are
c locked here.

};

In listing 3.9, the std::unique_lock objects could be passed to std::lock() c,
because std::unique_lock provides lock(), try_lock(), and unlock() member functions. These forward to the member functions of the same name on the underlying mutex to do the work and update a flag inside the std::unique_lock instance to indicate whether the mutex is currently owned by that instance. This flag is necessary in order to ensure that unlock() is called correctly in the destructor. If the instance does own the mutex, the destructor must call unlock(), and if the instance does not own the mutex, it must not call unlock(). This flag can be queried by calling the owns_lock() member function. Unless you’re going to be transferring lock ownership around or doing something else that requires std::unique_lock, you’re still better off using the C++17 variadic std::scoped_lock if it’s available to you (see section 3.2.4).
As you might expect, this flag has to be stored somewhere. Therefore, the size of a std::unique_lock object is typically larger than that of a std::lock_guard object, and there’s also a slight performance penalty when using std::unique_lock over std:: lock_guard because the flag has to be updated or checked, as appropriate. If std::lock_ guard is sufficient for your needs, I’d therefore recommend using it in preference. That said, there are cases where std::unique_lock is a better fit for the task at hand because you need to make use of the additional flexibility. One example is deferred locking, as you’ve already seen; another case is where the ownership of the lock needs to be transferred from one scope to another.

Protecting shared data with mutexes

61

3.2.7

Transferring mutex ownership between scopes
Because std::unique_lock instances don’t have to own their associated mutexes, the ownership of a mutex can be transferred between instances by moving the instances around. In some cases this transfer is automatic, such as when returning an instance from a function, and in other cases you have to do it explicitly by calling std::move(). Fundamentally this depends on whether the source is an lvalue—a real variable or reference to one—or an rvalue—a temporary of some kind. Ownership transfer is automatic if the source is an rvalue and must be done explicitly for an lvalue in order to avoid accidentally transferring ownership away from a variable. std::unique_lock is an example of a type that’s moveable but not copyable. See appendix A, section A.1.1, for more about move semantics.
One possible use is to allow a function to lock a mutex and transfer ownership of that lock to the caller, so the caller can then perform additional actions under the protection of the same lock. The following code snippet shows an example of this; the get_lock() function locks the mutex and then prepares the data before returning the lock to the caller:

std::unique_lock<std::mutex> get_lock()

{

extern std::mutex some_mutex;

std::unique_lock<std::mutex> lk(some_mutex);

prepare_data();

return lk;

B

}

void process_data()

{

std::unique_lock<std::mutex> lk(get_lock());

c

do_something();

}

Because lk is an automatic variable declared within the function, it can be returned
directly B, without a call to std:move(); the compiler takes care of calling the move
constructor. The process_data() function can then transfer ownership directly into
its own std::unique_lock instance c, and the call to do_something() can rely on
the data being correctly prepared without another thread altering the data in the meantime.
Typically this sort of pattern would be used where the mutex to be locked is dependent on the current state of the program or on an argument passed in to the function that returns the std::unique_lock object. One such usage is where the lock isn’t returned directly but is a data member of a gateway class used to ensure correctly locked access to some protected data. In this case, all access to the data is through this gateway class: when you want to access the data, you obtain an instance of the gateway class (by calling a function such as get_lock() in the preceding example), which acquires the lock. You can then access the data through member functions of the gateway object. When you’re finished, you destroy the gateway object, which releases the

62

CHAPTER 3 Sharing data between threads

lock and allows other threads to access the protected data. Such a gateway object may well be moveable (so that it can be returned from a function), in which case the lock object data member also needs to be moveable.
The flexibility of std::unique_lock also allows instances to relinquish their locks before they’re destroyed. You can do this with the unlock() member function, like for a mutex. std::unique_lock supports the same basic set of member functions for locking and unlocking as a mutex does, so that it can be used with generic functions such as std::lock. The ability to release a lock before the std::unique_lock instance is destroyed means that you can optionally release it in a specific code branch if it’s apparent that the lock is no longer required. This can be important for the performance of the application; holding a lock for longer than required can cause a drop in performance, because other threads waiting for the lock are prevented from proceeding for longer than necessary.

3.2.8

Locking at an appropriate granularity
The granularity of a lock is something I touched on earlier, in section 3.2.3: the lock granularity is a hand-waving term to describe the amount of data protected by a single lock. A fine-grained lock protects a small amount of data, and a coarse-grained lock protects a large amount of data. Not only is it important to choose a sufficiently coarse lock granularity to ensure the required data is protected, but it’s also important to ensure that a lock is held only for the operations that require it. We all know the frustration of waiting in the checkout line in a supermarket with a cart full of groceries only for the person currently being served to suddenly realize that they forgot some cranberry sauce and then leave everybody waiting while they go and find some, or for the cashier to be ready for payment and the customer to only then start rummaging in their bag for their wallet. Everything proceeds much more easily if everybody gets to the checkout with everything they want and with an appropriate method of payment ready.
The same applies to threads: if multiple threads are waiting for the same resource (the cashier at the checkout), then if any thread holds the lock for longer than necessary, it will increase the total time spent waiting (don’t wait until you’ve reached the checkout to start looking for the cranberry sauce). Where possible, lock a mutex only while accessing the shared data; try to do any processing of the data outside the lock. In particular, don’t do any time-consuming activities like file I/O while holding a lock. File I/O is typically hundreds (if not thousands) of times slower than reading or writing the same volume of data from memory. Unless the lock is intended to protect access to the file, performing I/O while holding the lock will delay other threads unnecessarily (because they’ll block while waiting to acquire the lock), potentially eliminating any performance gain from the use of multiple threads.
std::unique_lock works well in this situation, because you can call unlock() when the code no longer needs access to the shared data and then call lock() again if access is required later in the code:

Protecting shared data with mutexes

63

void get_and_process_data()

{

B std::unique_lock<std::mutex> my_lock(the_mutex);

Don’t need mutex

some_class data_to_process=get_next_data_chunk();

locked across call

my_lock.unlock();

to process()

result_type result=process(data_to_process);

my_lock.lock(); write_result(data_to_process,result); }

Relock mutex
c to write result

You don’t need the mutex locked across the call to process(), so you manually
unlock it before the call B and then lock it again afterward c.
Hopefully it’s obvious that if you have one mutex protecting an entire data structure, not only is there likely to be more contention for the lock, but also the potential for reducing the time that the lock is held is diminished. More of the operation steps will require a lock on the same mutex, so the lock must be held longer. This double whammy of a cost is also a double incentive to move toward finer-grained locking wherever possible.
As this example shows, locking at an appropriate granularity isn’t only about the amount of data locked; it’s also about how long the lock is held and what operations are performed while the lock is held. In general, a lock should be held for only the minimum possible time needed to perform the required operations. This also means that timeconsuming operations such as acquiring another lock (even if you know it won’t deadlock) or waiting for I/O to complete shouldn’t be done while holding a lock unless absolutely necessary.
In listings 3.6 and 3.9, the operation that required locking the two mutexes was a swap operation, which obviously requires concurrent access to both objects. Suppose instead you were trying to compare a simple data member that was a plain int. Would this make a difference? ints are cheap to copy, so you could easily copy the data for each object being compared while only holding the lock for that object and then compare the copied values. This would mean that you were holding the lock on each mutex for the minimum amount of time and also that you weren’t holding one lock while locking another. The following listing shows a class Y for which this is the case and a sample implementation of the equality comparison operator.

Listing 3.10 Locking one mutex at a time in a comparison operator

class Y

{

private:

int some_detail;

mutable std::mutex m;

int get_detail() const

{

std::lock_guard<std::mutex> lock_a(m);

b

return some_detail;

}

64

CHAPTER 3 Sharing data between threads

public:

Y(int sd):some_detail(sd){}

friend bool operator==(Y const& lhs, Y const& rhs)

{

if(&lhs==&rhs)

return true;

c

int const lhs_value=lhs.get_detail();

int const rhs_value=rhs.get_detail();

d

return lhs_value==rhs_value;

}

e

};

In this case, the comparison operator first retrieves the values to be compared by call-
ing the get_detail() member function, c and d. This function retrieves the value while protecting it with a lock B. The comparison operator then compares the retrieved values e. Note, however, that as well as reducing the locking periods so that
only one lock is held at a time (and eliminating the possibility of deadlock), this has subtly changed the semantics of the operation compared to holding both locks together. In listing 3.10, if the operator returns true, it means that the value of lhs.some_detail at one point in time is equal to the value of rhs.some_detail at another point in time. The two values could have been changed in any way in between the two reads;
the values could have been swapped in between c and d, for example, rendering
the comparison meaningless. The equality comparison might return true to indicate that the values were equal, even though there was never an instant in time when the values were equal. It’s therefore important to be careful when making these changes that the semantics of the operation are not changed in a problematic fashion: if you don’t hold the required locks for the entire duration of an operation, you’re exposing yourself to race conditions.
Sometimes, there isn’t an appropriate level of granularity because not all accesses to the data structure require the same level of protection. In this case, it might be appropriate to use an alternative mechanism, instead of a plain std::mutex.

3.3 Alternative facilities for protecting shared data
Although they’re the most general mechanism, mutexes aren’t the only game in town when it comes to protecting shared data; there are alternatives that provide more appropriate protection in specific scenarios.
One particularly extreme (but remarkably common) case is where the shared data needs protection only from concurrent access while it’s being initialized, but after that no explicit synchronization is required. This might be because the data is read-only once created, and so there are no possible synchronization issues, or it might be because the necessary protection is performed implicitly as part of the operations on the data. In either case, locking a mutex after the data has been initialized, purely in order to protect the initialization, is unnecessary and a needless hit to performance. It’s for this reason that the C++ Standard provides a mechanism purely for protecting shared data during initialization.

Alternative facilities for protecting shared data

65

3.3.1

Protecting shared data during initialization
Suppose you have a shared resource that’s so expensive to construct that you want to do so only if it’s required; maybe it opens a database connection or allocates a lot of memory. Lazy initialization such as this is common in single-threaded code—each operation that requires the resource first checks to see if it has been initialized and then initializes it before use if not:

std::shared_ptr<some_resource> resource_ptr;

void foo()

{

if(!resource_ptr)

{

resource_ptr.reset(new some_resource);

b

}

resource_ptr->do_something();

}

If the shared resource itself is safe for concurrent access, the only part that needs pro-
tecting when converting this to multithreaded code is the initialization B, but a naïve
translation such as that in the following listing can cause unnecessary serialization of threads using the resource. This is because each thread must wait on the mutex in order to check whether the resource has already been initialized.

Listing 3.11 Thread-safe lazy initialization using a mutex

std::shared_ptr<some_resource> resource_ptr; std::mutex resource_mutex; void foo() {
std::unique_lock<std::mutex> lk(resource_mutex); if(!resource_ptr) {
resource_ptr.reset(new some_resource); } lk.unlock(); resource_ptr->do_something(); }

All threads are serialized here
Only the initialization needs protection

This code is common enough, and the unnecessary serialization problematic enough, that many people have tried to come up with a better way of doing this, including the infamous double-checked locking pattern: the pointer is first read without acquiring the
lock (B in the following code), and the lock is acquired only if the pointer is NULL. The pointer is then checked again once the lock has been acquired (c, hence the
double-checked part) in case another thread has done the initialization between the first check and this thread acquiring the lock:

void undefined_behaviour_with_double_checked_locking()

{

if(!resource_ptr)

B

66

CHAPTER 3 Sharing data between threads

{

std::lock_guard<std::mutex> lk(resource_mutex);

if(!resource_ptr)

c

{

resource_ptr.reset(new some_resource);

d

}

}

resource_ptr->do_something();

e

}

Unfortunately, this pattern is infamous for a reason: it has the potential for nasty race
conditions, because the read outside the lock B, isn’t synchronized with the write done by another thread inside the lock d. This creates a race condition that covers
not only the pointer itself but also the object pointed to; even if a thread sees the pointer written by another thread, it might not see the newly created instance of
some_resource, resulting in the call to do_something() e operating on incorrect val-
ues. This is an example of the type of race condition defined as a data race by the C++ Standard and specified as undefined behavior. It’s therefore quite definitely something to avoid. See chapter 5 for a detailed discussion of the memory model, including what constitutes a data race.
The C++ Standards Committee also saw that this was an important scenario, and so the C++ Standard Library provides std::once_flag and std::call_once to handle this situation. Rather than locking a mutex and explicitly checking the pointer, every thread can use std::call_once, safe in the knowledge that the pointer will have been initialized by some thread (in a properly synchronized fashion) by the time std::call_once returns. The necessary synchronization data is stored in the std::once_flag instance; each instance of std::once_flag corresponds to a different initialization. Use of std::call_once will typically have a lower overhead than using a mutex explicitly, especially when the initialization has already been done, so it should be used in preference where it matches the required functionality. The following example shows the same operation as listing 3.11, rewritten to use std::call_once. In this case, the initialization is done by calling a function, but it could easily have been done with an instance of a class with a function call operator. Like most of the functions in the standard library that take functions or predicates as arguments, std::call_once works with any function or callable object:

std::shared_ptr<some_resource> resource_ptr;

std::once_flag resource_flag;

void init_resource()

b

{

resource_ptr.reset(new some_resource);

}

void foo()

{

std::call_once(resource_flag,init_resource);

resource_ptr->do_something();

}

Initialization is called exactly once.

Alternative facilities for protecting shared data

67

In this example, both the std::once_flag B and data being initialized are namespace-
scope objects, but std::call_once() can easily be used for lazy initialization of class members, as in the following listing.

Listing 3.12 Thread-safe lazy initialization of a class member using std::call_once

class X

{

private:

connection_info connection_details;

connection_handle connection;

std::once_flag connection_init_flag;

void open_connection()

{

connection=connection_manager.open(connection_details);

}

public:

X(connection_info const& connection_details_):

connection_details(connection_details_)

{}

void send_data(data_packet const& data)

B

{

std::call_once(connection_init_flag,&X::open_connection,this);

connection.send_data(data);

}

data_packet receive_data()

d

c

{

std::call_once(connection_init_flag,&X::open_connection,this);

return connection.receive_data();

}

};

In that example, the initialization is done either by the first call to send_data() B, or by the first call to receive_data() d. The use of the open_connection() member
function to initialize the data also requires that the this pointer be passed in. Just as for other functions in the Standard Library that accept callable objects, such as the constructors for std::thread and std::bind(), this is done by passing an additional
argument to std::call_once() c.
It’s worth noting that like std::mutex, std::once_flag instances can’t be copied or moved, so if you use them as a class member like this, you’ll have to explicitly define these special member functions should you require them.
One scenario where there’s a potential race condition over initialization is that of a local variable declared with static. The initialization of such a variable is defined to occur the first time control passes through its declaration; for multiple threads calling the function, this means there’s the potential for a race condition to define first. On many pre-C++11 compilers this race condition is problematic in practice, because multiple threads may believe they’re first and try to initialize the variable, or threads may try to use it after initialization has started on another thread but before it’s finished. In C++11 this problem is solved: the initialization is defined to happen on

68

CHAPTER 3 Sharing data between threads

exactly one thread, and no other threads will proceed until that initialization is complete, so the race condition is over which thread gets to do the initialization rather than anything more problematic. This can be used as an alternative to std::call_ once for those cases where a single global instance is required:

class my_class; my_class& get_my_class_instance() {
static my_class instance; return instance; }

B Initialization guaranteed
to be thread-safe

Multiple threads can then call get_my_class_instance() safely B, without having to
worry about race conditions on the initialization. Protecting data only for initialization is a special case of a more general scenario:
that of a rarely updated data structure. For most of the time, this data structure is read-only and can therefore be read by multiple threads concurrently, but on occasion the data structure may need updating. What’s needed here is a protection mechanism that acknowledges this fact.

3.3.2

Protecting rarely updated data structures
Consider a table used to store a cache of DNS entries for resolving domain names to their corresponding IP addresses. Typically, a given DNS entry will remain unchanged for a long period of time—in many cases, DNS entries remain unchanged for years. Although new entries may be added to the table from time to time as users access different websites, this data will therefore remain largely unchanged throughout its life. It’s important that the validity of the cached entries is checked periodically, but this still requires an update only if the details have changed.
Although updates are rare, they can still happen, and if this cache is to be accessed from multiple threads, it will need to be appropriately protected during updates to ensure that none of the threads reading the cache see a broken data structure.
In the absence of a special-purpose data structure that exactly fits the desired usage and that’s specially designed for concurrent updates and reads (such as those in chapters 6 and 7), this update requires that the thread doing the update has exclusive access to the data structure until it has completed the operation. Once the change is complete, the data structure is again safe for multiple threads to access concurrently. Using std::mutex to protect the data structure is therefore overly pessimistic, because it will eliminate the possible concurrency in reading the data structure when it isn’t undergoing modification; what’s needed is a different kind of mutex. This new kind of mutex is typically called a reader-writer mutex, because it allows for two different kinds of usage: exclusive access by a single “writer” thread or shared, and concurrent access by multiple “reader” threads.
The C++17 Standard Library provides two such mutexes out of the box, std:: shared_mutex and std::shared_timed_mutex. C++14 only features std::shared_ timed_mutex, and C++11 didn’t provide either. If you’re struck with a pre-C++14

Alternative facilities for protecting shared data

69

compiler, then you could use the implementation provided by the Boost library, which is based on the original proposal. The difference between std::shared_mutex and std::shared_timed_mutex is that std::shared_timed_mutex supports additional operations (as described in section 4.3), so std::shared_mutex might offer a performance benefit on some platforms, if you don’t need the additional operations.
As you’ll see in chapter 8, the use of such a mutex isn’t a panacea, and the performance is dependent on the number of processors involved and the relative workloads of the reader and updater threads. It’s therefore important to profile the performance of the code on the target system to ensure that there’s a benefit to the additional complexity.
Rather than using an instance of std::mutex for the synchronization, you use an instance of std::shared_mutex. For the update operations, std::lock_guard <std::shared_mutex> and std::unique_lock<std::shared_mutex> can be used for the locking, in place of the corresponding std::mutex specializations. These ensure exclusive access, as with std::mutex. Those threads that don’t need to update the data structure can instead use std::shared_lock<std::shared_mutex> to obtain shared access. This RAII class template was added in C++14, and is used the same as std::unique_lock, except that multiple threads may have a shared lock on the same std::shared_mutex at the same time. The only constraint is that if any thread has a shared lock, a thread that tries to acquire an exclusive lock will block until all other threads have relinquished their locks, and likewise if any thread has an exclusive lock, no other thread may acquire a shared or exclusive lock until the first thread has relinquished its lock.
The following listing shows a simple DNS cache like the one described, using std::map to hold the cached data, protected using std::shared_mutex.

Listing 3.13 Protecting a data structure with std::shared_mutex

#include <map>

#include <string>

#include <mutex>

#include <shared_mutex>

class dns_entry;

class dns_cache

{

std::map<std::string,dns_entry> entries;

mutable std::shared_mutex entry_mutex;

public:

dns_entry find_entry(std::string const& domain) const

{

std::shared_lock<std::shared_mutex> lk(entry_mutex);

b

std::map<std::string,dns_entry>::const_iterator const it=

entries.find(domain);

return (it==entries.end())?dns_entry():it->second;

}

void update_or_add_entry(std::string const& domain,

dns_entry const& dns_details)

70

CHAPTER 3 Sharing data between threads

{

std::lock_guard<std::shared_mutex> lk(entry_mutex);

c

entries[domain]=dns_details;

}

};

In listing 3.13, find_entry() uses an instance of std::shared_lock<> to protect it for
shared, read-only access B; multiple threads can therefore call find_entry() simulta-
neously without problems. On the other hand, update_or_add_entry() uses an instance of std::lock_guard<> to provide exclusive access while the table is updated
c; not only are other threads prevented from doing updates in a call to update_
or_add_entry(), but threads that call find_entry() are blocked too.

3.3.3

Recursive locking
With std::mutex, it’s an error for a thread to try to lock a mutex it already owns, and attempting to do so will result in undefined behavior. But in some circumstances it would be desirable for a thread to reacquire the same mutex several times without having first released it. For this purpose, the C++ Standard Library provides std::recursive_mutex. It works like std::mutex, except that you can acquire multiple locks on a single instance from the same thread. You must release all your locks before the mutex can be locked by another thread, so if you call lock() three times, you must also call unlock() three times. The correct use of std::lock_guard <std::recursive_mutex> and std::unique_lock<std::recursive_mutex> will handle this for you.
Most of the time, if you think you want a recursive mutex, you probably need to change your design instead. A common use of recursive mutexes is where a class is designed to be accessible from multiple threads concurrently, so it has a mutex protecting the member data. Each public member function locks the mutex, does the work, and then unlocks the mutex. But sometimes it’s desirable for one public member function to call another as part of its operation. In this case, the second member function will also try to lock the mutex, leading to undefined behavior. The quick-anddirty solution is to change the mutex to a recursive mutex. This will allow the mutex lock in the second member function to succeed and the function to proceed.
But such usage is not recommended because it can lead to sloppy thinking and bad design. In particular, the class invariants are typically broken while the lock is held, which means that the second member function needs to work even when called with the invariants broken. It’s usually better to extract a new private member function that’s called from both member functions, which does not lock the mutex (it expects it to already be locked). You can then think carefully about the circumstances under which that new function can be called and the state of the data under those circumstances.

Summary

71

Summary
In this chapter I discussed how problematic race conditions can be disastrous when sharing data between threads and how to use std::mutex and careful interface design to avoid them. You saw that mutexes aren’t a panacea and do have their own problems in the form of deadlock, though the C++ Standard Library provides a tool to help avoid that in the form of std::lock(). You then looked at some further techniques for avoiding deadlock, followed by a brief look at transferring lock ownership and issues surrounding choosing the appropriate granularity for locking. Finally, I covered the alternative data-protection facilities provided for specific scenarios, such as std:: call_once() and std::shared_mutex.
One thing that I haven’t covered yet, however, is waiting for input from other threads. Your thread-safe stack throws an exception if the stack is empty, so if one thread wanted to wait for another thread to push a value on the stack (which is, after all, one of the primary uses for a thread-safe stack), it would have to repeatedly try to pop a value, retrying if an exception gets thrown. This consumes valuable processing time in performing the check, without making any progress; indeed, the constant checking might hamper progress by preventing the other threads in the system from running. What’s needed is some way for a thread to wait for another thread to complete a task without consuming CPU time in the process. Chapter 4 builds on the facilities I’ve discussed for protecting shared data and introduces the various mechanisms for synchronizing operations between threads in C++; chapter 6 shows how these can be used to build larger reusable data structures.

Synchronizing concurrent operations
This chapter covers
 Waiting for an event  Waiting for one-off events with futures  Waiting with a time limit  Using the synchronization of operations to
simplify code
In the last chapter, we looked at various ways of protecting data that’s shared between threads. But sometimes you don’t just need to protect the data, you also need to synchronize actions on separate threads. One thread might need to wait for another thread to complete a task before the first thread can complete its own, for example. In general, it’s common to want a thread to wait for a specific event to happen or a condition to be true. Although it would be possible to do this by periodically checking a “task complete” flag or something similar stored in shared data, this is far from ideal. The need to synchronize operations between threads like this is such a common scenario that the C++ Standard Library provides facilities to handle it, in the form of condition variables and futures. These facilities are extended in the Concurrency Technical Specification (TS), which provides additional operations for futures, alongside new synchronization facilities in the form of latches and barriers.
72

Waiting for an event or other condition

73

In this chapter, I’ll discuss how to wait for events with condition variables, futures, latches, and barriers, and how to use them to simplify the synchronization of operations.

4.1 Waiting for an event or other condition
Suppose you’re traveling on an overnight train. One way to ensure you get off at the right station would be to stay awake all night and pay attention to where the train stops. You wouldn’t miss your station, but you’d be tired when you got there. Alternatively, you could look at the timetable to see when the train is supposed to arrive, set your alarm a bit before, and go to sleep. That would be OK; you wouldn’t miss your stop, but if the train got delayed, you’d wake up too early. There’s also the possibility that your alarm clock’s batteries would die, and you’d sleep too long and miss your station. What would be ideal is if you could go to sleep and have somebody or something wake you up when the train gets to your station, whenever that is.
How does that relate to threads? Well, if one thread is waiting for a second thread to complete a task, it has several options. First, it could keep checking a flag in shared data (protected by a mutex) and have the second thread set the flag when it completes the task. This is wasteful on two counts: the thread consumes valuable processing time repeatedly checking the flag, and when the mutex is locked by the waiting thread, it can’t be locked by any other thread. Both of these work against the thread doing the waiting: if the waiting thread is running, this limits the execution resources available to run the thread being waited for, and while the waiting thread has locked the mutex protecting the flag in order to check it, the thread being waited for is unable to lock the mutex to set the flag when it’s done. This is akin to staying awake all night talking to the train driver: he has to drive the train more slowly because you keep distracting him, so it takes longer to get there. Similarly, the waiting thread is consuming resources that could be used by other threads in the system and may end up waiting longer than necessary.
A second option is to have the waiting thread sleep for short periods between the checks using the std::this_thread::sleep_for() function (see section 4.3):

bool flag;

std::mutex m;

void wait_for_flag()

{ std::unique_lock<std::mutex> lk(m); while(!flag) {

B Unlock the
mutex.

Sleep for c
100 ms.

lk.unlock();

std::this_thread::sleep_for(std::chrono::milliseconds(100));

lk.lock();

} }

d Relock the mutex.

In the loop, the function unlocks the mutex B before the sleep c, and locks it again afterward d so another thread gets a chance to acquire it and set the flag.

74

CHAPTER 4 Synchronizing concurrent operations

This is an improvement because the thread doesn’t waste processing time while it’s sleeping, but it’s hard to get the sleep period right. Too short a sleep in between checks and the thread still wastes processing time checking; too long a sleep and the thread will keep on sleeping even when the task it’s waiting for is complete, introducing a delay. It’s rare that this oversleeping will have a direct impact on the operation of the program, but it could mean dropped frames in a fast-paced game or overrunning a time slice in a real-time application.
The third and preferred option is to use the facilities from the C++ Standard Library to wait for the event itself. The most basic mechanism for waiting for an event to be triggered by another thread (such as the presence of additional work in the pipeline mentioned previously) is the condition variable. Conceptually, a condition variable is associated with an event or other condition, and one or more threads can wait for that condition to be satisfied. When a thread has determined that the condition is satisfied, it can then notify one or more of the threads waiting on the condition variable in order to wake them up and allow them to continue processing.

4.1.1

Waiting for a condition with condition variables
The Standard C++ Library provides not one but two implementations of a condition variable: std::condition_variable and std::condition_variable_any. Both of these are declared in the <condition_variable> library header. In both cases, they need to work with a mutex in order to provide appropriate synchronization; the former is limited to working with std::mutex, whereas the latter can work with anything that meets the minimal criteria for being mutex-like, hence the _any suffix. Because std::condition_variable_any is more general, there’s the potential for additional costs in terms of size, performance, or OS resources, so std::condition_variable should be preferred unless the additional flexibility is required.
So, how do you use std::condition_variable to handle the example in the introduction? How do you let the thread that’s waiting for work sleep until there’s data to process? The following listing shows one way you could do this with a condition variable.

Listing 4.1 Waiting for data to process with std::condition_variable

std::mutex mut;

std::queue<data_chunk> data_queue;

b

std::condition_variable data_cond;

void data_preparation_thread()

{

while(more_data_to_prepare())

{

data_chunk const data=prepare_data();

{

std::lock_guard<std::mutex> lk(mut);

data_queue.push(data);

c

}

data_cond.notify_one();

d

Waiting for an event or other condition

75

}

}

void data_processing_thread()

{

while(true)

{

std::unique_lock<std::mutex> lk(mut);

data_cond.wait(

lk,[]{return !data_queue.empty();});

data_chunk data=data_queue.front();

data_queue.pop();

lk.unlock();

g

process(data);

if(is_last_chunk(data))

break;

}

}

e f

First off, you have a queue B that’s used to pass the data between the two threads.
When the data is ready, the thread preparing the data locks the mutex protecting the
queue using a std::lock_guard and pushes the data onto the queue c. It then calls
the notify_one() member function on the std::condition_variable instance to
notify the waiting thread (if there is one) d. Note that you put the code to push the
data onto the queue in a smaller scope, so you notify the condition variable after unlocking the mutex — this is so that, if the waiting thread wakes immediately, it doesn’t then have to block again, waiting for you to unlock the mutex.
On the other side of the fence, you have the processing thread. This thread first locks the mutex, but this time with a std::unique_lock rather than a std::lock_
guard e—you’ll see why in a minute. The thread then calls wait() on the std::
condition_variable, passing in the lock object and a lambda function that expresses
the condition being waited for f. Lambda functions are a new feature in C++11 that
allow you to write an anonymous function as part of another expression, and they’re ideally suited for specifying predicates for standard library functions such as wait(). In this case, the simple []{return !data_queue.empty();} lambda function checks to see if the data_queue is not empty()—that is, there’s some data in the queue ready for processing. Lambda functions are described in more detail in appendix A, section A.5.
The implementation of wait() then checks the condition (by calling the supplied lambda function) and returns if it’s satisfied (the lambda function returned true). If the condition isn’t satisfied (the lambda function returned false), wait() unlocks the mutex and puts the thread in a blocked or waiting state. When the condition variable is notified by a call to notify_one() from the data-preparation thread, the thread wakes from its slumber (unblocks it), reacquires the lock on the mutex, and checks the condition again, returning from wait() with the mutex still locked if the condition has been satisfied. If the condition hasn’t been satisfied, the thread unlocks the mutex and resumes waiting. This is why you need the std::unique_lock rather than the std::lock_guard—the waiting thread must unlock the mutex while it’s waiting

76

CHAPTER 4 Synchronizing concurrent operations

and lock it again afterward, and std::lock_guard doesn’t provide that flexibility. If the mutex remained locked while the thread was sleeping, the data-preparation thread wouldn’t be able to lock the mutex to add an item to the queue, and the waiting thread would never be able to see its condition satisfied.
Listing 4.1 uses a simple lambda function for the wait f, which checks to see if
the queue is not empty, but any function or callable object could be passed. If you already have a function to check the condition (perhaps because it’s more complicated than a simple test like this), then this function can be passed in directly; there’s no need to wrap it in a lambda. During a call to wait(), a condition variable may check the supplied condition any number of times; but it always does so with the mutex locked and will return immediately if (and only if) the function provided to test the condition returns true. When the waiting thread reacquires the mutex and checks the condition, if it isn’t in direct response to a notification from another thread, it’s called a spurious wake. Because the number and frequency of any such spurious wakes are by definition indeterminate, it isn’t advisable to use a function with side effects for the condition check. If you do so, you must be prepared for the side effects to occur multiple times.
Fundamentally, std::condition_variable::wait is an optimization over a busy-wait. Indeed, a conforming (though less than ideal) implementation technique is just a simple loop:
template<typename Predicate> void minimal_wait(std::unique_lock<std::mutex>& lk,Predicate pred){
while(!pred()){ lk.unlock(); lk.lock();
} }

Your code must be prepared to work with such a minimal implementation of wait(), as well as an implementation that only wakes up if notify_one() or notify_all() is called.
The flexibility to unlock a std::unique_lock isn’t just used for the call to wait();
it’s also used once you have the data to process but before processing it g. Processing
data can potentially be a time-consuming operation, and as you saw in chapter 3, it’s a bad idea to hold a lock on a mutex for longer than necessary.
Using a queue to transfer data between threads, as in listing 4.1, is a common scenario. Done well, the synchronization can be limited to the queue itself, which greatly reduces the possible number of synchronization problems and race conditions. In view of this, let’s now work on extracting a generic thread-safe queue from listing 4.1.

4.1.2

Building a thread-safe queue with condition variables
If you’re going to be designing a generic queue, it’s worth spending a few minutes thinking about the operations that are likely to be required, as you did with the

Waiting for an event or other condition

77

thread-safe stack back in section 3.2.3. Let’s look at the C++ Standard Library for inspiration, in the form of the std::queue<> container adaptor shown in the following listing.

Listing 4.2 std::queue interface
template <class T, class Container = std::deque<T> > class queue { public:
explicit queue(const Container&); explicit queue(Container&& = Container()); template <class Alloc> explicit queue(const Alloc&); template <class Alloc> queue(const Container&, const Alloc&); template <class Alloc> queue(Container&&, const Alloc&); template <class Alloc> queue(queue&&, const Alloc&); void swap(queue& q); bool empty() const; size_type size() const; T& front(); const T& front() const; T& back(); const T& back() const; void push(const T& x); void push(T&& x); void pop(); template <class... Args> void emplace(Args&&... args); };

If you ignore the construction, assignment, and swap operations, you’re left with three groups of operations: those that query the state of the whole queue (empty() and size()), those that query the elements of the queue (front() and back()), and those that modify the queue (push(), pop() and emplace()). This is the same as you had back in section 3.2.3 for the stack, and therefore you have the same issues regarding race conditions inherent in the interface. Consequently, you need to combine front() and pop() into a single function call, much as you combined top() and pop() for the stack. The code from listing 4.1 adds a new nuance, though: when using a queue to pass data between threads, the receiving thread often needs to wait for the data. Let’s provide two variants on pop(): try_pop(), which tries to pop the value from the queue but always returns immediately (with an indication of failure) even if there wasn’t a value to retrieve; and wait_and_pop(), which waits until there’s a value to retrieve. If you take your lead for the signatures from the stack example, your interface looks like the following.

Listing 4.3 The interface of your threadsafe_queue

#include <memory> template<typename T> class threadsafe_queue {

For std::shared_ptr

